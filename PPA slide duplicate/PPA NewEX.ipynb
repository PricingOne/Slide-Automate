{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import adodbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider=MSOLAP.8;Data Source=powerbi://api.powerbi.com/v1.0/myorg/Edgewell;Initial Catalog=Edgewell Mexico Suncare Dataset;Timeout=600;\n"
     ]
    }
   ],
   "source": [
    "client_manuf =[\"Edgewell\"]\n",
    "client_brands = [\"Hawaiian Tropic\",\"Banana Boat\",\"Beauty Care\"]\n",
    "\n",
    "decimals = 2\n",
    "sign = \"Before\"\n",
    "currency = '$'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    "ManufOn=False\n",
    "\n",
    "prodORitem = \"SKU\"\n",
    "categories = [\"Sun Care\"]\n",
    "# sectors = [\"Sunscreen\",\"Banded Pack\",\"Tanning\",\"After Sun\"]\n",
    "sectors = [\"Sunscreen\"]\n",
    "segments = [\"Sunscreen Cosmetics Aceite\",\"Sunscreen Cosmetics Aerosol\",\"Sunscreen Cosmetics Crema\",\"Sunscreen Cosmetics Emulsion\",\"Sunscreen Cosmetics Fluido\",\"Sunscreen Cosmetics Gel\",\"Sunscreen Cosmetics Liquido\",\"Sunscreen Cosmetics Locion\",\"Sunscreen Cosmetics Roll-On\",\"Sunscreen Cosmetics Serum\",\"Sunscreen Cosmetics Spray\",\"Sunscreen Cosmetics Stick\",\"Sunscreen Derma Aceite\",\"Sunscreen Derma Crema\",\"Sunscreen Derma Emulsion\",\"Sunscreen Derma Fluido\",\"Sunscreen Derma Gel\",\"Sunscreen Derma Gel-Crema\",\"Sunscreen Derma Liquido\",\"Sunscreen Derma Locion\",\"Sunscreen Derma Mousse\",\"Sunscreen Derma Spray\",\"Sunscreen Derma Stick\",\"Sunscreen Sport Crema\",\"Sunscreen Sport Liquido\",\"Sunscreen Sport Locion\",\"Sunscreen Sport Roll-On\",\"Sunscreen Sport Spray\",\"Sunscreen Sport Stick\"]\n",
    "\n",
    "# segments = [\"Sunscreen Aceite\",\"Sunscreen Aerosol\",\"Sunscreen Crema\",\"Sunscreen Emulsion\",\"Sunscreen Fluido\",\"Sunscreen Gel\",\"Sunscreen Gel-Crema\",\"Sunscreen Liquido\",\"Sunscreen Locion\",\"Sunscreen Mousse\",\"Sunscreen Roll-On\",\"Sunscreen Serum\",\"Sunscreen Spray\",\"Sunscreen Stick\"]\n",
    "\n",
    "# segments = [\"Sunscreen Adults\",\"Sunscreen Baby\",\"Sunscreen Kids\"]\n",
    "\n",
    "subsegments= []\n",
    "subcategories= []\n",
    " \n",
    "national = True\n",
    "customareas= \"AUTOS SCANNING\"\n",
    "areas = [\"NATIONAL\",\"CHANNEL\",f'{customareas}']#[\"RETAILER\"]\n",
    " \n",
    "regions_RET  = []\n",
    "channels_RET = []\n",
    "market_RET = []\n",
    "\n",
    "regions_CHAN = [\"Canal Moderno\", \"Convenience + Farmacias Cad + Hard Discounters\",\t\"Canal Tradicional\"]\n",
    "channels_CHAN = []\n",
    "market_CHAN = []\n",
    " \n",
    "regions_CUST = [\"Autos Scanning\"]\n",
    "channels_CUST = []\n",
    "market_CUST = []\n",
    "\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | April  2025\"\n",
    "years = {2023,2024,2025}\n",
    "\n",
    "\n",
    "ManufOrTopC =\"Top Companies\"\n",
    "BrandOrTopB = \"Top Brands\"\n",
    "end_date = \"2025-05-01\"\n",
    "\n",
    "past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "National=[\"NATIONAL\"]if national else []\n",
    "regions = regions_RET + regions_CHAN + regions_CUST\n",
    "channels = channels_RET + channels_CHAN + channels_CUST\n",
    "markets = market_RET + market_CHAN + market_CUST\n",
    "brands_only = True  # Get the Data of SKU Share by brands level only\n",
    "\n",
    "entity_hierarchy = [\n",
    "    (\"Area\",National),\n",
    "    (\"Region\", regions),\n",
    "    (\"Channel\", channels),\n",
    "    (\"Market\", markets)\n",
    "]\n",
    "hierarchy_levels = [\n",
    "    (\"Category\", categories),\n",
    "    (\"Sector\", sectors),\n",
    "    (\"Segment\", segments),\n",
    "    (\"SubSegment\", subsegments),\n",
    "    (\"SubCategory\", subcategories)\n",
    " \n",
    "]\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}\n",
    "\n",
    "server = \"powerbi://api.powerbi.com/v1.0/myorg/Edgewell\"\n",
    "dataset_name = \"Edgewell Mexico Suncare Dataset\"\n",
    "conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=600;\"\n",
    "print(conn_str)\n",
    "\n",
    "brackets = ['Base Price Bracket[Base Price Bracket]','Products[Pack Size Bracket]']\n",
    "\n",
    "ISDcolumn='Total Size'# default should be 'Total Size'\n",
    "path=os.path.join(os.getcwd(),\"PPA NewEX\")\n",
    "\n",
    "# Format months for DAX\n",
    "p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TopLine By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorManuf,entity_name, area, hierby, filter_p12m, entity_type, bracket):\n",
    "    outputdic = {}\n",
    "\n",
    "    time_filter = f\"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P12M\"\n",
    "        )\n",
    "    \"\"\" if filter_p12m else \"\"\n",
    "\n",
    "    key = f\"{entity_type} | {entity_name}\"\n",
    "    filters = \", \".join(filter(None, [time_filter]))\n",
    "\n",
    "    table_name, column_name = bracket.split(\"[\")\n",
    "    table_name = table_name.strip()\n",
    "    column_name = column_name.rstrip(\"]\").strip()\n",
    "\n",
    "    if table_name == 'Base Price Bracket':\n",
    "        rowlst = \"'Base Price Bracket','Base Price Bracket'[Base Price Bracket]\"\n",
    "        summarize_args = f\"'Base Price Bracket'[Base Price Bracket], Products[{BrandorManuf}]\"\n",
    "    else:\n",
    "        rowlst = f\"{table_name},{table_name}[{column_name}]\"\n",
    "        summarize_args = f\"'{table_name}'[{column_name}], Products[{BrandorManuf}]\"\n",
    "\n",
    "    if BrandorManuf==f'{BrandOrTopB}':\n",
    "        columns = [\"Value Share\", \"Brand WoB %\", \"WoB %\", \"Value Sales IYA\", \"Relative Price\"]\n",
    "    else:\n",
    "        columns = [\"Value Share\", \"Company WoB %\", \"WoB %\", \"Value Sales IYA\", \"Relative Price\"]\n",
    "\n",
    "    \n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Main query\n",
    "    main_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{BrandorManuf}]))\n",
    "                    ),\n",
    "                    {summarize_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Totals query: uses VALUES to simulate total values per column\n",
    "    total_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[{hierby}]),  -- dummy base just to return a single row\n",
    "                \"Value Share\", COALESCE([Value Share], 0),\n",
    "                \"Value Sales\", COALESCE([Value Sales], 0),\n",
    "                \"Company WoB %\", COALESCE([Brand WoB %], 0),  -- Assuming this is Brand WoB %\n",
    "                \"Relative Price\", COALESCE([Relative Price], 0),\n",
    "                \"Value Sales IYA\", COALESCE([Value Sales IYA], 0)\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            # Execute main query\n",
    "            cursor.execute(main_dax_query)\n",
    "            main_columns = [desc[0] for desc in cursor.description]\n",
    "            main_data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            # Execute total query\n",
    "            cursor.execute(total_dax_query)\n",
    "            total_columns = [desc[0] for desc in cursor.description]\n",
    "            total_data = cursor.fetchall()\n",
    "            \n",
    "        df = pd.DataFrame(main_data, columns=main_columns)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if df.empty:\n",
    "                outputdic[key] = df\n",
    "                return outputdic\n",
    "        if not df.empty:\n",
    "            grouped = []\n",
    "            numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "            for group_name, group_df in df.groupby(df.columns[0]):\n",
    "                grouped.append(group_df)\n",
    "\n",
    "                subtotal_values = {\n",
    "                    col: group_df[col].mean() if \"price\" in col.lower() else group_df[col].sum()\n",
    "                    for col in numeric_cols\n",
    "                }\n",
    "                subtotal_row = {df.columns[0]: f\"{group_name} Total\"}\n",
    "                subtotal_row.update(subtotal_values)\n",
    "                grouped.append(pd.DataFrame([subtotal_row]))\n",
    "\n",
    "            df_with_totals = pd.concat(grouped, ignore_index=True)\n",
    "\n",
    "            # Process grand total row\n",
    "            if total_data:\n",
    "                dt = pd.DataFrame([total_data[0]], columns=[col.replace(']', '').split('[')[-1] for col in total_columns])\n",
    "                grand_total_row = {df.columns[0]: \"Grand Total\"}\n",
    "                for col in numeric_cols:\n",
    "                    if col in dt.columns:\n",
    "                        grand_total_row[col] = dt[col].values[0]\n",
    "                df_with_totals = pd.concat([\n",
    "                    df_with_totals,\n",
    "                    pd.DataFrame([grand_total_row], columns=df.columns)\n",
    "                ], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df_with_totals if not df.empty else df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {entity_name}: {str(e)}\")\n",
    "        print(f\"DAX Query:\\n{main_dax_query}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "def process_dax_queries(BrandorManuf,entity_hierarchy, hierarchy_levels, time_filter, brackets):\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for bracket in brackets:\n",
    "            dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "            futures = {}\n",
    "            ordered_keys=[]\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    if isinstance(hier_values, list):\n",
    "                        for value in hier_values:\n",
    "                            for area, entity_list in entity_hierarchy:\n",
    "                                for entity in entity_list:\n",
    "                                    # print(hierby,value,entity)                                    \n",
    "                                    key = f\"{value} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(execute_dax_query,BrandorManuf, entity, area, hierby, time_filter, value, bracket)\n",
    "                                    futures[future] = key\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            # Insert results in original order\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "            # Save results only for the current bracket\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            if BrandorManuf==f'{BrandOrTopB}':\n",
    "                filename = f\"share_topline_{column_name}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            else:\n",
    "                filename = f\"share_topline_{column_name}_manuf\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "\n",
    "            output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "\n",
    "            print(f\"Saved data for {bracket} to {output_file}.\")\n",
    "\n",
    "# Example function call\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels, time_filter=True, brackets=brackets)\n",
    "if ManufOn:\n",
    "    process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels, time_filter=True, brackets=brackets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ManufOn==False:\n",
    "    for bracket in brackets:\n",
    "        dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "        futures = {}\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            # Save results only for the current bracket\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            filename = f\"share_topline_{column_name}_manuf\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CatScope By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_catscope_sector.pkl.\n",
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_catscope_segment.pkl.\n",
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_catscope_subsegment.pkl.\n",
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_catscope_subcategory.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_catscope_sector.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_catscope_segment.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_catscope_subsegment.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_catscope_subcategory.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorMaunf,entity_name, area, hierby, filter_p12m, bracket):\n",
    "    outputdic = {}\n",
    "    \n",
    "    time_filter = f\"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P12M\"\n",
    "        )\n",
    "    \"\"\" if filter_p12m else \"\"\n",
    "\n",
    "    key = f\"{entity_name}\"\n",
    "    filters = \", \".join(filter(None, [time_filter]))\n",
    "    \n",
    "    table_name, column_name = bracket.split(\"[\")\n",
    "    table_name = table_name.strip()\n",
    "    column_name = column_name.rstrip(\"]\").strip()\n",
    "    \n",
    "    if table_name == 'Base Price Bracket':\n",
    "        # Correct format for SUMMARIZE arguments\n",
    "        rowlst = \"'Base Price Bracket','Base Price Bracket'[Base Price Bracket]\"\n",
    "        summarize_args = f\"'Base Price Bracket'[Base Price Bracket],Products[{hierby}], Products[{BrandorMaunf}]\"\n",
    "        summarizetotal_args = f\"'Base Price Bracket'[Base Price Bracket],Products[{hierby}]\"\n",
    "\n",
    "    else:\n",
    "        rowlst = f\"{table_name},{table_name}[{column_name}]\"\n",
    "        summarize_args = f\"'{table_name}'[{column_name}],Products[{hierby}], Products[{BrandorMaunf}]\"\n",
    "        summarizetotal_args = f\"'{table_name}'[{column_name}],Products[{hierby}]\"\n",
    "        \n",
    "    if BrandorMaunf==f'{BrandOrTopB}':\n",
    "        columns = [\"Value Share\", \"Brand WoB %\", \"WoB %\"]\n",
    "    else:\n",
    "        columns = [\"Value Share\", 'Company WoB %', \"WoB %\"]\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Construct the DAX query with correct SUMMARIZE syntax\n",
    "    main_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{hierby}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{BrandorMaunf}]))\n",
    "                    ),\n",
    "                    {summarize_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )   \n",
    "    \"\"\"\n",
    "    grandtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),  -- dummy base just to return a single row\n",
    "                {column_exprs}\n",
    "\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    maintotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{hierby}]))\n",
    "                    ),\n",
    "                    {summarizetotal_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    total_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                    '{table_name}'[{column_name}]\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                # Execute main query\n",
    "                cursor.execute(main_dax_query)\n",
    "                main_columns = [desc[0] for desc in cursor.description]\n",
    "                main_data = cursor.fetchall()\n",
    "\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                # Execute total query\n",
    "                cursor.execute(grandtotal_dax_query)\n",
    "                grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "                grandtotal_data = cursor.fetchall()\n",
    "                \n",
    "            # Execute main query\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(maintotal_dax_query)\n",
    "                maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "                maintotal_data = cursor.fetchall()\n",
    "\n",
    "            # Execute total query\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(total_dax_query)\n",
    "                total_columns = [desc[0] for desc in cursor.description]\n",
    "                total_data = cursor.fetchall()\n",
    "                \n",
    "            maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "            maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "            total_df = pd.DataFrame(total_data, columns=total_columns)\n",
    "            total_df.columns = total_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            total_df = total_df.loc[~(total_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            if maintotal_df.shape[1] > 1:\n",
    "                maintotal_df.iloc[:, 1] = maintotal_df.iloc[:, 1].astype(str) + \" total\"\n",
    "\n",
    "            # ✅ Prepend \"total \" to column 0 in total_df\n",
    "            if total_df.shape[1] > 0:\n",
    "                total_df.iloc[:, 0] = total_df.iloc[:, 0].astype(str) +\" total\" \n",
    "\n",
    "            if maintotal_df.empty:\n",
    "                outputdic[key] = maintotal_df\n",
    "                return outputdic\n",
    "            if not maintotal_df.empty:\n",
    "                df_with_totals = pd.concat([maintotal_df, total_df], ignore_index=True)\n",
    "        \n",
    "            df = pd.DataFrame(main_data, columns=main_columns)\n",
    "            df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "            if df.empty:\n",
    "                    outputdic[key] = df\n",
    "                    return outputdic\n",
    "            if not df.empty:\n",
    "                grouped = []\n",
    "                numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "                # Process grand total row\n",
    "                if grandtotal_data:\n",
    "                    dt = pd.DataFrame([grandtotal_data[0]], columns=[col.replace(']', '').split('[')[-1] for col in grandtotal_columns])\n",
    "                    grand_total_row = {df.columns[0]: \"Grand Total\"}\n",
    "                    for col in numeric_cols:\n",
    "                        if col in dt.columns:\n",
    "                            grand_total_row[col] = dt[col].values[0]\n",
    "                    df_with_totals = pd.concat([\n",
    "                        df,df_with_totals,\n",
    "                        pd.DataFrame([grand_total_row], columns=df.columns)\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df_with_totals if not df.empty else df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {entity_name}: {str(e)}\")\n",
    "        print(f\"DAX Query:\\n{main_dax_query}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "def process_dax_queries(BrandorMaunf,entity_hierarchy, hierarchy_levels, time_filter, brackets):\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for bracket in brackets:\n",
    "            dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                if hierby==\"Category\":\n",
    "                    continue\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        key = f\"{entity}\"  # Match key used in `execute_dax_query`\n",
    "                        ordered_keys.append(key)\n",
    "                        future = executor.submit(execute_dax_query,BrandorMaunf, entity, area, hierby, time_filter, bracket)\n",
    "                        futures[future] = key\n",
    "\n",
    "                temp_results = {}\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    temp_results.update(result)\n",
    "\n",
    "                # Insert results in original order\n",
    "                for key in ordered_keys:\n",
    "                    if key in temp_results:\n",
    "                        dfs_results[key] = temp_results[key]\n",
    "                # Save results only for the current bracket\n",
    "                column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "                if BrandorMaunf==f'{BrandOrTopB}':\n",
    "                    filename = f\"share_topline_{column_name}_catscope_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "                else:\n",
    "                    filename = f\"share_topline_{column_name}_catscope_manuf_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "\n",
    "                output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "\n",
    "                with open(output_file, \"wb\") as f:\n",
    "                    pd.to_pickle(dfs_results, f)\n",
    "\n",
    "                print(f\"Saved data for {bracket} to {output_file}.\")\n",
    "\n",
    "# Example function call\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels, time_filter=True, brackets=brackets)\n",
    "if ManufOn:\n",
    "    process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels, time_filter=True, brackets=brackets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if ManufOn==False: Create empty  pickle files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ManufOn==False:\n",
    "    for bracket in brackets:\n",
    "        dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "        futures = {}\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby==\"Category\":\n",
    "                continue\n",
    "            # Save results only for the current bracket\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            filename = f\"share_topline_{column_name}_catscope_manuf_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent Scope By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bracket: Base Price Bracket[Base Price Bracket] | hierby: Sector\n",
      "✅ Saved data for bracket: Base Price Bracket[Base Price Bracket], hierby: Sector to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_parentscope_sector.pkl\n",
      "Processing bracket: Base Price Bracket[Base Price Bracket] | hierby: Segment\n",
      "✅ Saved data for bracket: Base Price Bracket[Base Price Bracket], hierby: Segment to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_parentscope_segment.pkl\n",
      "Processing bracket: Base Price Bracket[Base Price Bracket] | hierby: SubSegment\n",
      "✅ Saved data for bracket: Base Price Bracket[Base Price Bracket], hierby: SubSegment to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_parentscope_subsegment.pkl\n",
      "Processing bracket: Base Price Bracket[Base Price Bracket] | hierby: SubCategory\n",
      "✅ Saved data for bracket: Base Price Bracket[Base Price Bracket], hierby: SubCategory to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_parentscope_subcategory.pkl\n",
      "Processing bracket: Products[Pack Size Bracket] | hierby: Sector\n",
      "✅ Saved data for bracket: Products[Pack Size Bracket], hierby: Sector to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_parentscope_sector.pkl\n",
      "Processing bracket: Products[Pack Size Bracket] | hierby: Segment\n",
      "✅ Saved data for bracket: Products[Pack Size Bracket], hierby: Segment to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_parentscope_segment.pkl\n",
      "Processing bracket: Products[Pack Size Bracket] | hierby: SubSegment\n",
      "✅ Saved data for bracket: Products[Pack Size Bracket], hierby: SubSegment to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_parentscope_subsegment.pkl\n",
      "Processing bracket: Products[Pack Size Bracket] | hierby: SubCategory\n",
      "✅ Saved data for bracket: Products[Pack Size Bracket], hierby: SubCategory to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_parentscope_subcategory.pkl\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorMaunf,entity_name, area, hierby,direct_parent, filter_p12m, bracket):\n",
    "    outputdic = {}\n",
    "    \n",
    "    time_filter = f\"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P12M\"\n",
    "        )\n",
    "    \"\"\" if filter_p12m else \"\"\n",
    "\n",
    "    key = f\"{entity_name}\"\n",
    "    filters = \", \".join(filter(None, [time_filter]))\n",
    "    \n",
    "    table_name, column_name = bracket.split(\"[\")\n",
    "    table_name = table_name.strip()\n",
    "    column_name = column_name.rstrip(\"]\").strip()\n",
    "    \n",
    "    if table_name == 'Base Price Bracket':\n",
    "        # Correct format for SUMMARIZE arguments\n",
    "        rowlst = \"'Base Price Bracket','Base Price Bracket'[Base Price Bracket]\"\n",
    "        summarize_args = f\"'Base Price Bracket'[Base Price Bracket], Products[{direct_parent[hierby]}],Products[{hierby}], Products[{BrandorMaunf}]\"\n",
    "        summarizeparenttot_args = f\"'Base Price Bracket'[Base Price Bracket], Products[{direct_parent[hierby]}]\"\n",
    "        summarizetotal_args = f\"'Base Price Bracket'[Base Price Bracket], Products[{direct_parent[hierby]}],Products[{hierby}]\"\n",
    "        \n",
    "\n",
    "    else:\n",
    "        rowlst = f\"{table_name},{table_name}[{column_name}]\"\n",
    "        summarize_args = f\"'{table_name}'[{column_name}], Products[{direct_parent[hierby]}],Products[{hierby}], Products[{BrandorMaunf}]\"\n",
    "        summarizeparenttot_args = f\"'{table_name}'[{column_name}], Products[{direct_parent[hierby]}]\"\n",
    "        summarizetotal_args = f\"'{table_name}'[{column_name}], Products[{direct_parent[hierby]}],Products[{hierby}]\"\n",
    "        \n",
    "\n",
    "    if BrandorMaunf==f'{BrandOrTopB}':\n",
    "        columns = [\"Value Share\", \"Brand WoB %\", \"WoB %\",\"Value Sales\"]\n",
    "    else:            \n",
    "        columns = [\"Value Share\",'Company WoB %', \"WoB %\",\"Value Sales\"]\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    # Construct the DAX query with correct SUMMARIZE syntax\n",
    "    main_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{direct_parent[hierby]}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{hierby}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{BrandorMaunf}]))\n",
    "                    ),\n",
    "                    {summarize_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )   \n",
    "    \"\"\"\n",
    "    grandtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),  -- dummy base just to return a single row\n",
    "                {column_exprs}\n",
    "\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{direct_parent[hierby]}]))\n",
    "                    ),\n",
    "                    {summarizeparenttot_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    childtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{direct_parent[hierby]}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{hierby}]))\n",
    "\n",
    "                        \n",
    "                    ),\n",
    "                    {summarizetotal_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    total_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                    '{table_name}'[{column_name}]\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                # Execute main query\n",
    "                cursor.execute(main_dax_query)\n",
    "                main_columns = [desc[0] for desc in cursor.description]\n",
    "                main_data = cursor.fetchall()\n",
    "\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                # Execute total query\n",
    "                cursor.execute(grandtotal_dax_query)\n",
    "                grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "                grandtotal_data = cursor.fetchall()\n",
    "                \n",
    "            # Execute main query\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(parenttotal_dax_query)\n",
    "                maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "                maintotal_data = cursor.fetchall()\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(childtotal_dax_query)\n",
    "                childtotal_columns = [desc[0] for desc in cursor.description]\n",
    "                childtotal_data = cursor.fetchall()\n",
    "\n",
    "            # Execute total query\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(total_dax_query)\n",
    "                total_columns = [desc[0] for desc in cursor.description]\n",
    "                total_data = cursor.fetchall()\n",
    "                \n",
    "            maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "            maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "            childtotal_df = pd.DataFrame(childtotal_data, columns=childtotal_columns)\n",
    "            childtotal_df.columns = childtotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            childtotal_df = childtotal_df.loc[~(childtotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "\n",
    "            total_df = pd.DataFrame(total_data, columns=total_columns)\n",
    "            total_df.columns = total_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            total_df = total_df.loc[~(total_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "            if childtotal_df.shape[1] > 0:\n",
    "                childtotal_df.iloc[:, 2] = childtotal_df.iloc[:, 2].astype(str) +\" total\" \n",
    "                \n",
    "            if maintotal_df.shape[1] > 1:\n",
    "                maintotal_df.iloc[:, 1] = maintotal_df.iloc[:, 1].astype(str) + \" total\"\n",
    "\n",
    "            # ✅ Prepend \"total \" to column 0 in total_df\n",
    "            if total_df.shape[1] > 0:\n",
    "                total_df.iloc[:, 0] = total_df.iloc[:, 0].astype(str) +\" total\" \n",
    "                \n",
    "\n",
    "\n",
    "            if maintotal_df.empty:\n",
    "                outputdic[key] = maintotal_df\n",
    "                return outputdic\n",
    "            if not maintotal_df.empty:\n",
    "                df_with_totals = pd.concat([maintotal_df,childtotal_df, total_df], ignore_index=True)\n",
    "        \n",
    "            df = pd.DataFrame(main_data, columns=main_columns)\n",
    "            df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "            if df.empty:\n",
    "                    outputdic[key] = df\n",
    "                    return outputdic\n",
    "            if not df.empty:\n",
    "                grouped = []\n",
    "                numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "                # Process grand total row\n",
    "                if grandtotal_data:\n",
    "                    dt = pd.DataFrame([grandtotal_data[0]], columns=[col.replace(']', '').split('[')[-1] for col in grandtotal_columns])\n",
    "                    grand_total_row = {df.columns[0]: \"Grand Total\"}\n",
    "                    for col in numeric_cols:\n",
    "                        if col in dt.columns:\n",
    "                            grand_total_row[col] = dt[col].values[0]\n",
    "                    df_with_totals = pd.concat([\n",
    "                        df,df_with_totals,\n",
    "                        pd.DataFrame([grand_total_row], columns=df.columns)\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df_with_totals if not df.empty else df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {entity_name}: {str(e)}\")\n",
    "        print(f\"DAX Query:\\n{main_dax_query}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(BrandorMaunf,entity_hierarchy, hierarchy_levels, direct_parent, time_filter, brackets):\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for bracket in brackets:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                if hierby == \"Category\":\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing bracket: {bracket} | hierby: {hierby}\")\n",
    "                dfs_results = {}  # Reset for each combination of bracket + hierby\n",
    "                futures = {}\n",
    "\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        future = executor.submit(execute_dax_query,BrandorMaunf, entity, area, hierby, direct_parent, time_filter, bracket)\n",
    "                        futures[future] = (entity, area)\n",
    "\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    dfs_results.update(result)\n",
    "\n",
    "                column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "                if BrandorMaunf==f\"{ManufOrTopC}\":\n",
    "                    filename = f\"share_topline_{column_name}_parentscope_manuf_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "                else:    \n",
    "                    filename = f\"share_topline_{column_name}_parentscope_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "                output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "\n",
    "                with open(output_file, \"wb\") as f:\n",
    "                    pd.to_pickle(dfs_results, f)\n",
    "\n",
    "                print(f\"✅ Saved data for bracket: {bracket}, hierby: {hierby} to {output_file}\")\n",
    "\n",
    "\n",
    "# Example function call\n",
    "process_dax_queries(f\"{BrandOrTopB}\",entity_hierarchy, hierarchy_levels,direct_parent, time_filter=True, brackets=brackets)\n",
    "if ManufOn:\n",
    "    process_dax_queries(f\"{ManufOrTopC}\",entity_hierarchy, hierarchy_levels,direct_parent, time_filter=True, brackets=brackets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if ManufOn==False: Create empty  pickle files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ManufOn==False:\n",
    "    for bracket in brackets:\n",
    "        dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "        futures = {}\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby==\"Category\" or direct_parent[hierby]==\"Category\":\n",
    "                continue\n",
    "            # Save results only for the current bracket\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            filename = f\"share_topline_{column_name}_parentscope_manuf_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-size Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames saved to c:\\Users\\RaphaellaASHRAF\\OneDrive - Pricing One SA\\Documents\\GitHub\\Slide-Automate\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\inter_size.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, filter_p3m, brand=None):\n",
    "    outputdic = {}\n",
    "    \n",
    "    time_filter = \"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P3M\"\n",
    "        )\n",
    "    \"\"\" if filter_p3m else \"\"\n",
    "\n",
    "    brand_filter = f'Products[{BrandOrTopB}]=\"{brand}\"' if brand else \"\"\n",
    "    key = f\"{categories[0]} | {brand} | {entity_name}\" if brand else \"\"\n",
    "    filters = \", \".join(filter(None, [brand_filter, time_filter]))\n",
    "\n",
    "    # Define columns dynamically\n",
    "    columns = [\n",
    "        \"Base Price/Unit\",\"Value Sales\", \"Unit Sales\",\"Value Sales IYA\",\"VSOD\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    row_list=[]\n",
    "    if ISDcolumn != 'Total Size':\n",
    "        her_list = [\"Products[Variant]\", \"Products[Total Size]\", f\"Products[{ISDcolumn}]\"]\n",
    "    else:\n",
    "        her_list = [\"Products[Variant]\", \"Products[Total Size]\"]\n",
    "    if her_list:\n",
    "        summarize_expr = f\"SUMMARIZE(Products, {', '.join(her_list)})\"\n",
    "        \n",
    "    main_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                {summarize_expr},\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",            \n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )   \n",
    "    \"\"\"\n",
    "    total_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )   \n",
    "    \"\"\"\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            # Execute main query\n",
    "            cursor.execute(main_dax_query)\n",
    "            main_columns = [desc[0] for desc in cursor.description]\n",
    "            main_data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            # Execute total query\n",
    "            cursor.execute(total_dax_query)\n",
    "            total_columns = [desc[0] for desc in cursor.description]\n",
    "            total_data = cursor.fetchall()\n",
    "            \n",
    "        df = pd.DataFrame(main_data, columns=main_columns)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if df.empty:\n",
    "                outputdic[key] = df\n",
    "                return outputdic\n",
    "        if not df.empty:\n",
    "            grouped = []\n",
    "            numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "            for group_name, group_df in df.groupby(df.columns[0]):\n",
    "                grouped.append(group_df)\n",
    "\n",
    "                subtotal_values = {\n",
    "                    col: group_df[col].mean() if \"price\" in col.lower() else group_df[col].sum()\n",
    "                    for col in numeric_cols\n",
    "                }\n",
    "                subtotal_row = {df.columns[0]: f\"{group_name} Total\"}\n",
    "                subtotal_row.update(subtotal_values)\n",
    "                grouped.append(pd.DataFrame([subtotal_row]))\n",
    "\n",
    "            df_with_totals = pd.concat(grouped, ignore_index=True)\n",
    "\n",
    "            # Process grand total row\n",
    "            if total_data:\n",
    "                dt = pd.DataFrame([total_data[0]], columns=[col.replace(']', '').split('[')[-1] for col in total_columns])\n",
    "                grand_total_row = {df.columns[0]: \"Grand Total\"}\n",
    "                for col in numeric_cols:\n",
    "                    if col in dt.columns:\n",
    "                        grand_total_row[col] = dt[col].values[0]\n",
    "                df_with_totals = pd.concat([\n",
    "                    df_with_totals,\n",
    "                    pd.DataFrame([grand_total_row], columns=df.columns)\n",
    "                ], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df_with_totals if not df.empty else df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {entity_name}: {str(e)}\")\n",
    "        print(f\"DAX Query:\\n{main_dax_query}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, time_filter, client_brands=None):\n",
    "    dfs_results = {}\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = []\n",
    "        for area, entity_list in entity_hierarchy:\n",
    "            for entity in entity_list:                    \n",
    "                if client_brands:\n",
    "                    for brand in client_brands:\n",
    "                        futures.append(executor.submit(execute_dax_query, entity, area, time_filter, brand=brand))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                dfs_results.update(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query: {e}\")\n",
    "\n",
    "        # Construct the output file name correctly\n",
    "        \n",
    "        filename = \"inter_size.pkl\" \n",
    "\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "\n",
    "        print(f\"All DataFrames saved to {output_file}.\")\n",
    "\n",
    "\n",
    "process_dax_queries(entity_hierarchy, time_filter=True,client_brands=client_brands)  # Brand-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if ManufOn==False: Create empty  pickle files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ManufOn==False:\n",
    "    for bracket in brackets:\n",
    "        dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "        column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "        filename = f\"share_topline_{column_name}_manuf\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "        output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: Thu Jul  3 13:11:48 2025\n",
      "Script ended at: Thu Jul  3 13:17:20 2025\n",
      "Elapsed time: 331.25 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Script started at: {time.ctime(start_time)}\")\n",
    "print(f\"Script ended at: {time.ctime(end_time)}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "datasets_path =os.getcwd()+\"\\\\PPA NewEX\\\\\"\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
