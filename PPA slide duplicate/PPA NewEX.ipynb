{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import adodbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider=MSOLAP.8;Data Source=powerbi://api.powerbi.com/v1.0/myorg/Nicolas;Initial Catalog=Nicolas Sparkling Dataset;Timeout=600;\n"
     ]
    }
   ],
   "source": [
    "client_manuf =[\"Malard Nicolas\"]\n",
    "client_brands = [\"Nicolas\"]\n",
    " \n",
    "decimals = 2\n",
    "sign = \"After\"\n",
    "currency = '€'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    "ManufOn=False\n",
    "prodORitem = \"SKU\"\n",
    "categories = [\"Effervescents\"]\n",
    "sectors = [\"Champagnes\",\"Effervescents Sans Alcool\",\"Mousseux\"]\n",
    "segments = [\"Blanc De Blancs\",\"Bruts\",\"Crus\",\"Extra Brut\",\"Millesimes\",\n",
    "            \"Rosés\",\"Mousseux Bourgogne\",\"Mousseux Italiens\",\"Mousseux Val De Loire\",\n",
    "            \"Mousseux Alsace\",\"Mousseux Bordelais\"]\n",
    "subsegments= []\n",
    "subcategories= []\n",
    "\n",
    "national = True\n",
    "customareas= \"REVENUE\"\n",
    "areas = [\"NATIONAL\",\"CHANNEL\",\"REGION\",f'{customareas}']\n",
    " \n",
    "regions_RET  = []\n",
    "channels_RET = []\n",
    "market_RET = []\n",
    " \n",
    "regions_CHAN = [\"NICOLAS\",\"HMSM\"]\n",
    "channels_CHAN = [\"NICOLAS QCN\",\"NICOLAS VCN\",\"NICOLAS QCT\",\"NICOLAS QCA\",\"NICOLAS CCP\"]\n",
    "market_CHAN = []\n",
    " \n",
    "regions_CUST = []\n",
    "channels_CUST = [\"CA A\",\"CA B\",\"CA C\",\"CA D\",\"CA E\"]\n",
    "market_CUST = []\n",
    " \n",
    "regions_REG =[]\n",
    "channels_REG = [\"NICOLAS IDF\",\"NICOLAS PAC\",\"NICOLAS RHO\",\"NICOLAS AQU\",\"NICOLAS EST\"]\n",
    "market_REG= []\n",
    " \n",
    " \n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending June  2025\"\n",
    "years = {2023,2024,2025}\n",
    " \n",
    " \n",
    "ManufOrTopC =\"Top Companies\"\n",
    "BrandOrTopB = \"Top Brands\"\n",
    "end_date = \"2025-07-01\"\n",
    " \n",
    "past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "National=[\"NATIONAL\"]if national else []\n",
    "entity_hierarchy = [\n",
    "    (\"Area\",National),\n",
    "    (\"RETAILER\",\"Region\", regions_RET),\n",
    "    (\"RETAILER\",\"Channel\", channels_RET),\n",
    "    (\"RETAILER\",\"Market\", market_RET),\n",
    "    \n",
    "    (\"CHANNEL\",\"Region\", regions_CHAN),\n",
    "    (\"CHANNEL\",\"Channel\", channels_CHAN),\n",
    "    (\"CHANNEL\",\"Market\", market_CHAN),\n",
    "    \n",
    "    (\"REGION\",\"Region\", regions_REG),\n",
    "    (\"REGION\",\"Channel\", channels_REG),\n",
    "    (\"REGION\",\"Market\", market_REG),\n",
    "    \n",
    "    (f\"{customareas}\",\"Region\", regions_CUST),\n",
    "    (f\"{customareas}\",\"Channel\", channels_CUST),\n",
    "    (f\"{customareas}\",\"Market\", market_CUST)\n",
    "]\n",
    "entity_hierarchy = [\n",
    "    item for item in entity_hierarchy\n",
    "    if len(item) == 3 and item[2]  # item[2] is the entity list\n",
    "]\n",
    "hierarchy_levels = [\n",
    "    (\"Category\", categories),\n",
    "    (\"Sector\", sectors),\n",
    "    (\"Segment\", segments),\n",
    "    (\"SubSegment\", subsegments),\n",
    "    (\"SubCategory\", subcategories)\n",
    "]\n",
    "\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\",\n",
    "                \"SubCategory\":\"Segment\"}\n",
    " \n",
    "server = \"powerbi://api.powerbi.com/v1.0/myorg/Nicolas\"\n",
    "dataset_name = \"Nicolas Sparkling Dataset\"\n",
    "conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=600;\"\n",
    "print(conn_str)\n",
    " \n",
    "brackets = ['Base Price Bracket[Base Price Bracket]','Products[Pack Size Bracket]','Products[Pack Count Bracket]']\n",
    " \n",
    "ISDcolumn='Total Size'# default should be 'Total Size'\n",
    "path=os.path.join(os.getcwd(),\"PPA NewEX\")\n",
    " \n",
    "# Format months for DAX\n",
    "p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TopLine By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket.pkl.\n",
      "Saved data for Products[Portion Count Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorManuf,entity_name, area,market, hierby, filter_p12m, entity_type, bracket):\n",
    "    outputdic = {}\n",
    "\n",
    "    time_filter = f\"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P12M\"\n",
    "        )\n",
    "    \"\"\" if filter_p12m else \"\"\n",
    "\n",
    "    key = f\"{entity_type} | {entity_name}\"\n",
    "    filters = \", \".join(filter(None, [time_filter]))\n",
    "\n",
    "    table_name, column_name = bracket.split(\"[\")\n",
    "    table_name = table_name.strip()\n",
    "    column_name = column_name.rstrip(\"]\").strip()\n",
    "\n",
    "    if table_name == 'Base Price Bracket':\n",
    "        rowlst = \"'Base Price Bracket','Base Price Bracket'[Base Price Bracket]\"\n",
    "        summarize_args = f\"'Base Price Bracket'[Base Price Bracket], Products[{BrandorManuf}]\"\n",
    "    else:\n",
    "        rowlst = f\"{table_name},{table_name}[{column_name}]\"\n",
    "        summarize_args = f\"'{table_name}'[{column_name}], Products[{BrandorManuf}]\"\n",
    "\n",
    "    if BrandorManuf==f'{BrandOrTopB}':\n",
    "        # if BrandOrTopB == \"Brand\":\n",
    "            columns = [\"Value Share\", \"Brand WoB %\", \"WoB %\", \"Value Sales IYA\", \"Relative Price\"]\n",
    "        # if BrandOrTopB == \"SubBrand\":\n",
    "        #     columns = [\"Value Share\", \"SubBrand WoB %\", \"WoB %\", \"Value Sales IYA\", \"Relative Price\"]\n",
    "    else:\n",
    "        columns = [\"Value Share\", \"Company WoB %\", \"WoB %\", \"Value Sales IYA\", \"Relative Price\"]\n",
    "\n",
    "    \n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Main query\n",
    "    main_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{BrandorManuf}]))\n",
    "                    ),\n",
    "                    {summarize_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Totals query: uses VALUES to simulate total values per column\n",
    "    total_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[{hierby}]),  -- dummy base just to return a single row\n",
    "                \"Value Share\", COALESCE([Value Share], 0),\n",
    "                \"Value Sales\", COALESCE([Value Sales], 0),\n",
    "                \"Company WoB %\", COALESCE([Brand WoB %], 0),  -- Assuming this is Brand WoB %\n",
    "                \"Relative Price\", COALESCE([Relative Price], 0),\n",
    "                \"Value Sales IYA\", COALESCE([Value Sales IYA], 0)\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            # Execute main query\n",
    "            cursor.execute(main_dax_query)\n",
    "            main_columns = [desc[0] for desc in cursor.description]\n",
    "            main_data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            # Execute total query\n",
    "            cursor.execute(total_dax_query)\n",
    "            total_columns = [desc[0] for desc in cursor.description]\n",
    "            total_data = cursor.fetchall()\n",
    "            \n",
    "        df = pd.DataFrame(main_data, columns=main_columns)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if df.empty:\n",
    "                outputdic[key] = df\n",
    "                return outputdic\n",
    "        if not df.empty:\n",
    "            grouped = []\n",
    "            numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "            for group_name, group_df in df.groupby(df.columns[0]):\n",
    "                grouped.append(group_df)\n",
    "\n",
    "                subtotal_values = {\n",
    "                    col: group_df[col].mean() if \"price\" in col.lower() else group_df[col].sum()\n",
    "                    for col in numeric_cols\n",
    "                }\n",
    "                subtotal_row = {df.columns[0]: f\"{group_name} Total\"}\n",
    "                subtotal_row.update(subtotal_values)\n",
    "                grouped.append(pd.DataFrame([subtotal_row]))\n",
    "\n",
    "            df_with_totals = pd.concat(grouped, ignore_index=True)\n",
    "\n",
    "            # Process grand total row\n",
    "            if total_data:\n",
    "                dt = pd.DataFrame([total_data[0]], columns=[col.replace(']', '').split('[')[-1] for col in total_columns])\n",
    "                grand_total_row = {df.columns[0]: \"Grand Total\"}\n",
    "                for col in numeric_cols:\n",
    "                    if col in dt.columns:\n",
    "                        grand_total_row[col] = dt[col].values[0]\n",
    "                df_with_totals = pd.concat([\n",
    "                    df_with_totals,\n",
    "                    pd.DataFrame([grand_total_row], columns=df.columns)\n",
    "                ], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df_with_totals if not df.empty else df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {entity_name}: {str(e)}\")\n",
    "        print(f\"DAX Query:\\n{main_dax_query}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "def process_dax_queries(BrandorManuf,entity_hierarchy, hierarchy_levels, time_filter, brackets):\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for bracket in brackets:\n",
    "            dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "            futures = {}\n",
    "            ordered_keys=[]\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    if isinstance(hier_values, list):\n",
    "                        for value in hier_values:\n",
    "                            for area,market, entity_list in entity_hierarchy:\n",
    "                                for entity in entity_list:\n",
    "                                    # print(hierby,value,entity)                                    \n",
    "                                    key = f\"{value} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(execute_dax_query,BrandorManuf, entity, area,market, hierby, time_filter, value, bracket)\n",
    "                                    futures[future] = key\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            # Insert results in original order\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "            # Save results only for the current bracket\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            if BrandorManuf==f'{BrandOrTopB}':\n",
    "                filename = f\"share_topline_{column_name}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            else:\n",
    "                filename = f\"share_topline_{column_name}_manuf\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "\n",
    "            output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "\n",
    "            print(f\"Saved data for {bracket} to {output_file}.\")\n",
    "\n",
    "# Example function call\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels, time_filter=True, brackets=brackets)\n",
    "if ManufOn:\n",
    "    process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels, time_filter=True, brackets=brackets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ManufOn==False:\n",
    "    for bracket in brackets:\n",
    "        dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "        futures = {}\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            # Save results only for the current bracket\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            filename = f\"share_topline_{column_name}_manuf\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CatScope By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_catscope_sector.pkl.\n",
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_catscope_segment.pkl.\n",
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_catscope_subsegment.pkl.\n",
      "Saved data for Base Price Bracket[Base Price Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_catscope_subcategory.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_catscope_sector.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_catscope_segment.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_catscope_subsegment.pkl.\n",
      "Saved data for Products[Pack Size Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_catscope_subcategory.pkl.\n",
      "Saved data for Products[Portion Count Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket_catscope_sector.pkl.\n",
      "Saved data for Products[Portion Count Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket_catscope_segment.pkl.\n",
      "Saved data for Products[Portion Count Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket_catscope_subsegment.pkl.\n",
      "Saved data for Products[Portion Count Bracket] to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket_catscope_subcategory.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorMaunf,entity_name, area,market, hierby, filter_p12m, bracket):\n",
    "    outputdic = {}\n",
    "    \n",
    "    time_filter = f\"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P12M\"\n",
    "        )\n",
    "    \"\"\" if filter_p12m else \"\"\n",
    "\n",
    "    key = f\"{entity_name}\"\n",
    "    filters = \", \".join(filter(None, [time_filter]))\n",
    "    \n",
    "    table_name, column_name = bracket.split(\"[\")\n",
    "    table_name = table_name.strip()\n",
    "    column_name = column_name.rstrip(\"]\").strip()\n",
    "    \n",
    "    if table_name == 'Base Price Bracket':\n",
    "        # Correct format for SUMMARIZE arguments\n",
    "        rowlst = \"'Base Price Bracket','Base Price Bracket'[Base Price Bracket]\"\n",
    "        summarize_args = f\"'Base Price Bracket'[Base Price Bracket],Products[{hierby}], Products[{BrandorMaunf}]\"\n",
    "        summarizetotal_args = f\"'Base Price Bracket'[Base Price Bracket],Products[{hierby}]\"\n",
    "\n",
    "    else:\n",
    "        rowlst = f\"{table_name},{table_name}[{column_name}]\"\n",
    "        summarize_args = f\"'{table_name}'[{column_name}],Products[{hierby}], Products[{BrandorMaunf}]\"\n",
    "        summarizetotal_args = f\"'{table_name}'[{column_name}],Products[{hierby}]\"\n",
    "        \n",
    "    if BrandorMaunf==f'{BrandOrTopB}':\n",
    "        # if BrandOrTopB == \"Brand\":\n",
    "            columns = [\"Value Share\", \"Brand WoB %\", \"WoB %\"]\n",
    "        # if BrandOrTopB == \"SubBrand\":\n",
    "        #     columns = [\"Value Share\", \"SubBrand WoB %\", \"WoB %\"]\n",
    "    else:\n",
    "        columns = [\"Value Share\", 'Company WoB %', \"WoB %\"]\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Construct the DAX query with correct SUMMARIZE syntax\n",
    "    main_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{hierby}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{BrandorMaunf}]))\n",
    "                    ),\n",
    "                    {summarize_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )   \n",
    "    \"\"\"\n",
    "    grandtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),  -- dummy base just to return a single row\n",
    "                {column_exprs}\n",
    "\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    maintotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{hierby}]))\n",
    "                    ),\n",
    "                    {summarizetotal_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    total_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                    '{table_name}'[{column_name}]\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                # Execute main query\n",
    "                cursor.execute(main_dax_query)\n",
    "                main_columns = [desc[0] for desc in cursor.description]\n",
    "                main_data = cursor.fetchall()\n",
    "\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                # Execute total query\n",
    "                cursor.execute(grandtotal_dax_query)\n",
    "                grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "                grandtotal_data = cursor.fetchall()\n",
    "                \n",
    "            # Execute main query\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(maintotal_dax_query)\n",
    "                maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "                maintotal_data = cursor.fetchall()\n",
    "\n",
    "            # Execute total query\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(total_dax_query)\n",
    "                total_columns = [desc[0] for desc in cursor.description]\n",
    "                total_data = cursor.fetchall()\n",
    "                \n",
    "            maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "            maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "            total_df = pd.DataFrame(total_data, columns=total_columns)\n",
    "            total_df.columns = total_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            total_df = total_df.loc[~(total_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            if maintotal_df.shape[1] > 1:\n",
    "                maintotal_df.iloc[:, 1] = maintotal_df.iloc[:, 1].astype(str) + \" total\"\n",
    "\n",
    "            # ✅ Prepend \"total \" to column 0 in total_df\n",
    "            if total_df.shape[1] > 0:\n",
    "                total_df.iloc[:, 0] = total_df.iloc[:, 0].astype(str) +\" total\" \n",
    "\n",
    "            if maintotal_df.empty:\n",
    "                outputdic[key] = maintotal_df\n",
    "                return outputdic\n",
    "            if not maintotal_df.empty:\n",
    "                df_with_totals = pd.concat([maintotal_df, total_df], ignore_index=True)\n",
    "        \n",
    "            df = pd.DataFrame(main_data, columns=main_columns)\n",
    "            df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "            if df.empty:\n",
    "                    outputdic[key] = df\n",
    "                    return outputdic\n",
    "            if not df.empty:\n",
    "                grouped = []\n",
    "                numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "                # Process grand total row\n",
    "                if grandtotal_data:\n",
    "                    dt = pd.DataFrame([grandtotal_data[0]], columns=[col.replace(']', '').split('[')[-1] for col in grandtotal_columns])\n",
    "                    grand_total_row = {df.columns[0]: \"Grand Total\"}\n",
    "                    for col in numeric_cols:\n",
    "                        if col in dt.columns:\n",
    "                            grand_total_row[col] = dt[col].values[0]\n",
    "                    df_with_totals = pd.concat([\n",
    "                        df,df_with_totals,\n",
    "                        pd.DataFrame([grand_total_row], columns=df.columns)\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df_with_totals if not df.empty else df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {entity_name}: {str(e)}\")\n",
    "        print(f\"DAX Query:\\n{main_dax_query}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "def process_dax_queries(BrandorMaunf,entity_hierarchy, hierarchy_levels, time_filter, brackets):\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for bracket in brackets:\n",
    "            dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                if hierby==\"Category\":\n",
    "                    continue\n",
    "                for area,market, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        key = f\"{entity}\"  # Match key used in `execute_dax_query`\n",
    "                        ordered_keys.append(key)\n",
    "                        future = executor.submit(execute_dax_query,BrandorMaunf, entity, area,market, hierby, time_filter, bracket)\n",
    "                        futures[future] = key\n",
    "\n",
    "                temp_results = {}\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    temp_results.update(result)\n",
    "\n",
    "                # Insert results in original order\n",
    "                for key in ordered_keys:\n",
    "                    if key in temp_results:\n",
    "                        dfs_results[key] = temp_results[key]\n",
    "                # Save results only for the current bracket\n",
    "                column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "                if BrandorMaunf==f'{BrandOrTopB}':\n",
    "                    filename = f\"share_topline_{column_name}_catscope_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "                else:\n",
    "                    filename = f\"share_topline_{column_name}_catscope_manuf_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "\n",
    "                output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "\n",
    "                with open(output_file, \"wb\") as f:\n",
    "                    pd.to_pickle(dfs_results, f)\n",
    "\n",
    "                print(f\"Saved data for {bracket} to {output_file}.\")\n",
    "\n",
    "# Example function call\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels, time_filter=True, brackets=brackets)\n",
    "if ManufOn:\n",
    "    process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels, time_filter=True, brackets=brackets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if ManufOn==False: Create empty  pickle files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ManufOn==False:\n",
    "    for bracket in brackets:\n",
    "        dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "        futures = {}\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby==\"Category\":\n",
    "                continue\n",
    "            # Save results only for the current bracket\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            filename = f\"share_topline_{column_name}_catscope_manuf_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent Scope By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bracket: Base Price Bracket[Base Price Bracket] | hierby: Sector\n",
      "✅ Saved data for bracket: Base Price Bracket[Base Price Bracket], hierby: Sector to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_parentscope_sector.pkl\n",
      "Processing bracket: Base Price Bracket[Base Price Bracket] | hierby: Segment\n",
      "✅ Saved data for bracket: Base Price Bracket[Base Price Bracket], hierby: Segment to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_parentscope_segment.pkl\n",
      "Processing bracket: Base Price Bracket[Base Price Bracket] | hierby: SubSegment\n",
      "✅ Saved data for bracket: Base Price Bracket[Base Price Bracket], hierby: SubSegment to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_parentscope_subsegment.pkl\n",
      "Processing bracket: Base Price Bracket[Base Price Bracket] | hierby: SubCategory\n",
      "✅ Saved data for bracket: Base Price Bracket[Base Price Bracket], hierby: SubCategory to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_base_price_bracket_parentscope_subcategory.pkl\n",
      "Processing bracket: Products[Pack Size Bracket] | hierby: Sector\n",
      "✅ Saved data for bracket: Products[Pack Size Bracket], hierby: Sector to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_parentscope_sector.pkl\n",
      "Processing bracket: Products[Pack Size Bracket] | hierby: Segment\n",
      "✅ Saved data for bracket: Products[Pack Size Bracket], hierby: Segment to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_parentscope_segment.pkl\n",
      "Processing bracket: Products[Pack Size Bracket] | hierby: SubSegment\n",
      "✅ Saved data for bracket: Products[Pack Size Bracket], hierby: SubSegment to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_parentscope_subsegment.pkl\n",
      "Processing bracket: Products[Pack Size Bracket] | hierby: SubCategory\n",
      "✅ Saved data for bracket: Products[Pack Size Bracket], hierby: SubCategory to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_pack_size_bracket_parentscope_subcategory.pkl\n",
      "Processing bracket: Products[Portion Count Bracket] | hierby: Sector\n",
      "✅ Saved data for bracket: Products[Portion Count Bracket], hierby: Sector to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket_parentscope_sector.pkl\n",
      "Processing bracket: Products[Portion Count Bracket] | hierby: Segment\n",
      "✅ Saved data for bracket: Products[Portion Count Bracket], hierby: Segment to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket_parentscope_segment.pkl\n",
      "Processing bracket: Products[Portion Count Bracket] | hierby: SubSegment\n",
      "✅ Saved data for bracket: Products[Portion Count Bracket], hierby: SubSegment to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket_parentscope_subsegment.pkl\n",
      "Processing bracket: Products[Portion Count Bracket] | hierby: SubCategory\n",
      "✅ Saved data for bracket: Products[Portion Count Bracket], hierby: SubCategory to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\share_topline_portion_count_bracket_parentscope_subcategory.pkl\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorMaunf,entity_name, area,market, hierby,direct_parent, filter_p12m, bracket):\n",
    "    outputdic = {}\n",
    "    \n",
    "    time_filter = f\"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P12M\"\n",
    "        )\n",
    "    \"\"\" if filter_p12m else \"\"\n",
    "\n",
    "    key = f\"{entity_name}\"\n",
    "    filters = \", \".join(filter(None, [time_filter]))\n",
    "    \n",
    "    table_name, column_name = bracket.split(\"[\")\n",
    "    table_name = table_name.strip()\n",
    "    column_name = column_name.rstrip(\"]\").strip()\n",
    "    \n",
    "    if table_name == 'Base Price Bracket':\n",
    "        # Correct format for SUMMARIZE arguments\n",
    "        rowlst = \"'Base Price Bracket','Base Price Bracket'[Base Price Bracket]\"\n",
    "        summarize_args = f\"'Base Price Bracket'[Base Price Bracket], Products[{direct_parent[hierby]}],Products[{hierby}], Products[{BrandorMaunf}]\"\n",
    "        summarizeparenttot_args = f\"'Base Price Bracket'[Base Price Bracket], Products[{direct_parent[hierby]}]\"\n",
    "        summarizetotal_args = f\"'Base Price Bracket'[Base Price Bracket], Products[{direct_parent[hierby]}],Products[{hierby}]\"\n",
    "        \n",
    "\n",
    "    else:\n",
    "        rowlst = f\"{table_name},{table_name}[{column_name}]\"\n",
    "        summarize_args = f\"'{table_name}'[{column_name}], Products[{direct_parent[hierby]}],Products[{hierby}], Products[{BrandorMaunf}]\"\n",
    "        summarizeparenttot_args = f\"'{table_name}'[{column_name}], Products[{direct_parent[hierby]}]\"\n",
    "        summarizetotal_args = f\"'{table_name}'[{column_name}], Products[{direct_parent[hierby]}],Products[{hierby}]\"\n",
    "        \n",
    "\n",
    "    if BrandorMaunf==f'{BrandOrTopB}':\n",
    "        # if BrandOrTopB == \"Brand\":\n",
    "            columns = [\"Value Share\", \"Brand WoB %\", \"WoB %\",\"Value Sales\"]\n",
    "        # if BrandOrTopB == \"SubBrand\":\n",
    "        #     columns = [\"Value Share\", \"SubBrand WoB %\", \"WoB %\",\"Value Sales\"]\n",
    "    else:            \n",
    "        columns = [\"Value Share\",'Company WoB %', \"WoB %\",\"Value Sales\"]\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    # Construct the DAX query with correct SUMMARIZE syntax\n",
    "    main_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{direct_parent[hierby]}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{hierby}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{BrandorMaunf}]))\n",
    "                    ),\n",
    "                    {summarize_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )   \n",
    "    \"\"\"\n",
    "    grandtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),  -- dummy base just to return a single row\n",
    "                {column_exprs}\n",
    "\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{direct_parent[hierby]}]))\n",
    "                    ),\n",
    "                    {summarizeparenttot_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    childtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{direct_parent[hierby]}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, Products[{hierby}]))\n",
    "\n",
    "                        \n",
    "                    ),\n",
    "                    {summarizetotal_args}\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    total_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    DISTINCT(SELECTCOLUMNS({rowlst})),\n",
    "                    '{table_name}'[{column_name}]\n",
    "                ), \n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                # Execute main query\n",
    "                cursor.execute(main_dax_query)\n",
    "                main_columns = [desc[0] for desc in cursor.description]\n",
    "                main_data = cursor.fetchall()\n",
    "\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                # Execute total query\n",
    "                cursor.execute(grandtotal_dax_query)\n",
    "                grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "                grandtotal_data = cursor.fetchall()\n",
    "                \n",
    "            # Execute main query\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(parenttotal_dax_query)\n",
    "                maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "                maintotal_data = cursor.fetchall()\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(childtotal_dax_query)\n",
    "                childtotal_columns = [desc[0] for desc in cursor.description]\n",
    "                childtotal_data = cursor.fetchall()\n",
    "\n",
    "            # Execute total query\n",
    "            with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "                cursor.execute(total_dax_query)\n",
    "                total_columns = [desc[0] for desc in cursor.description]\n",
    "                total_data = cursor.fetchall()\n",
    "                \n",
    "            maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "            maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "            childtotal_df = pd.DataFrame(childtotal_data, columns=childtotal_columns)\n",
    "            childtotal_df.columns = childtotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            childtotal_df = childtotal_df.loc[~(childtotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "\n",
    "            total_df = pd.DataFrame(total_data, columns=total_columns)\n",
    "            total_df.columns = total_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            total_df = total_df.loc[~(total_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "            if childtotal_df.shape[1] > 0:\n",
    "                childtotal_df.iloc[:, 2] = childtotal_df.iloc[:, 2].astype(str) +\" total\" \n",
    "                \n",
    "            if maintotal_df.shape[1] > 1:\n",
    "                maintotal_df.iloc[:, 1] = maintotal_df.iloc[:, 1].astype(str) + \" total\"\n",
    "\n",
    "            # ✅ Prepend \"total \" to column 0 in total_df\n",
    "            if total_df.shape[1] > 0:\n",
    "                total_df.iloc[:, 0] = total_df.iloc[:, 0].astype(str) +\" total\" \n",
    "                \n",
    "\n",
    "\n",
    "            if maintotal_df.empty:\n",
    "                outputdic[key] = maintotal_df\n",
    "                return outputdic\n",
    "            if not maintotal_df.empty:\n",
    "                df_with_totals = pd.concat([maintotal_df,childtotal_df, total_df], ignore_index=True)\n",
    "        \n",
    "            df = pd.DataFrame(main_data, columns=main_columns)\n",
    "            df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "            if df.empty:\n",
    "                    outputdic[key] = df\n",
    "                    return outputdic\n",
    "            if not df.empty:\n",
    "                grouped = []\n",
    "                numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "                # Process grand total row\n",
    "                if grandtotal_data:\n",
    "                    dt = pd.DataFrame([grandtotal_data[0]], columns=[col.replace(']', '').split('[')[-1] for col in grandtotal_columns])\n",
    "                    grand_total_row = {df.columns[0]: \"Grand Total\"}\n",
    "                    for col in numeric_cols:\n",
    "                        if col in dt.columns:\n",
    "                            grand_total_row[col] = dt[col].values[0]\n",
    "                    df_with_totals = pd.concat([\n",
    "                        df,df_with_totals,\n",
    "                        pd.DataFrame([grand_total_row], columns=df.columns)\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df_with_totals if not df.empty else df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {entity_name}: {str(e)}\")\n",
    "        print(f\"DAX Query:\\n{main_dax_query}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(BrandorMaunf,entity_hierarchy, hierarchy_levels, direct_parent, time_filter, brackets):\n",
    "    os.makedirs(path, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for bracket in brackets:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                if hierby == \"Category\":\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing bracket: {bracket} | hierby: {hierby}\")\n",
    "                dfs_results = {}  # Reset for each combination of bracket + hierby\n",
    "                futures = {}\n",
    "\n",
    "                for area,market, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        future = executor.submit(execute_dax_query,BrandorMaunf, entity, area,market, hierby, direct_parent, time_filter, bracket)\n",
    "                        futures[future] = (entity, area)\n",
    "\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    dfs_results.update(result)\n",
    "\n",
    "                column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "                if BrandorMaunf==f\"{ManufOrTopC}\":\n",
    "                    filename = f\"share_topline_{column_name}_parentscope_manuf_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "                else:    \n",
    "                    filename = f\"share_topline_{column_name}_parentscope_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "                output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "\n",
    "                with open(output_file, \"wb\") as f:\n",
    "                    pd.to_pickle(dfs_results, f)\n",
    "\n",
    "                print(f\"✅ Saved data for bracket: {bracket}, hierby: {hierby} to {output_file}\")\n",
    "\n",
    "\n",
    "# Example function call\n",
    "process_dax_queries(f\"{BrandOrTopB}\",entity_hierarchy, hierarchy_levels,direct_parent, time_filter=True, brackets=brackets)\n",
    "if ManufOn:\n",
    "    process_dax_queries(f\"{ManufOrTopC}\",entity_hierarchy, hierarchy_levels,direct_parent, time_filter=True, brackets=brackets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if ManufOn==False: Create empty  pickle files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ManufOn==False:\n",
    "    for bracket in brackets:\n",
    "        dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "        futures = {}\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby==\"Category\" or direct_parent[hierby]==\"Category\":\n",
    "                continue\n",
    "            # Save results only for the current bracket\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            filename = f\"share_topline_{column_name}_parentscope_manuf_{hierby}\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-size Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\inter_size.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area,market, filter_p3m, brand=None):\n",
    "    outputdic = {}\n",
    "    \n",
    "    time_filter = \"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P3M\"\n",
    "        )\n",
    "    \"\"\" if filter_p3m else \"\"\n",
    "\n",
    "    brand_filter = f'Products[{BrandOrTopB}]=\"{brand}\"' if brand else \"\"\n",
    "    key = f\"{categories[0]} | {brand} | {entity_name}\" if brand else \"\"\n",
    "    filters = \", \".join(filter(None, [brand_filter, time_filter]))\n",
    "\n",
    "    # Define columns dynamically\n",
    "    columns = [\n",
    "        \"Base Price/Unit\",\"Value Sales\", \"Unit Sales\",\"Value Sales IYA\",\"VSOD\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    row_list=[]\n",
    "    if ISDcolumn != 'Total Size':\n",
    "        her_list = [\"Products[Variant]\", \"Products[Total Size]\", f\"Products[{ISDcolumn}]\"]\n",
    "    else:\n",
    "        her_list = [\"Products[Variant]\", \"Products[Total Size]\"]\n",
    "    if her_list:\n",
    "        summarize_expr = f\"SUMMARIZE(Products, {', '.join(her_list)})\"\n",
    "        \n",
    "    main_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                {summarize_expr},\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",     \n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),       \n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )   \n",
    "    \"\"\"\n",
    "    total_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {filters},\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{\"{entity_name}\"}}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )   \n",
    "    \"\"\"\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            # Execute main query\n",
    "            cursor.execute(main_dax_query)\n",
    "            main_columns = [desc[0] for desc in cursor.description]\n",
    "            main_data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            # Execute total query\n",
    "            cursor.execute(total_dax_query)\n",
    "            total_columns = [desc[0] for desc in cursor.description]\n",
    "            total_data = cursor.fetchall()\n",
    "            \n",
    "        df = pd.DataFrame(main_data, columns=main_columns)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if df.empty:\n",
    "                outputdic[key] = df\n",
    "                return outputdic\n",
    "        if not df.empty:\n",
    "            grouped = []\n",
    "            numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "            for group_name, group_df in df.groupby(df.columns[0]):\n",
    "                grouped.append(group_df)\n",
    "\n",
    "                subtotal_values = {\n",
    "                    col: group_df[col].mean() if \"price\" in col.lower() else group_df[col].sum()\n",
    "                    for col in numeric_cols\n",
    "                }\n",
    "                subtotal_row = {df.columns[0]: f\"{group_name} Total\"}\n",
    "                subtotal_row.update(subtotal_values)\n",
    "                grouped.append(pd.DataFrame([subtotal_row]))\n",
    "\n",
    "            df_with_totals = pd.concat(grouped, ignore_index=True)\n",
    "\n",
    "            # Process grand total row\n",
    "            if total_data:\n",
    "                dt = pd.DataFrame([total_data[0]], columns=[col.replace(']', '').split('[')[-1] for col in total_columns])\n",
    "                grand_total_row = {df.columns[0]: \"Grand Total\"}\n",
    "                for col in numeric_cols:\n",
    "                    if col in dt.columns:\n",
    "                        grand_total_row[col] = dt[col].values[0]\n",
    "                df_with_totals = pd.concat([\n",
    "                    df_with_totals,\n",
    "                    pd.DataFrame([grand_total_row], columns=df.columns)\n",
    "                ], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df_with_totals if not df.empty else df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {entity_name}: {str(e)}\")\n",
    "        print(f\"DAX Query:\\n{main_dax_query}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, time_filter, client_brands=None):\n",
    "    dfs_results = {}\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = []\n",
    "        for area,market, entity_list in entity_hierarchy:\n",
    "            for entity in entity_list:                    \n",
    "                if client_brands:\n",
    "                    for brand in client_brands:\n",
    "                        futures.append(executor.submit(execute_dax_query, entity, area,market, time_filter, brand=brand))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                dfs_results.update(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query: {e}\")\n",
    "\n",
    "        # Construct the output file name correctly\n",
    "        \n",
    "        filename = \"inter_size.pkl\" \n",
    "\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "\n",
    "        print(f\"All DataFrames saved to {output_file}.\")\n",
    "\n",
    "\n",
    "process_dax_queries(entity_hierarchy, time_filter=True,client_brands=client_brands)  # Brand-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if ManufOn==False: Create empty  pickle files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ManufOn==False:\n",
    "    for bracket in brackets:\n",
    "        dfs_results = {}  # Reset dfs_results for each bracket!\n",
    "        column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "        filename = f\"share_topline_{column_name}_manuf\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "        output_file = os.path.join(path, f\"{filename}.pkl\")\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction  NewSlide BY RET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Category | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Saved DataFrames for ['Market[Area]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_area_base_price_bracket.pkl.\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Category | ['Market[Area]'].\n",
      "Saved DataFrames for ['Market[Area]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_area_pack_size_bracket.pkl.\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Query executed successfully for Category | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Saved DataFrames for ['Market[Area]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_area_portion_count_bracket.pkl.\n",
      "Query executed successfully for Category | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Saved DataFrames for ['Market[Region]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_retailer_base_price_bracket.pkl.\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Category | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Saved DataFrames for ['Market[Region]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_retailer_pack_size_bracket.pkl.\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Category | ['Market[Region]'].\n",
      "Saved DataFrames for ['Market[Region]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_retailer_portion_count_bracket.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorMaunf, entity_name, hierby, filter_p12m, area, bracket, entity_type=None, row_field=None,areamarket=True):\n",
    "    outputdic = {}\n",
    "\n",
    "    time_filter = \"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P12M\"\n",
    "        )\n",
    "    \"\"\" if filter_p12m else \"\"\n",
    "\n",
    "    key = f\"{entity_name}\"\n",
    "\n",
    "    crossjoin_parts = []\n",
    "\n",
    "    if isinstance(row_field, list):\n",
    "        for field in row_field:\n",
    "            table, col = field.split(\"[\")\n",
    "            col = col.rstrip(\"]\")\n",
    "            crossjoin_parts.append(\n",
    "                f'DISTINCT(SELECTCOLUMNS({table.strip()}, \"{col.strip()}\", {table.strip()}[{col.strip()}]))'\n",
    "            )\n",
    "    else:\n",
    "        table, col = row_field.split(\"[\")\n",
    "        col = col.rstrip(\"]\")\n",
    "        crossjoin_parts.append(\n",
    "            f'DISTINCT(SELECTCOLUMNS({table.strip()}, \"{col.strip()}\", {table.strip()}[{col.strip()}]))'\n",
    "        )\n",
    "\n",
    "    crossjoin_expr = \",\\n                \".join(crossjoin_parts)\n",
    "    row_fields_expr = \", \".join(\n",
    "    f\"[{field.split('[')[1].rstrip(']')}]\"\n",
    "    for field in row_field\n",
    "    ) if isinstance(row_field, list) else f\"[{row_field.split('[')[1].rstrip(']')}]\"\n",
    "\n",
    "    # print(row_fields_expr)\n",
    "    table_name, column_name = bracket.split(\"[\")\n",
    "    table_name = table_name.strip()\n",
    "    column_name = column_name.rstrip(\"]\").strip()\n",
    "\n",
    "    # Build summarize_args as a string with proper quoting\n",
    "    if table_name == 'Base Price Bracket':\n",
    "        column_name=\"Base Price Bracket\"\n",
    "\n",
    "    else:\n",
    "        rowlst = f\"{table_name},{table_name}[{column_name}]\"\n",
    "\n",
    "        \n",
    "    # Define columns for ADDCOLUMNS dynamicall\n",
    "    # print(crossjoin_expr)\n",
    "    crossjoin_expr_list = [crossjoin_expr]\n",
    "    crossjoin_expr_list=crossjoin_expr_list[0]\n",
    "    row_fields_exprlist=[row_fields_expr]\n",
    "    row_fields_expr_list = [field.strip() for field in row_fields_expr.split(\",\")]\n",
    "    row_fields_expr_first = row_fields_expr_list[0]\n",
    "    if areamarket is True and area:\n",
    "        areafilter = f\"\"\"FILTER('Market', 'Market'[Area] = \"{area}\"),\"\"\"\n",
    "    else:\n",
    "        areafilter = \"\"\n",
    "\n",
    "\n",
    "    areafilter = \", \".join(filter(None, [areafilter]))\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SUMMARIZE(\n",
    "                CROSSJOIN(\n",
    "\n",
    "                    DISTINCT(SELECTCOLUMNS('{table_name}', \"{column_name}\", '{table_name}'[{column_name}])),\n",
    "                    DISTINCT(SELECTCOLUMNS(Products, \"{BrandorMaunf}\", Products[{BrandorMaunf}])),\n",
    "                    {crossjoin_expr}\n",
    "                    \n",
    "                ),\n",
    "                [{column_name}], [{BrandorMaunf}],{row_fields_expr}\n",
    "            ),\n",
    "            \"Value Share\", COALESCE([Value Share], 0),\n",
    "            \"WoB %\", COALESCE([WoB %], 0),\n",
    "            \"Value Sales IYA\", COALESCE([Value Sales IYA], 0),\n",
    "            \"Relative Price\", COALESCE([Relative Price], 0)\n",
    "        ),\n",
    "        {time_filter},\n",
    "        {areafilter}\n",
    "        FILTER(Products, Products[Category] = \"{categories[0]}\" && Products[{hierby}] = \"{entity_type}\"),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    maintotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "            CALCULATETABLE(\n",
    "                ADDCOLUMNS(\n",
    "                    SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS('{table_name}', \"{column_name}\", '{table_name}'[{column_name}])),\n",
    "                        DISTINCT(SELECTCOLUMNS(Products, \"{BrandorMaunf}\", Products[{BrandorMaunf}])),\n",
    "                        \n",
    "                        {crossjoin_expr_list}\n",
    "                        \n",
    "                    ),\n",
    "                    [{column_name}], [{BrandorMaunf}],{row_fields_expr_first}\n",
    "                ),\n",
    "                    \"Value Share\", COALESCE([Value Share], 0),\n",
    "                    \"WoB %\", COALESCE([WoB %], 0),\n",
    "                    \"Value Sales IYA\", COALESCE([Value Sales IYA], 0),\n",
    "                    \"Relative Price\", COALESCE([Relative Price], 0)\n",
    "                ),\n",
    "                {time_filter},\n",
    "                {areafilter}       \n",
    "                FILTER(Products, Products[Category] = \"{categories[0]}\" && Products[{hierby}] = \"{entity_type}\"),\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "            )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(maintotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        \n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if maintotal_df.shape[1] > 1:\n",
    "                maintotal_df.iloc[:, 2] = maintotal_df.iloc[:, 2].astype(str) + \" total\"\n",
    "\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            # Ensure the total dataframe has the same columns as df\n",
    "            for col in df.columns:\n",
    "                if col not in maintotal_df.columns:\n",
    "                    maintotal_df[col] = pd.NA  # or np.nan if you prefer numeric NaNs\n",
    "\n",
    "            # Reorder columns to match df\n",
    "            maintotal_df = maintotal_df[df.columns]\n",
    "\n",
    "            # Concatenate totals on top of the main dataframe\n",
    "            df_with_totals = pd.concat([maintotal_df, df], ignore_index=True)\n",
    "\n",
    "\n",
    "            outputdic[key] = df_with_totals\n",
    "        print(f\"Query executed successfully for {hierby} | {row_field}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {hierby} {entity_type} | {row_field}: {db_error}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(BrandorManuf, hierarchy_levels, time_filter, area, row_list, brackets,areamarket):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for bracket in brackets:\n",
    "            dfs_results = {}  # ← Reset per bracket\n",
    "            futures = []      # ← Reset per bracket\n",
    "\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                for value in hier_values:\n",
    "                    futures.append(executor.submit(\n",
    "                        execute_dax_query,\n",
    "                        BrandorManuf, value, hierby, time_filter, area, bracket,\n",
    "                        entity_type=value, row_field=row_list\n",
    "                    ))\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    dfs_results.update(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing query: {e}\")\n",
    "\n",
    "            # suffix = row_list[0].split('[')[-1].rstrip(']')\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            if area == \"RETAILER\" and areamarket==True :\n",
    "                filename = f\"modified_retailer_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            elif area == \"CHANNEL\" and areamarket==True:\n",
    "                filename = f\"modified_channels_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            elif area == \"REGION\" and areamarket==True:\n",
    "                filename = f\"modified_region_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            elif area == customareas and areamarket==True:\n",
    "                filename = f\"modified_cust_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            else:\n",
    "                filename = f\"modified_Area_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            print(f\"Saved DataFrames for {row_list} to {output_file}.\")\n",
    "\n",
    "\n",
    "area_list = []\n",
    "area_list.append('Market[Area]')\n",
    "process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=\"\", row_list=area_list, brackets=brackets,areamarket=False)\n",
    "\n",
    "# Execute per area\n",
    "if \"RETAILER\" in areas:\n",
    "    RET_list = []\n",
    "    if regions_RET:\n",
    "        RET_list.append('Market[Region]')\n",
    "    if channels_RET:\n",
    "        RET_list.append('Market[Channel]')\n",
    "    if market_RET:\n",
    "        RET_list.append('Market[Market]')\n",
    "    process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=\"RETAILER\", row_list=RET_list, brackets=brackets,areamarket=True)\n",
    "\n",
    "if \"CHANNEL\" in areas:\n",
    "    CHA_list = []\n",
    "    if regions_CHAN:\n",
    "        CHA_list.append('Market[Region]')\n",
    "    if channels_CHAN:\n",
    "        CHA_list.append('Market[Channel]')\n",
    "    if market_CHAN:\n",
    "        CHA_list.append('Market[Market]')\n",
    "    process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=\"CHANNEL\", row_list=CHA_list, brackets=brackets,areamarket=True)\n",
    "\n",
    "if \"REGION\" in areas:\n",
    "    REG_list = []\n",
    "    if regions_CHAN:\n",
    "        REG_list.append('Market[Region]')\n",
    "    if channels_CHAN:\n",
    "        REG_list.append('Market[Channel]')\n",
    "    if market_CHAN:\n",
    "        REG_list.append('Market[Market]')\n",
    "    process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=\"REGION\", row_list=REG_list, brackets=brackets,areamarket=True)\n",
    "\n",
    "\n",
    "if f'{customareas}' in areas:\n",
    "    CUST_list = []\n",
    "    if regions_CUST:\n",
    "        CUST_list.append('Market[Region]')\n",
    "    if channels_CUST:\n",
    "        CUST_list.append('Market[Channel]')\n",
    "    if market_CUST:\n",
    "        CUST_list.append('Market[Market]')\n",
    "    process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=customareas, row_list=CUST_list, brackets=brackets,areamarket=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Category | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Saved DataFrames for ['Market[Area]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_cat_area_base_price_bracket.pkl.\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Query executed successfully for Category | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Saved DataFrames for ['Market[Area]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_cat_area_pack_size_bracket.pkl.\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Category | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Query executed successfully for Segment | ['Market[Area]'].\n",
      "Query executed successfully for Sector | ['Market[Area]'].\n",
      "Saved DataFrames for ['Market[Area]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_cat_area_portion_count_bracket.pkl.\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Category | ['Market[Region]'].\n",
      "Saved DataFrames for ['Market[Region]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_catretailer_base_price_bracket.pkl.\n",
      "Query executed successfully for Category | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Saved DataFrames for ['Market[Region]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_catretailer_pack_size_bracket.pkl.\n",
      "Query executed successfully for Category | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Sector | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Query executed successfully for Segment | ['Market[Region]'].\n",
      "Saved DataFrames for ['Market[Region]'] to c:\\Users\\aleaa\\Documents\\Slide-Automate\\PPA slide duplicate\\PPA NewEX\\modified_catretailer_portion_count_bracket.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorMaunf, entity_name, hierby, filter_p12m, area, bracket, entity_type=None, row_field=None,areamarket=True):\n",
    "    outputdic = {}\n",
    "\n",
    "    time_filter = \"\"\"\n",
    "        FILTER(\n",
    "            VALUES('Time Logic'[Time Period]), \n",
    "            'Time Logic'[Time Period] = \"P12M\"\n",
    "        )\n",
    "    \"\"\" if filter_p12m else \"\"\n",
    "\n",
    "    key = f\"{entity_name}\"\n",
    "    filters = \", \".join(filter(None, [time_filter]))\n",
    "    if areamarket is True and area:\n",
    "        areafilter = f\"\"\"FILTER('Market', 'Market'[Area] = \"{area}\"),\"\"\"\n",
    "    else:\n",
    "        areafilter = \"\"\n",
    "    areafilter = \", \".join(filter(None, [areafilter]))\n",
    "# Convert row_field list into DISTINCT(SELECTCOLUMNS(...)) parts\n",
    "    crossjoin_parts = []\n",
    "\n",
    "    if isinstance(row_field, list):\n",
    "        for field in row_field:\n",
    "            table, col = field.split(\"[\")\n",
    "            col = col.rstrip(\"]\")\n",
    "            crossjoin_parts.append(\n",
    "                f'DISTINCT(SELECTCOLUMNS({table.strip()}, \"{col.strip()}\", {table.strip()}[{col.strip()}]))'\n",
    "            )\n",
    "    else:\n",
    "        table, col = row_field.split(\"[\")\n",
    "        col = col.rstrip(\"]\")\n",
    "        crossjoin_parts.append(\n",
    "            f'DISTINCT(SELECTCOLUMNS({table.strip()}, \"{col.strip()}\", {table.strip()}[{col.strip()}]))'\n",
    "        )\n",
    "\n",
    "    crossjoin_expr = \",\\n                \".join(crossjoin_parts)\n",
    "    row_fields_expr = \", \".join(\n",
    "    f\"[{field.split('[')[1].rstrip(']')}]\"\n",
    "    for field in row_field\n",
    "    ) if isinstance(row_field, list) else f\"[{row_field.split('[')[1].rstrip(']')}]\"\n",
    "\n",
    "    # print(row_fields_expr)\n",
    "    table_name, column_name = bracket.split(\"[\")\n",
    "    table_name = table_name.strip()\n",
    "    column_name = column_name.rstrip(\"]\").strip()\n",
    "\n",
    "    # Build summarize_args as a string with proper quoting\n",
    "    if table_name == 'Base Price Bracket':\n",
    "        column_name=\"Base Price Bracket\"\n",
    "        \n",
    "\n",
    "    else:\n",
    "        rowlst = f\"{table_name},{table_name}[{column_name}]\"\n",
    "\n",
    "    # print(crossjoin_expr)\n",
    "    crossjoin_expr_list = [crossjoin_expr]\n",
    "    crossjoin_expr_list=crossjoin_expr_list[0]\n",
    "    row_fields_exprlist=[row_fields_expr]\n",
    "    row_fields_expr_list = [field.strip() for field in row_fields_expr.split(\",\")]\n",
    "    row_fields_expr_first = row_fields_expr_list[0]\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SUMMARIZE(\n",
    "                CROSSJOIN(\n",
    "\n",
    "                    DISTINCT(SELECTCOLUMNS('{table_name}', \"{column_name}\", '{table_name}'[{column_name}])),\n",
    "                    {crossjoin_expr}\n",
    "                    \n",
    "                ),\n",
    "                [{column_name}],{row_fields_expr}\n",
    "            ),\n",
    "            \"Value Share\", COALESCE([Value Share], 0),\n",
    "            \"WoB %\", COALESCE([WoB %], 0),\n",
    "            \"Value Sales IYA\", COALESCE([Value Sales IYA], 0),\n",
    "            \"Relative Price\", COALESCE([Relative Price], 0)\n",
    "        ),\n",
    "        {time_filter},\n",
    "        {areafilter} \n",
    "        FILTER(Products,Products[{hierby}] = \"{entity_type}\"),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    maintotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "            CALCULATETABLE(\n",
    "                ADDCOLUMNS(\n",
    "                    SUMMARIZE(\n",
    "                    CROSSJOIN(\n",
    "                        DISTINCT(SELECTCOLUMNS('{table_name}', \"{column_name}\", '{table_name}'[{column_name}])),\n",
    "                        {crossjoin_expr_list}\n",
    "                        \n",
    "                    ),\n",
    "                    [{column_name}],{row_fields_expr_first}\n",
    "                ),\n",
    "                    \"Value Share\", COALESCE([Value Share], 0),\n",
    "                    \"WoB %\", COALESCE([WoB %], 0),\n",
    "                    \"Value Sales IYA\", COALESCE([Value Sales IYA], 0),\n",
    "                    \"Relative Price\", COALESCE([Relative Price], 0)\n",
    "                ),\n",
    "                {time_filter},\n",
    "                {areafilter}     \n",
    "                FILTER(Products,Products[{hierby}] = \"{entity_type}\"),\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "            )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(maintotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        # print(maintotal_df)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if maintotal_df.shape[1] > 1:\n",
    "                maintotal_df.iloc[:, 1] = maintotal_df.iloc[:, 1].astype(str) + \" total\"\n",
    "\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            # Ensure the total dataframe has the same columns as df\n",
    "            for col in df.columns:\n",
    "                if col not in maintotal_df.columns:\n",
    "                    maintotal_df[col] = pd.NA  # or np.nan if you prefer numeric NaNs\n",
    "\n",
    "            # Reorder columns to match df\n",
    "            maintotal_df = maintotal_df[df.columns]\n",
    "            frames = [df_ for df_ in [maintotal_df, df] if not df_.empty and not df_.isna().all(axis=None)]\n",
    "            df_with_totals = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "            # Concatenate totals on top of the main dataframe\n",
    "\n",
    "\n",
    "            outputdic[key] = df_with_totals\n",
    "        print(f\"Query executed successfully for {hierby} | {row_field}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {hierby} {entity_type} | {row_field}: {db_error}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(BrandorManuf, hierarchy_levels, time_filter, area, row_list, brackets,areamarket):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for bracket in brackets:\n",
    "            dfs_results = {}  # ← Reset per bracket\n",
    "            futures = []      # ← Reset per bracket\n",
    "\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                for value in hier_values:\n",
    "                    futures.append(executor.submit(\n",
    "                        execute_dax_query,\n",
    "                        BrandorManuf, value, hierby, time_filter, area, bracket,\n",
    "                        entity_type=value, row_field=row_list\n",
    "                    ))\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    dfs_results.update(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing query: {e}\")\n",
    "\n",
    "            # suffix = row_list[0].split('[')[-1].rstrip(']')\n",
    "            column_name = bracket.split(\"[\")[-1].rstrip(\"]\")\n",
    "            if area == \"RETAILER\" and areamarket==True:\n",
    "                filename = f\"modified_catretailer_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            elif area == \"CHANNEL\" and areamarket==True:\n",
    "                filename = f\"modified_catchannels_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            elif area == \"REGION\" and areamarket==True:\n",
    "                filename = f\"modified_catregion_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            elif area == customareas and areamarket==True:\n",
    "                filename = f\"modified_catcust_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "            else:\n",
    "                filename = f\"modified_cat_Area_{column_name}.pkl\".replace(\" \", \"_\").replace(\"\\xa0\", \"_\").lower()\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            print(f\"Saved DataFrames for {row_list} to {output_file}.\")\n",
    "\n",
    "\n",
    "area_list = []\n",
    "area_list.append('Market[Area]')\n",
    "process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=\"\", row_list=area_list, brackets=brackets,areamarket=False)\n",
    "    \n",
    "if \"RETAILER\" in areas:\n",
    "    RET_list = []\n",
    "    if regions_RET:\n",
    "        RET_list.append('Market[Region]')\n",
    "    if channels_RET:\n",
    "        RET_list.append('Market[Channel]')\n",
    "    if market_RET:\n",
    "        RET_list.append('Market[Market]')\n",
    "    process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=\"RETAILER\", row_list=RET_list, brackets=brackets,areamarket=True)\n",
    "\n",
    "if \"CHANNEL\" in areas:\n",
    "    CHA_list = []\n",
    "    if regions_CHAN:\n",
    "        CHA_list.append('Market[Region]')\n",
    "    if channels_CHAN:\n",
    "        CHA_list.append('Market[Channel]')\n",
    "    if market_CHAN:\n",
    "        CHA_list.append('Market[Market]')\n",
    "    process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=\"CHANNEL\", row_list=CHA_list, brackets=brackets,areamarket=True)\n",
    "\n",
    "if \"REGION\" in areas:\n",
    "    REG_list = []\n",
    "    if regions_CHAN:\n",
    "        REG_list.append('Market[Region]')\n",
    "    if channels_CHAN:\n",
    "        REG_list.append('Market[Channel]')\n",
    "    if market_CHAN:\n",
    "        REG_list.append('Market[Market]')\n",
    "    process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=\"REGION\", row_list=REG_list, brackets=brackets,areamarket=True)\n",
    "\n",
    "if f'{customareas}' in areas:\n",
    "    CUST_list = []\n",
    "    if regions_CUST:\n",
    "        CUST_list.append('Market[Region]')\n",
    "    if channels_CUST:\n",
    "        CUST_list.append('Market[Channel]')\n",
    "    if market_CUST:\n",
    "        CUST_list.append('Market[Market]')\n",
    "    process_dax_queries(BrandOrTopB, hierarchy_levels, time_filter=True, area=customareas, row_list=CUST_list, brackets=brackets,areamarket=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: Wed Jul  9 12:50:26 2025\n",
      "Script ended at: Wed Jul  9 12:52:36 2025\n",
      "Elapsed time: 130.16 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Script started at: {time.ctime(start_time)}\")\n",
    "print(f\"Script ended at: {time.ctime(end_time)}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "datasets_path =os.getcwd()+\"\\\\PPA NewEX\\\\\"\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
