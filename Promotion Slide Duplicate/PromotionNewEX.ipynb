{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a04a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import adodbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f0d903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Apr-24\", \"May-24\", \"Jun-24\", \"Jul-24\", \"Aug-24\", \"Sep-24\", \"Oct-24\", \"Nov-24\", \"Dec-24\", \"Jan-25\", \"Feb-25\", \"Mar-25\"}\n",
      "Provider=MSOLAP.8;Data Source=powerbi://api.powerbi.com/v1.0/myorg/Edgewell;Initial Catalog=Edgewell US Male Dataset;Timeout=900;\n"
     ]
    }
   ],
   "source": [
    "ManufOrTopC =\"Top Companies\"\n",
    "BrandOrTopB = \"Top Brands\"\n",
    "prodORitem=\"SKU\"\n",
    "\n",
    "client_manuf = [\"Edgewell Personal Care\"]\n",
    "client_brands = [\"Schick\", \"Equate\", \"Cremo\"]\n",
    "\n",
    "categories = [\"Manual Shave Men\"]\n",
    "sectors = [\"System\",\"Disposables\"]\n",
    "segments = [\"Razors\", \"Refills\", \"Disposables\"]\n",
    "subsegments= []\n",
    "subcategories= []\n",
    "\n",
    "decimals = 2\n",
    "sign = \"Before\"\n",
    "currency = '$'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    "\n",
    "customareas=''\n",
    "national = False\n",
    "areas = [\"RETAILER\"]\n",
    "regions_RET  =[\"Bj's And Sam's\",\"Walmart\"]\n",
    "channels_RET = []\n",
    "market_RET = []\n",
    "\n",
    "regions_CHAN = []\n",
    "channels_CHAN = []\n",
    "market_CHAN = []\n",
    "\n",
    "regions_CUST = []\n",
    "channels_CUST = []\n",
    "market_CUST = []\n",
    " \n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending March  2025\"\n",
    "end_date = \"2025-04-01\"\n",
    "years = {2023,2024,2025}\n",
    "\n",
    "past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "past_3_months = pd.date_range(end=end_date, periods=3, freq='ME').strftime('%b-%y').tolist()\n",
    "past_36_months = pd.date_range(end=end_date, periods=36, freq='ME').strftime('%b-%y').tolist()\n",
    "\n",
    "National=[\"NATIONAL\"]if national else []\n",
    "regions = regions_RET + regions_CHAN + regions_CUST\n",
    "channels = channels_RET + channels_CHAN + channels_CUST\n",
    "markets = market_RET + market_CHAN + market_CUST\n",
    "brands_only = True  # Get the Data of SKU Share by brands level only\n",
    "\n",
    "entity_hierarchy = [\n",
    "    (\"Area\",National),\n",
    "    (\"Region\", regions),\n",
    "    (\"Channel\", channels),\n",
    "    (\"Market\", markets)\n",
    "]\n",
    "hierarchy_levels = [\n",
    "    (\"Category\", categories),\n",
    "    (\"Sector\", sectors),\n",
    "    (\"Segment\", segments),\n",
    "    (\"SubSegment\", subsegments),\n",
    "    (\"SubCategory\", subcategories)\n",
    " \n",
    "]\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}\n",
    "server = \"powerbi://api.powerbi.com/v1.0/myorg/Edgewell\"\n",
    "dataset_name = \"Edgewell US Male Dataset\"\n",
    "p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "p3m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_3_months) + \"}\"\n",
    "p36m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_36_months) + \"}\"\n",
    "normalized = True\n",
    "promo_type = True\n",
    "dispaly_share = False  # True if Available\n",
    "feature_share = False\n",
    "\n",
    "print(p12m_dax)\n",
    "path=os.path.join(os.getcwd(),\"Promotion Datasets NewEX\")\n",
    "\n",
    "conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=900;\"\n",
    "print(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f7c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_5108\\1033031194.py:1: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  month_years =  pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n"
     ]
    }
   ],
   "source": [
    "month_years =  pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n",
    "regions = regions_RET + regions_CHAN + regions_CUST\n",
    "channels = channels_RET + channels_CHAN + channels_CUST\n",
    "markets = market_RET + market_CHAN + market_CUST\n",
    "promo_col = []\n",
    "promo_col = promo_col+['Display Share'] if dispaly_share else promo_col\n",
    "promo_col = promo_col+['Feature Share'] if feature_share else promo_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8dabf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580c82",
   "metadata": {},
   "source": [
    "## By Promo Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a84f2b",
   "metadata": {},
   "source": [
    "#### Brands and Promo Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa533ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for System Walmart.\n",
      "Query executed successfully for Razors Bj's And Sam's.\n",
      "Query executed successfully for Disposables Bj's And Sam's.\n",
      "Query executed successfully for System Bj's And Sam's.\n",
      "Query executed successfully for Disposables Bj's And Sam's.\n",
      "Query executed successfully for Refills Bj's And Sam's.\n",
      "Query executed successfully for Manual Shave Men Bj's And Sam's.\n",
      "Query executed successfully for Manual Shave Men Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n",
      "Query executed successfully for Razors Walmart.\n",
      "Query executed successfully for Refills Walmart.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorManuf, entity_name, area, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    columns = [\"Value Share\", \"Promo Sales\"]\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{BrandorManuf}]),\n",
    "                VALUES('Promo Description'[Promo Type])\n",
    "            ),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({p12m_dax}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{area}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    parentdax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[{BrandorManuf}]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({p12m_dax}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{area}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[Category]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({p12m_dax}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{area}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parentdax_query)\n",
    "            pcols = [desc[0] for desc in cursor.description]\n",
    "            pdata = cursor.fetchall()    \n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        pdf = pd.DataFrame(pdata, columns=pcols)\n",
    "        pdf.columns = pdf.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        pdf = pdf.loc[~(pdf.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if pdf.shape[1] > 1:\n",
    "            pdf.iloc[:, 0] = pdf.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if not pdf.empty:\n",
    "            df = pd.concat([df,pdf], ignore_index=True)\n",
    "        dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "        dt = dt[[col for col in dt.columns if col in df.columns]]\n",
    "\n",
    "        missing_cols = [col for col in df.columns if col not in dt.columns]\n",
    "        for col in missing_cols:\n",
    "            dt[col] = pd.NA\n",
    "        dt = dt[df.columns]\n",
    "        dt[df.columns[0]] = 'Grand Total'\n",
    "        \n",
    "        df = pd.concat([df, dt], ignore_index=True)\n",
    "\n",
    "        key = f\"{entity_type} | {entity_name}\"\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "brands_client_dfs = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area, entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query, f\"{BrandOrTopB}\",entity, area, hierby, value))\n",
    "               \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        result=future.result()\n",
    "        brands_client_dfs.update(result)\n",
    "\n",
    "pd.to_pickle(brands_client_dfs, os.path.join(path,\"brands_promo_type.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edfae3",
   "metadata": {},
   "source": [
    "#### By Brands For P12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a4ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Refills Bj's And Sam's.\n",
      "Query executed successfully for Razors Bj's And Sam's.\n",
      "Query executed successfully for Manual Shave Men Bj's And Sam's.\n",
      "Query executed successfully for Disposables Bj's And Sam's.\n",
      "Query executed successfully for Disposables Bj's And Sam's.\n",
      "Query executed successfully for System Bj's And Sam's.\n",
      "Query executed successfully for System Walmart.\n",
      "Query executed successfully for Manual Shave Men Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n",
      "Query executed successfully for Razors Walmart.\n",
      "Query executed successfully for Refills Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    if normalized == True:\n",
    "        columns=[\"Promo Value\", 'VSOD', 'VSOD IYA','Value Share','Promo Share','Value Uplift (v. base) Normalized','Value Uplift Normalized IYA','Volume Uplift (v. Base) Normalized', 'Volume Uplift Normalized IYA'] + promo_col\n",
    "\n",
    "    else:        \n",
    "        columns=[\"Promo Value\", 'VSOD', 'VSOD IYA','Value Share','Promo Share','Value Uplift (v. base)','Value Uplift IYA','Volume Uplift (v. Base)', 'Volume Uplift IYA'] + promo_col\n",
    "\n",
    "\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],  \n",
    "            Products[Total Size],  \n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {p12m_dax},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{area}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    parentdax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],              \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {p12m_dax},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{area}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{hierby}],\n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {p12m_dax},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{area}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parentdax_query)\n",
    "            parentcols = [desc[0] for desc in cursor.description]\n",
    "            parentdata = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        parentdf = pd.DataFrame(parentdata, columns=parentcols)\n",
    "        parentdf.columns = parentdf.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        parentdf = parentdf.loc[~(parentdf.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if parentdf.shape[1] > 1:\n",
    "            parentdf.iloc[:, 0] = parentdf.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "            df = pd.concat([df,parentdf], ignore_index=True)\n",
    "       \n",
    "        dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "        dt[df.columns[0]] = 'Grand Total'\n",
    "        # Keep only columns that exist in df\n",
    "        dt = dt.loc[:, dt.columns.isin(df.columns)]\n",
    "\n",
    "        # Ensure column order matches df\n",
    "        dt = dt.reindex(columns=df.columns)\n",
    "\n",
    "        df = pd.concat([df, dt], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "        # You may need this only if a junk column gets appended:\n",
    "        # df = df.loc[:, df.columns.notna()]\n",
    "\n",
    "        key = f\"{entity_type} | {entity_name}\"\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "promotions_brands_P12M = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area, entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query,entity, area, hierby, value))\n",
    "               \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        result=future.result()\n",
    "        promotions_brands_P12M.update(result)\n",
    "\n",
    "pd.to_pickle(promotions_brands_P12M, os.path.join(path,\"promotions_brands_P12M.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe1f94",
   "metadata": {},
   "source": [
    "# VSOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ad8232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Sector_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Segment_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubSegment_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubCategory_VSOD.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby,direct_parent):\n",
    "    outputdic = {}\n",
    "    key = f\"{categories[0]} | {entity_name}\"\n",
    "    columns = [\n",
    "        \"VSOD\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{hierby}]\n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "            \"VSOD\", COALESCE([VSOD], 0)\n",
    "        ),\n",
    "        TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\" :\n",
    "                    continue\n",
    "            if isinstance(hier_values, list):\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        key = f\"{categories[0]} | {entity}\"\n",
    "                        ordered_keys.append(key)\n",
    "                        future = executor.submit(\n",
    "                            execute_dax_query, entity, area, hierby,direct_parent\n",
    "                        )\n",
    "                        futures[future] = key\n",
    "      \n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"{hierby}_VSOD.pkl\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df5803",
   "metadata": {},
   "source": [
    "# VSOD Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eab97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Sector_client_VSOD.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Segment_client_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubSegment_client_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubCategory_client_VSOD.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Sector_manuf_VSOD.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Segment_manuf_VSOD.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubSegment_manuf_VSOD.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubCategory_manuf_VSOD.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(ManuforBrand,entity_name, area, hierby,direct_parent,client_list):\n",
    "    outputdic = {}\n",
    "    key = f\"{categories[0]} | {entity_name}\"\n",
    "\n",
    "    columns = [\n",
    "        \"VSOD\"\n",
    "    ]\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    client_values_dax = \", \".join([f'\"{c.replace(\"\\\"\", \"\\\"\\\"\")}\"' for c in client_list])\n",
    "    cat_filter = \"\"\n",
    "    if hierby !=\"Sector\":\n",
    "        cat_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Category] = \"{categories[0]}\"\n",
    "            ),\n",
    "        '''    \n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{hierby}],\n",
    "                    Products[{ManuforBrand}]\n",
    "                    \n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            {cat_filter}\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{ManuforBrand}]\n",
    "                                       \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {cat_filter}\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    childtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "              {cat_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "            \"VSOD\", COALESCE([VSOD], 0)\n",
    "        ),\n",
    "        \n",
    "        TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "                    FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "            {cat_filter}\n",
    "              \n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(childtotal_dax_query)\n",
    "            childtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            childtotal_data = cursor.fetchall()         \n",
    "              \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        childtotal_df = pd.DataFrame(childtotal_data, columns=childtotal_columns)\n",
    "        childtotal_df.columns = childtotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        childtotal_df = childtotal_df.loc[~(childtotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "                    \n",
    "        if childtotal_df.shape[1] > 1:\n",
    "            childtotal_df.iloc[:, 0] =  childtotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "            df = pd.concat([df, childtotal_df], ignore_index=True)\n",
    "            \n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(ManuforBrand,entity_hierarchy, hierarchy_levels,direct_parent,client_list):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\" :\n",
    "                    continue\n",
    "            if isinstance(hier_values, list):\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        for client in client_brands:\n",
    "                            key = f\"{categories[0]} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query,ManuforBrand, entity, area, hierby,direct_parent,client_list\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            if ManuforBrand==f'{BrandOrTopB}':\n",
    "                filename = f\"{hierby}_client_VSOD.pkl\"\n",
    "            else:\n",
    "                filename = f\"{hierby}_manuf_VSOD\"\n",
    "                 \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,direct_parent,client_brands)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,direct_parent,client_manuf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0de6d8",
   "metadata": {},
   "source": [
    "# By Products/Item P12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41918a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Refills Bj's And Sam's.\n",
      "Query executed successfully for Manual Shave Men Bj's And Sam's.\n",
      "Query executed successfully for System Bj's And Sam's.\n",
      "Query executed successfully for Razors Bj's And Sam's.\n",
      "Query executed successfully for Disposables Bj's And Sam's.\n",
      "Query executed successfully for Disposables Bj's And Sam's.\n",
      "Query executed successfully for System Walmart.\n",
      "Query executed successfully for Manual Shave Men Walmart.\n",
      "Query executed successfully for Razors Walmart.\n",
      "Query executed successfully for Refills Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    if normalized == True:\n",
    "        columns = [\"Promo Sales\",\"Promo Value\",\"Discount Depth (%)\",\"Promo Share\",\"VSOD\",\"Base Price/Unit\",\"Promo Price/Unit\",\"Gross Margin %\",\"Value Uplift (v. base) Normalized\",'Volume Uplift (v. Base) Normalized',\"Trade Effectiveness\",\"Incr Value\"]\n",
    "    else:        \n",
    "        columns = [\"Promo Sales\",\"Promo Value\",\"Discount Depth (%)\",\"Promo Share\",\"VSOD\",\"Base Price/Unit\",\"Promo Price/Unit\",\"Gross Margin %\",\"Value Uplift (v. base)\",'Volume Uplift (v. Base)',\"Trade Effectiveness\",\"Incr Value\"]\n",
    "\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],  \n",
    "            Products[{prodORitem}],  \n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {p12m_dax},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{area}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[Category]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({p12m_dax}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{area}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "        \n",
    "        dt[df.columns[0]] = 'Grand Total'\n",
    "        for col in df.columns:\n",
    "            if col not in dt.columns:\n",
    "                dt[col] = np.nan\n",
    "\n",
    "        # Reorder columns exactly as in df\n",
    "        dt = dt[df.columns]\n",
    "        df = pd.concat([df, dt], ignore_index=True)\n",
    "\n",
    "        # You may need this only if a junk column gets appended:\n",
    "        # df = df.loc[:, df.columns.notna()]\n",
    "\n",
    "        key = f\"{entity_type} | {entity_name}\"\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "promotions_products_P12M = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area, entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query,entity, area, hierby, value))\n",
    "               \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        result=future.result()\n",
    "        promotions_products_P12M.update(result)\n",
    "\n",
    "pd.to_pickle(promotions_products_P12M, os.path.join(path,\"promotions_products_P12M.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1e35b",
   "metadata": {},
   "source": [
    "# By End of The Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04ee71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\promotions_EndOfWeek.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type,hierby, area, client,manuf):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {client} | {entity_name}\" if client else (f\"{entity_type} | {manuf} | {entity_name}\" if manuf else f\"{entity_name}\")\n",
    "    if normalized == True:\n",
    "        columns = ['Value Uplift (v. base) Normalized','Promo Value','Non Promo Value','Value Sales','Base Sales','VSOD','Promo Volume','Non Promo Volume']\n",
    "    else:\n",
    "        columns = ['Value Uplift (v. base)','Promo Value','Non Promo Value','Value Sales','Base Sales','VSOD','Promo Volume','Non Promo Volume']\n",
    "\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''    \n",
    "    manuf_filter=''\n",
    "    if  manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Calendar,\n",
    "                    Calendar[End of Week]\n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "            \n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        # Rename the column if necessary (optional)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "\n",
    "        # Find the column name for the date column (usually \"End of Week\")\n",
    "        date_col = df.columns[0]  # Assuming the first column is the date\n",
    "        if not df.empty:\n",
    "            df[date_col] = pd.to_datetime(df[date_col]).dt.strftime('%Y-%m-%d')\n",
    "        if not df.empty:\n",
    "            df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0]).dt.date\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            # grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            if client_manuf:\n",
    "                                for manuf in client_manuf:\n",
    "                                    key = f\"{value} | {manuf} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area, '', manuf\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "                            if client_brands:\n",
    "                                for client in client_brands:\n",
    "                                    key = f\"{value} | {client} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area, client, ''\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in future: {e}\")\n",
    "\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "\n",
    "        filename = f\"promotions_EndOfWeek\"\n",
    "        output_file = f\"{path}\\\\{filename}.pkl\"\n",
    "\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "\n",
    "        print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf,client_brands=client_brands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72454168",
   "metadata": {},
   "source": [
    "# Value Uplift vs Discount depth By  Client Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827dd3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0    0.24\n",
      "3    0.62\n",
      "4    0.35\n",
      "5    0.37\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2    0.46\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0     0.58\n",
      "2     0.01\n",
      "3     0.08\n",
      "4     0.43\n",
      "5     0.57\n",
      "6     0.67\n",
      "9     0.11\n",
      "10    0.24\n",
      "11     0.1\n",
      "12     0.3\n",
      "14    0.53\n",
      "15    0.05\n",
      "16    0.08\n",
      "17    0.36\n",
      "18    0.52\n",
      "20    0.06\n",
      "21    0.07\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '23     0.18\n",
      "67     0.12\n",
      "95     0.24\n",
      "119    0.13\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0      0.63\n",
      "1       0.3\n",
      "2       0.0\n",
      "3      0.17\n",
      "6      0.63\n",
      "7       0.7\n",
      "9      0.67\n",
      "10     0.67\n",
      "12     0.66\n",
      "15     0.13\n",
      "16     0.15\n",
      "18      0.0\n",
      "21     0.21\n",
      "22     0.07\n",
      "24     0.11\n",
      "25     0.13\n",
      "26     0.57\n",
      "27     0.14\n",
      "29     0.08\n",
      "32      0.0\n",
      "33     0.16\n",
      "34     0.09\n",
      "35     0.38\n",
      "36      0.4\n",
      "42     0.09\n",
      "44     0.89\n",
      "47     0.29\n",
      "54     0.33\n",
      "57     0.11\n",
      "60     0.89\n",
      "62     0.53\n",
      "63     0.55\n",
      "64     0.62\n",
      "66     0.11\n",
      "68     0.14\n",
      "69     0.44\n",
      "71     0.09\n",
      "74     0.86\n",
      "75     0.32\n",
      "76     0.69\n",
      "77     0.15\n",
      "78     0.58\n",
      "81     0.89\n",
      "83      0.1\n",
      "90     0.85\n",
      "102    0.88\n",
      "106     0.9\n",
      "109     0.0\n",
      "114    0.51\n",
      "115    0.13\n",
      "116    0.22\n",
      "118    0.84\n",
      "128    0.83\n",
      "130     0.0\n",
      "131     0.5\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2    0.46\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0     0.58\n",
      "2     0.57\n",
      "3     0.67\n",
      "6     0.11\n",
      "7     0.24\n",
      "9     0.53\n",
      "10    0.36\n",
      "11    0.07\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0     0.63\n",
      "3      0.7\n",
      "5     0.66\n",
      "7     0.13\n",
      "8      0.0\n",
      "10    0.21\n",
      "11    0.07\n",
      "12    0.13\n",
      "13    0.57\n",
      "14    0.14\n",
      "16     0.0\n",
      "17    0.38\n",
      "22    0.89\n",
      "30    0.89\n",
      "31    0.55\n",
      "32    0.62\n",
      "34    0.14\n",
      "38    0.32\n",
      "39    0.69\n",
      "40    0.58\n",
      "41    0.89\n",
      "44    0.85\n",
      "52    0.88\n",
      "56     0.9\n",
      "60    0.51\n",
      "61    0.22\n",
      "63    0.84\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0    0.24\n",
      "3    0.62\n",
      "4    0.35\n",
      "5    0.37\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '11    0.18\n",
      "33    0.12\n",
      "48    0.24\n",
      "55    0.13\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0      0.3\n",
      "1      0.0\n",
      "2     0.17\n",
      "3     0.63\n",
      "5     0.67\n",
      "6     0.67\n",
      "8     0.15\n",
      "12    0.11\n",
      "14    0.08\n",
      "16    0.16\n",
      "17    0.09\n",
      "18     0.4\n",
      "21    0.09\n",
      "22    0.29\n",
      "26    0.33\n",
      "28    0.11\n",
      "31    0.53\n",
      "32    0.11\n",
      "34    0.44\n",
      "35    0.09\n",
      "36    0.86\n",
      "37    0.15\n",
      "41     0.1\n",
      "51     0.0\n",
      "54    0.13\n",
      "60    0.83\n",
      "61     0.0\n",
      "62     0.5\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0    0.01\n",
      "1    0.08\n",
      "2    0.43\n",
      "3     0.1\n",
      "4     0.3\n",
      "5    0.05\n",
      "6    0.08\n",
      "7    0.52\n",
      "9    0.06\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2    0.46\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1     0.66\n",
      "3     0.13\n",
      "4      0.0\n",
      "6     0.13\n",
      "7     0.57\n",
      "8     0.14\n",
      "10     0.0\n",
      "13    0.89\n",
      "20    0.32\n",
      "28    0.88\n",
      "33    0.51\n",
      "34    0.22\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '4    0.53\n",
      "5    0.07\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0    0.24\n",
      "3    0.35\n",
      "4    0.37\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1      0.7\n",
      "2     0.21\n",
      "3     0.07\n",
      "9     0.89\n",
      "10    0.55\n",
      "11    0.62\n",
      "12    0.14\n",
      "14    0.69\n",
      "15    0.58\n",
      "16    0.89\n",
      "17    0.85\n",
      "20     0.9\n",
      "23    0.84\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0    0.58\n",
      "1    0.57\n",
      "2    0.67\n",
      "3    0.11\n",
      "4    0.24\n",
      "5    0.36\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0    0.62\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '11    0.18\n",
      "33    0.12\n",
      "48    0.24\n",
      "55    0.13\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0    0.01\n",
      "1    0.08\n",
      "2    0.43\n",
      "3     0.1\n",
      "4     0.3\n",
      "5    0.05\n",
      "6    0.08\n",
      "7    0.52\n",
      "9    0.06\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\value_uplift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:111: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0      0.3\n",
      "1      0.0\n",
      "2     0.17\n",
      "3     0.63\n",
      "5     0.67\n",
      "6     0.67\n",
      "8     0.15\n",
      "12    0.11\n",
      "14    0.08\n",
      "16    0.16\n",
      "17    0.09\n",
      "18     0.4\n",
      "21    0.09\n",
      "22    0.29\n",
      "26    0.33\n",
      "28    0.11\n",
      "31    0.53\n",
      "32    0.11\n",
      "34    0.44\n",
      "35    0.09\n",
      "36    0.86\n",
      "37    0.15\n",
      "41     0.1\n",
      "51     0.0\n",
      "54    0.13\n",
      "60    0.83\n",
      "61     0.0\n",
      "62     0.5\n",
      "Name: Discount Depth (%), dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  dp.iloc[:, 1] = dp.iloc[:, 1].astype(str).replace('NaT', '')\n",
      "C:\\Users\\aleaa\\AppData\\Local\\Temp\\ipykernel_19668\\3642822558.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type,hierby, area, client,manuf):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {client} | {entity_name}\" if client else (f\"{entity_type} | {manuf} | {entity_name}\" if manuf else f\"{entity_name}\")\n",
    "    if normalized == True:\n",
    "        columns = ['Discount Depth (%)','Promo Price/Unit','Value Uplift (v. base) Normalized','Promo Sales']\n",
    "    else:\n",
    "        columns = ['Discount Depth (%)','Promo Price/Unit','Value Uplift (v. base)','Promo Sales']\n",
    "\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''    \n",
    "    manuf_filter=''\n",
    "    if manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{prodORitem}]),\n",
    "                VALUES('Calendar'[End of Week])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[{prodORitem}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            \n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_query)\n",
    "            parcol = [desc[0] for desc in cursor.description]\n",
    "            pardata = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        dp = pd.DataFrame(pardata, columns=parcol)\n",
    "        dp.columns = dp.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        dp = dp.loc[~(dp.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if dp.shape[1] > 1:\n",
    "            dp.iloc[:, 0] = dp.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if dp.empty:\n",
    "            outputdic[key] = dp\n",
    "            return outputdic\n",
    "        if not df.empty: \n",
    "            df = pd.concat([df, dp], ignore_index=True)\n",
    "\n",
    "            if not grand_tot.empty and grand_tot.dropna(axis=1, how='all').shape[1] > 0:\n",
    "                grand_tot[df.columns[0]] = 'Grand Total'\n",
    "                for col in df.columns:\n",
    "                    if col not in grand_tot.columns:\n",
    "                        grand_tot[col] = np.nan\n",
    "                grand_tot = grand_tot[df.columns]\n",
    "                if not  df.empty:\n",
    "                    df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df\n",
    "            print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            if client_manuf:\n",
    "                                for manuf in client_manuf:\n",
    "                                    key = f\"{value} | {manuf} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area, '', manuf\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "                            if client_brands:\n",
    "                                for client in client_brands:\n",
    "                                    key = f\"{value} | {client} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area, client, ''\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "    \n",
    "\n",
    "\n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        if client_brands:\n",
    "            filename = f\"value_uplift\"\n",
    "        else:\n",
    "            filename = f\"value_uplift_manuf\"\n",
    "\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames for saved to {output_file}.\")\n",
    "\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands)\n",
    "if client_manuf:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f22135",
   "metadata": {},
   "source": [
    "# Seasonality index dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bf3d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Sector_MonthYear.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Segment_MonthYear.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubSegment_MonthYear.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubCategory_MonthYear.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {entity_name}\"\n",
    "\n",
    "    columns = [\"Value Sales\"]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{hierby}]),\n",
    "                VALUES('Calendar'[MonthYear])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "            \n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "\n",
    "        hierarchy_dict = dict(hierarchy_levels)\n",
    "        for hierby, _ in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    "            dfs_results = {} \n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "            # Get the parent level name (e.g., 'Segment')\n",
    "            parent_level = direct_parent[hierby]\n",
    "\n",
    "            # Get the list of values associated with that parent level\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    "            if isinstance(parent_values, list):\n",
    "                for value in parent_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{value} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query, entity, value, area, hierby\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "            \n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"{hierby}_MonthYear.pkl\"\n",
    "\n",
    "                 \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d09a0",
   "metadata": {},
   "source": [
    "# Seasonality index dic Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "292c1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area,manuf):\n",
    "    outputdic = {}\n",
    "    key =f\"{manuf} | {entity_name}\"\n",
    "    columns = ['Value Sales']\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    manuf_filter=''\n",
    "    if manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[Category]),\n",
    "                VALUES('Calendar'[MonthYear])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy,client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        \n",
    "        if client_manuf:\n",
    "            for area, entity_list in entity_hierarchy:\n",
    "                for entity in entity_list:\n",
    "                    for manuf in client_manuf:\n",
    "                        key = f\"{manuf} | {entity}\" \n",
    "                        ordered_keys.append(key)\n",
    "                        future = executor.submit(\n",
    "                            execute_dax_query, entity, area,manuf,\n",
    "                        )\n",
    "                        futures[future] = key               \n",
    "\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"Category_MonthYear\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "\n",
    "\n",
    "if client_manuf:\n",
    "    process_dax_queries(entity_hierarchy, client_manuf=client_manuf)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e7aa473",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "datasets_path =r\"c:\\Users\\aleaa\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\\\\"\n",
    "datasets = os.listdir(datasets_path)    \n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adaf8d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>End of Week</th>\n",
       "      <th>Discount Depth (%)</th>\n",
       "      <th>Promo Price/Unit</th>\n",
       "      <th>Value Uplift (v. base) Normalized</th>\n",
       "      <th>Promo Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREMO 5 BLADE REFILL NORMAL 4CT</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREMO 5 BLADE RAZOR NORMAL 2CT</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>0.24</td>\n",
       "      <td>13.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREMO 5 BLADE RAZOR NORMAL 2CT</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREMO 5 BLADE RAZOR NORMAL 2CT</td>\n",
       "      <td>2023-01-22</td>\n",
       "      <td>0.54</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREMO 5 BLADE REFILL NORMAL 4CT</td>\n",
       "      <td>2023-01-29</td>\n",
       "      <td>0.13</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>CREMO HOLIDAY GIFT PACK 5 BLADE RAZOR 2CT Total</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.24</td>\n",
       "      <td>11.352506</td>\n",
       "      <td>1.172623</td>\n",
       "      <td>554604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>CREMO 5 BLADE REFILL NORMAL 4CT Total</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.590623</td>\n",
       "      <td>-0.203235</td>\n",
       "      <td>40901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>CREMO 5 BLADE RAZOR NORMAL 2CT Total</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.35</td>\n",
       "      <td>11.452575</td>\n",
       "      <td>-0.187056</td>\n",
       "      <td>29582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>CREMO HERITAGE RED HOLIDAY GIFT PACK REM BLADE...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.37</td>\n",
       "      <td>9.335988</td>\n",
       "      <td>1.946835</td>\n",
       "      <td>456063.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Grand Total</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0.26</td>\n",
       "      <td>10.047676</td>\n",
       "      <td>1.084054</td>\n",
       "      <td>1081150.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   SKU End of Week  \\\n",
       "0                      CREMO 5 BLADE REFILL NORMAL 4CT  2023-01-08   \n",
       "1                       CREMO 5 BLADE RAZOR NORMAL 2CT  2023-01-08   \n",
       "2                       CREMO 5 BLADE RAZOR NORMAL 2CT  2023-01-15   \n",
       "3                       CREMO 5 BLADE RAZOR NORMAL 2CT  2023-01-22   \n",
       "4                      CREMO 5 BLADE REFILL NORMAL 4CT  2023-01-29   \n",
       "..                                                 ...         ...   \n",
       "263    CREMO HOLIDAY GIFT PACK 5 BLADE RAZOR 2CT Total         NaT   \n",
       "264              CREMO 5 BLADE REFILL NORMAL 4CT Total         NaT   \n",
       "265               CREMO 5 BLADE RAZOR NORMAL 2CT Total         NaT   \n",
       "266  CREMO HERITAGE RED HOLIDAY GIFT PACK REM BLADE...         NaT   \n",
       "267                                        Grand Total         NaT   \n",
       "\n",
       "    Discount Depth (%)  Promo Price/Unit  Value Uplift (v. base) Normalized  \\\n",
       "0                 0.03         14.500000                           0.000000   \n",
       "1                 0.24         13.733333                           0.000000   \n",
       "2                 0.22         14.000000                           0.000000   \n",
       "3                 0.54          8.333333                           0.000000   \n",
       "4                 0.13         13.000000                           0.000000   \n",
       "..                 ...               ...                                ...   \n",
       "263               0.24         11.352506                           1.172623   \n",
       "264               0.62          5.590623                          -0.203235   \n",
       "265               0.35         11.452575                          -0.187056   \n",
       "266               0.37          9.335988                           1.946835   \n",
       "267               0.26         10.047676                           1.084054   \n",
       "\n",
       "     Promo Sales  \n",
       "0           87.0  \n",
       "1          206.0  \n",
       "2           84.0  \n",
       "3          125.0  \n",
       "4           13.0  \n",
       "..           ...  \n",
       "263     554604.0  \n",
       "264      40901.0  \n",
       "265      29582.0  \n",
       "266     456063.0  \n",
       "267    1081150.0  \n",
       "\n",
       "[268 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_uplift['Manual Shave Men | Cremo | Walmart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecca619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: Wed Aug  6 10:18:41 2025\n",
      "Script ended at: Wed Aug  6 10:22:15 2025\n",
      "Elapsed time: 213.90 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Script started at: {time.ctime(start_time)}\")\n",
    "print(f\"Script ended at: {time.ctime(end_time)}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
