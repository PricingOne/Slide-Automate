{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a04a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import adodbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f0d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "ManufOrTopC =\"Top Companies\"\n",
    "BrandOrTopB = \"Top Brands\"\n",
    "prodORitem=\"Business Name\"\n",
    "\n",
    "client_manuf = [\"Bel\"]\n",
    "client_brands = [\"Kiri\", \"La Vache Qui Rit\", \"Boursin\"]\n",
    "\n",
    "categories = [\"Total Fromage\"]\n",
    "sectors = [\"Soft Cheese\", \"Aperitif\"]\n",
    "segments = [\"Enfant\", \"Frais A Tartiner\", \"Salade\"]\n",
    "subsegments= []\n",
    "subcategories= []\n",
    "\n",
    "decimals = 2\n",
    "sign = \"After\"\n",
    "currency = 'â‚¬'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    "\n",
    "customareas=''\n",
    "national = False\n",
    "areas = [\"RETAILER\"]\n",
    "regions_RET  =[\"Carrefour\", \"Intermarche\"]\n",
    "channels_RET = [\"Carrefour Hyper + Drive\", \"Carrefour Supermarket + Drive\", \"Carrefour Proximite\", \"Intermarche Super\", \"Intermarche Hyper\", \"Intermarche Proxi\"]\n",
    "market_RET = []\n",
    "\n",
    "regions_CHAN = []\n",
    "channels_CHAN = []\n",
    "market_CHAN = []\n",
    "\n",
    "regions_CUST = []\n",
    "channels_CUST = []\n",
    "market_CUST = []\n",
    "\n",
    " \n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | July 2025\"\n",
    "end_date = \"2025-08-01\"\n",
    "years = {2023,2024,2025}\n",
    "\n",
    "past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "past_3_months = pd.date_range(end=end_date, periods=3, freq='ME').strftime('%b-%y').tolist()\n",
    "past_6_months = pd.date_range(end=end_date, periods=6, freq='ME').strftime('%b-%y').tolist()\n",
    "past_36_months = pd.date_range(end=end_date, periods=36, freq='ME').strftime('%b-%y').tolist()\n",
    "\n",
    " \n",
    "National=[\"NATIONAL\"]if national else []\n",
    "entity_hierarchy = [\n",
    "    (\"NATIONAL\",\"Area\",National),\n",
    "    (\"RETAILER\",\"Region\", regions_RET),\n",
    "    (\"RETAILER\",\"Channel\", channels_RET),\n",
    "    (\"RETAILER\",\"Market\", market_RET),\n",
    "    \n",
    "    (\"CHANNEL\",\"Region\", regions_CHAN),\n",
    "    (\"CHANNEL\",\"Channel\", channels_CHAN),\n",
    "    (\"CHANNEL\",\"Market\", market_CHAN),\n",
    "    \n",
    "    (f\"{customareas}\",\"Region\", regions_CUST),\n",
    "    (f\"{customareas}\",\"Channel\", channels_CUST),\n",
    "    (f\"{customareas}\",\"Market\", market_CUST)\n",
    "]\n",
    "\n",
    "hierarchy_levels = [\n",
    "    (\"Category\", categories),\n",
    "    (\"Sector\", sectors),\n",
    "    (\"Segment\", segments),\n",
    "    (\"SubSegment\", subsegments),\n",
    "    (\"SubCategory\", subcategories)\n",
    " \n",
    "]\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}\n",
    "\n",
    "server = \"powerbi://api.powerbi.com/v1.0/myorg/Groupe Bel\"\n",
    "dataset_name = \"Bel France Dataset\"\n",
    "\n",
    "p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "p3m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_3_months) + \"}\"\n",
    "p6m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_6_months) + \"}\"\n",
    "p36m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_36_months) + \"}\"\n",
    "\n",
    "slides_Period=\"P3M\"\n",
    "period= p12m_dax if slides_Period==\"P12M\" else p3m_dax if slides_Period==\"P3M\" else p6m_dax if slides_Period==\"P6M\" else p36m_dax\n",
    "\n",
    "normalized = True\n",
    "promo_type = False\n",
    "dispaly_share = True  # True if Available\n",
    "feature_share = True\n",
    "\n",
    "path=os.path.join(os.getcwd(),\"Promotion Datasets NewEX\")\n",
    "conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=300;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f7c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\656077122.py:5: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  month_years =  pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n"
     ]
    }
   ],
   "source": [
    "entity_hierarchy = [\n",
    "    item for item in entity_hierarchy\n",
    "    if len(item) == 3 and item[2]  # item[2] is the entity list\n",
    "]\n",
    "month_years =  pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n",
    "regions = regions_RET + regions_CHAN + regions_CUST\n",
    "channels = channels_RET + channels_CHAN + channels_CUST\n",
    "markets = market_RET + market_CHAN + market_CUST\n",
    "promo_col = []\n",
    "promo_col = promo_col+['Display Share'] if dispaly_share else promo_col\n",
    "promo_col = promo_col+['Feature Share'] if feature_share else promo_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8dabf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580c82",
   "metadata": {},
   "source": [
    "## By Promo Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a84f2b",
   "metadata": {},
   "source": [
    "#### Brands and Promo Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa533ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Total Fromage Intermarche.\n",
      "Query executed successfully for Aperitif Carrefour.\n",
      "Query executed successfully for Salade Carrefour.\n",
      "Query executed successfully for Enfant Carrefour.\n",
      "Query executed successfully for Frais A Tartiner Carrefour.\n",
      "Query executed successfully for Total Fromage Carrefour.\n",
      "Query executed successfully for Soft Cheese Carrefour.\n",
      "Query executed successfully for Soft Cheese Intermarche.\n",
      "Query executed successfully for Aperitif Intermarche.\n",
      "Query executed successfully for Enfant Intermarche.\n",
      "Query executed successfully for Frais A Tartiner Intermarche.\n",
      "Query executed successfully for Salade Intermarche.\n",
      "Query executed successfully for Enfant Carrefour Hyper + Drive.\n",
      "Query executed successfully for Total Fromage Carrefour Hyper + Drive.\n",
      "Query executed successfully for Salade Carrefour Hyper + Drive.\n",
      "Query executed successfully for Total Fromage Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Aperitif Carrefour Hyper + Drive.\n",
      "Query executed successfully for Soft Cheese Carrefour Hyper + Drive.\n",
      "Query executed successfully for Soft Cheese Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Hyper + Drive.\n",
      "Query executed successfully for Aperitif Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Enfant Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Total Fromage Carrefour Proximite.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Salade Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Soft Cheese Carrefour Proximite.\n",
      "Query executed successfully for Aperitif Carrefour Proximite.\n",
      "Query executed successfully for Enfant Carrefour Proximite.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Proximite.\n",
      "Query executed successfully for Salade Carrefour Proximite.\n",
      "Query executed successfully for Soft Cheese Intermarche Super.\n",
      "Query executed successfully for Enfant Intermarche Super.\n",
      "Query executed successfully for Aperitif Intermarche Super.\n",
      "Query executed successfully for Total Fromage Intermarche Super.\n",
      "Query executed successfully for Salade Intermarche Super.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Super.\n",
      "Query executed successfully for Total Fromage Intermarche Hyper.\n",
      "Query executed successfully for Soft Cheese Intermarche Hyper.\n",
      "Query executed successfully for Aperitif Intermarche Hyper.\n",
      "Query executed successfully for Enfant Intermarche Hyper.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Hyper.\n",
      "Query executed successfully for Salade Intermarche Hyper.\n",
      "Query executed successfully for Total Fromage Intermarche Proxi.\n",
      "Query executed successfully for Soft Cheese Intermarche Proxi.\n",
      "Query executed successfully for Aperitif Intermarche Proxi.\n",
      "Query executed successfully for Enfant Intermarche Proxi.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Proxi.\n",
      "Query executed successfully for Salade Intermarche Proxi.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorManuf, entity_name, area,market, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    columns = [\"Value Share\", \"Promo Sales\"]\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{BrandorManuf}]),\n",
    "                VALUES('Promo Description'[Promo Type])\n",
    "            ),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "        TREATAS({period}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{market}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    parentdax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[{BrandorManuf}]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({period}, 'Calendar'[MonthYear]),\n",
    "        FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{market}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[Category]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({period}, 'Calendar'[MonthYear]),\n",
    "        FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{market}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parentdax_query)\n",
    "            pcols = [desc[0] for desc in cursor.description]\n",
    "            pdata = cursor.fetchall()    \n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        pdf = pd.DataFrame(pdata, columns=pcols)\n",
    "        pdf.columns = pdf.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        pdf = pdf.loc[~(pdf.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if not df.empty:\n",
    "            if pdf.shape[1] > 1:\n",
    "                pdf.iloc[:, 0] = pdf.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "            if not pdf.empty:\n",
    "                df = pd.concat([df,pdf], ignore_index=True)\n",
    "            dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "            dt = dt[[col for col in dt.columns if col in df.columns]]\n",
    "\n",
    "            missing_cols = [col for col in df.columns if col not in dt.columns]\n",
    "            for col in missing_cols:\n",
    "                dt[col] = pd.NA\n",
    "            dt = dt[df.columns]\n",
    "            dt[df.columns[0]] = 'Grand Total'\n",
    "            \n",
    "            df = pd.concat([df, dt], ignore_index=True)\n",
    "\n",
    "            key = f\"{entity_type} | {entity_name}\"\n",
    "            outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "brands_client_dfs = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area,market,entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query, f\"{BrandOrTopB}\",entity,area, market, hierby, value))\n",
    "                \n",
    "        # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            result=future.result()\n",
    "            brands_client_dfs.update(result)\n",
    "\n",
    "pd.to_pickle(brands_client_dfs, os.path.join(path,\"brands_promo_type.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edfae3",
   "metadata": {},
   "source": [
    "#### By Brands For P12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a4ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Enfant Carrefour.\n",
      "Query executed successfully for Aperitif Carrefour.\n",
      "Query executed successfully for Frais A Tartiner Carrefour.\n",
      "Query executed successfully for Salade Carrefour.\n",
      "Query executed successfully for Total Fromage Intermarche.\n",
      "Query executed successfully for Soft Cheese Intermarche.\n",
      "Query executed successfully for Total Fromage Carrefour.\n",
      "Query executed successfully for Soft Cheese Carrefour.\n",
      "Query executed successfully for Aperitif Intermarche.\n",
      "Query executed successfully for Enfant Intermarche.\n",
      "Query executed successfully for Frais A Tartiner Intermarche.\n",
      "Query executed successfully for Salade Intermarche.\n",
      "Query executed successfully for Enfant Carrefour Hyper + Drive.\n",
      "Query executed successfully for Aperitif Carrefour Hyper + Drive.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Hyper + Drive.\n",
      "Query executed successfully for Salade Carrefour Hyper + Drive.\n",
      "Query executed successfully for Total Fromage Carrefour Hyper + Drive.\n",
      "Query executed successfully for Total Fromage Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Soft Cheese Carrefour Hyper + Drive.\n",
      "Query executed successfully for Soft Cheese Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Enfant Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Aperitif Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Salade Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Aperitif Carrefour Proximite.\n",
      "Query executed successfully for Enfant Carrefour Proximite.\n",
      "Query executed successfully for Total Fromage Carrefour Proximite.\n",
      "Query executed successfully for Soft Cheese Carrefour Proximite.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Proximite.\n",
      "Query executed successfully for Salade Carrefour Proximite.\n",
      "Query executed successfully for Total Fromage Intermarche Super.\n",
      "Query executed successfully for Soft Cheese Intermarche Super.\n",
      "Query executed successfully for Aperitif Intermarche Super.\n",
      "Query executed successfully for Enfant Intermarche Super.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Super.\n",
      "Query executed successfully for Salade Intermarche Super.\n",
      "Query executed successfully for Total Fromage Intermarche Hyper.\n",
      "Query executed successfully for Aperitif Intermarche Hyper.\n",
      "Query executed successfully for Soft Cheese Intermarche Hyper.\n",
      "Query executed successfully for Enfant Intermarche Hyper.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Hyper.\n",
      "Query executed successfully for Salade Intermarche Hyper.\n",
      "Query executed successfully for Total Fromage Intermarche Proxi.\n",
      "Query executed successfully for Soft Cheese Intermarche Proxi.\n",
      "Query executed successfully for Aperitif Intermarche Proxi.\n",
      "Query executed successfully for Enfant Intermarche Proxi.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Proxi.\n",
      "Query executed successfully for Salade Intermarche Proxi.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area,market, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    if normalized == True:\n",
    "        columns=[\"Promo Value\", 'VSOD', 'VSOD IYA','Value Share','Promo Share','Value Uplift (v. base) Normalized','Value Uplift Normalized IYA','Volume Uplift (v. Base) Normalized', 'Volume Uplift Normalized IYA'] + promo_col\n",
    "\n",
    "    else:        \n",
    "        columns=[\"Promo Value\", 'VSOD', 'VSOD IYA','Value Share','Promo Share','Value Uplift (v. base)','Value Uplift IYA','Volume Uplift (v. Base)', 'Volume Uplift IYA'] + promo_col\n",
    "\n",
    "\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],  \n",
    "            Products[Total Size],  \n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {period},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{market}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"     \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    parentdax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],              \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {period},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{market}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{hierby}],\n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {period},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "        \n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{market}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"Category\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parentdax_query)\n",
    "            parentcols = [desc[0] for desc in cursor.description]\n",
    "            parentdata = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if not df.empty: \n",
    "            parentdf = pd.DataFrame(parentdata, columns=parentcols)\n",
    "            parentdf.columns = parentdf.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            parentdf = parentdf.loc[~(parentdf.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            if parentdf.shape[1] > 1:\n",
    "                parentdf.iloc[:, 0] = parentdf.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "                df = pd.concat([df,parentdf], ignore_index=True)\n",
    "        \n",
    "            dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "            dt[df.columns[0]] = 'Grand Total'\n",
    "            # Keep only columns that exist in df\n",
    "            dt = dt.loc[:, dt.columns.isin(df.columns)]\n",
    "\n",
    "            # Ensure column order matches df\n",
    "            dt = dt.reindex(columns=df.columns)\n",
    "\n",
    "            df = pd.concat([df, dt], ignore_index=True)\n",
    "\n",
    "            key = f\"{entity_type} | {entity_name}\"\n",
    "            outputdic[key] = df\n",
    "            print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "promotions_brands_P12M = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area,market, entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query,entity, area,market, hierby, value))\n",
    "                \n",
    "    # Wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            result=future.result()\n",
    "            promotions_brands_P12M.update(result)\n",
    "\n",
    "pd.to_pickle(promotions_brands_P12M, os.path.join(path,f\"promotions_brands_{slides_Period}.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe1f94",
   "metadata": {},
   "source": [
    "# VSOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ad8232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour.\n",
      "All DataFrames for Sector saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Sector_VSOD.pkl.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "All DataFrames for Segment saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Segment_VSOD.pkl.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubSegment_VSOD.pkl.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubCategory_VSOD.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area,market, hierby,direct_parent):\n",
    "    outputdic = {}\n",
    "    key = f\"{categories[0]} | {entity_name}\"\n",
    "    columns = [\n",
    "        \"VSOD\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{hierby}]\n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({period}, Calendar[MonthYear]),\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({period}, Calendar[MonthYear]),\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "            \"VSOD\", COALESCE([VSOD], 0)\n",
    "        ),\n",
    "        TREATAS({period}, Calendar[MonthYear]),\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}])\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\" :\n",
    "                    continue\n",
    "            if isinstance(hier_values, list):\n",
    "                for area,market, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        key = f\"{categories[0]} | {entity}\"\n",
    "                        ordered_keys.append(key)\n",
    "                        future = executor.submit(\n",
    "                            execute_dax_query, entity, area,market, hierby,direct_parent\n",
    "                        )\n",
    "                        futures[future] = key\n",
    "      \n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"{hierby}_VSOD.pkl\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df5803",
   "metadata": {},
   "source": [
    "# VSOD Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eab97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for Sector saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Sector_client_VSOD.pkl.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for Segment saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Segment_client_VSOD.pkl.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubSegment_client_VSOD.pkl.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubCategory_client_VSOD.pkl.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for Sector saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Sector_manuf_VSOD.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for Segment saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Segment_manuf_VSOD.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubSegment_manuf_VSOD.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubCategory_manuf_VSOD.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(ManuforBrand,entity_name, area,market, hierby,direct_parent,client_list):\n",
    "    outputdic = {}\n",
    "    key = f\"{categories[0]} | {entity_name}\"\n",
    "\n",
    "    columns = [\n",
    "        \"VSOD\"\n",
    "    ]\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    client_values_dax = \", \".join([f'\"{c.replace(\"\\\"\", \"\\\"\\\"\")}\"' for c in client_list])\n",
    "    cat_filter = \"\"\n",
    "    if hierby !=\"Sector\":\n",
    "        cat_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Category] = \"{categories[0]}\"\n",
    "            ),\n",
    "        '''    \n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{hierby}],\n",
    "                    Products[{ManuforBrand}]\n",
    "                      \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({period}, Calendar[MonthYear]),\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            {cat_filter}\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{ManuforBrand}]\n",
    "                                       \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({period}, Calendar[MonthYear]),\n",
    "            {cat_filter}\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    childtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({period}, Calendar[MonthYear]),\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "              {cat_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"), \n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "            \"VSOD\", COALESCE([VSOD], 0)\n",
    "        ),\n",
    "        \n",
    "        TREATAS({period}, Calendar[MonthYear]),\n",
    "                    FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "            {cat_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}])\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(childtotal_dax_query)\n",
    "            childtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            childtotal_data = cursor.fetchall()         \n",
    "              \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        childtotal_df = pd.DataFrame(childtotal_data, columns=childtotal_columns)\n",
    "        childtotal_df.columns = childtotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        childtotal_df = childtotal_df.loc[~(childtotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "                    \n",
    "        if childtotal_df.shape[1] > 1:\n",
    "            childtotal_df.iloc[:, 0] =  childtotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "            df = pd.concat([df, childtotal_df], ignore_index=True)\n",
    "            \n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(ManuforBrand,entity_hierarchy, hierarchy_levels,direct_parent,client_list):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\" :\n",
    "                    continue\n",
    "            if isinstance(hier_values, list):\n",
    "                for area,market, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        for client in client_brands:\n",
    "                            key = f\"{categories[0]} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query,ManuforBrand, entity, area,market, hierby,direct_parent,client_list\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            if ManuforBrand==f'{BrandOrTopB}':\n",
    "                filename = f\"{hierby}_client_VSOD.pkl\"\n",
    "            else:\n",
    "                filename = f\"{hierby}_manuf_VSOD\"\n",
    "                 \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,direct_parent,client_brands)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,direct_parent,client_manuf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0de6d8",
   "metadata": {},
   "source": [
    "# By Products/Item P12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41918a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Enfant Carrefour.\n",
      "Query executed successfully for Aperitif Carrefour.\n",
      "Query executed successfully for Frais A Tartiner Carrefour.\n",
      "Query executed successfully for Salade Carrefour.\n",
      "Query executed successfully for Total Fromage Intermarche.\n",
      "Query executed successfully for Soft Cheese Intermarche.\n",
      "Query executed successfully for Total Fromage Carrefour.\n",
      "Query executed successfully for Soft Cheese Carrefour.\n",
      "Query executed successfully for Aperitif Intermarche.\n",
      "Query executed successfully for Enfant Intermarche.\n",
      "Query executed successfully for Frais A Tartiner Intermarche.\n",
      "Query executed successfully for Salade Intermarche.\n",
      "Query executed successfully for Aperitif Carrefour Hyper + Drive.\n",
      "Query executed successfully for Total Fromage Carrefour Hyper + Drive.\n",
      "Query executed successfully for Enfant Carrefour Hyper + Drive.\n",
      "Query executed successfully for Soft Cheese Carrefour Hyper + Drive.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Hyper + Drive.\n",
      "Query executed successfully for Salade Carrefour Hyper + Drive.\n",
      "Query executed successfully for Enfant Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Total Fromage Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Aperitif Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Soft Cheese Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Salade Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Total Fromage Carrefour Proximite.\n",
      "Query executed successfully for Soft Cheese Carrefour Proximite.\n",
      "Query executed successfully for Aperitif Carrefour Proximite.\n",
      "Query executed successfully for Enfant Carrefour Proximite.\n",
      "Query executed successfully for Frais A Tartiner Carrefour Proximite.\n",
      "Query executed successfully for Salade Carrefour Proximite.\n",
      "Query executed successfully for Total Fromage Intermarche Super.\n",
      "Query executed successfully for Aperitif Intermarche Super.\n",
      "Query executed successfully for Soft Cheese Intermarche Super.\n",
      "Query executed successfully for Enfant Intermarche Super.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Super.\n",
      "Query executed successfully for Salade Intermarche Super.\n",
      "Query executed successfully for Total Fromage Intermarche Hyper.\n",
      "Query executed successfully for Soft Cheese Intermarche Hyper.\n",
      "Query executed successfully for Aperitif Intermarche Hyper.\n",
      "Query executed successfully for Enfant Intermarche Hyper.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Hyper.\n",
      "Query executed successfully for Salade Intermarche Hyper.\n",
      "Query executed successfully for Aperitif Intermarche Proxi.\n",
      "Query executed successfully for Total Fromage Intermarche Proxi.\n",
      "Query executed successfully for Soft Cheese Intermarche Proxi.\n",
      "Query executed successfully for Enfant Intermarche Proxi.\n",
      "Query executed successfully for Frais A Tartiner Intermarche Proxi.\n",
      "Query executed successfully for Salade Intermarche Proxi.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area,market, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    if normalized == True:\n",
    "        columns = [\"Promo Sales\",\"Promo Value\",\"Discount Depth (%)\",\"Promo Share\",\"VSOD\",\"Base Price/Unit\",\"Promo Price/Unit\",\"Gross Margin %\",\"Value Uplift (v. base) Normalized\",'Volume Uplift (v. Base) Normalized',\"Trade Effectiveness\",\"Incr Value\"]\n",
    "    else:        \n",
    "        columns = [\"Promo Sales\",\"Promo Value\",\"Discount Depth (%)\",\"Promo Share\",\"VSOD\",\"Base Price/Unit\",\"Promo Price/Unit\",\"Gross Margin %\",\"Value Uplift (v. base)\",'Volume Uplift (v. Base)',\"Trade Effectiveness\",\"Incr Value\"]\n",
    "\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],  \n",
    "            Products[{prodORitem}],  \n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {period},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{market}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[Category]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({period}, 'Calendar'[MonthYear]),\n",
    "        FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{market}])\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "        \n",
    "        dt[df.columns[0]] = 'Grand Total'\n",
    "        for col in df.columns:\n",
    "            if col not in dt.columns:\n",
    "                dt[col] = np.nan\n",
    "\n",
    "        dt = dt[df.columns]\n",
    "        df = pd.concat([df, dt], ignore_index=True)\n",
    "        key = f\"{entity_type} | {entity_name}\"\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "promotions_products_P12M = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area,market, entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query,entity, area,market, hierby, value))\n",
    "               \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        result=future.result()\n",
    "        promotions_products_P12M.update(result)\n",
    "\n",
    "pd.to_pickle(promotions_products_P12M, os.path.join(path,f\"promotions_products_{slides_Period}.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1e35b",
   "metadata": {},
   "source": [
    "# By End of The Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ee71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\promotions_EndOfWeek.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type,hierby, area,market, client,manuf):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {client} | {entity_name}\" if client else (f\"{entity_type} | {manuf} | {entity_name}\" if manuf else f\"{entity_name}\")\n",
    "    if normalized == True:\n",
    "        columns = ['Value Uplift (v. base) Normalized','Promo Value','Non Promo Value','Value Sales','Base Sales','VSOD','Discount Depth (%)','Promo Volume','Non Promo Volume']\n",
    "    else:\n",
    "        columns = ['Value Uplift (v. base)','Promo Value','Non Promo Value','Value Sales','Base Sales','VSOD','Discount Depth (%)','Promo Volume','Non Promo Volume']\n",
    "\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''    \n",
    "    manuf_filter=''\n",
    "    if  manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Calendar,\n",
    "                    Calendar[End of Week]\n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "            \n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        # Rename the column if necessary (optional)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "\n",
    "        # Find the column name for the date column (usually \"End of Week\")\n",
    "        date_col = df.columns[0]  # Assuming the first column is the date\n",
    "        if not df.empty:\n",
    "            df[date_col] = pd.to_datetime(df[date_col]).dt.strftime('%Y-%m-%d')\n",
    "        if not df.empty:\n",
    "            df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0]).dt.date\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            # grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area,market, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            if client_manuf:\n",
    "                                for manuf in client_manuf:\n",
    "                                    key = f\"{value} | {manuf} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area,market, '', manuf\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "                            if client_brands:\n",
    "                                for client in client_brands:\n",
    "                                    key = f\"{value} | {client} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area,market, client, ''\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in future: {e}\")\n",
    "\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "\n",
    "        filename = f\"promotions_EndOfWeek\"\n",
    "        output_file = f\"{path}\\\\{filename}.pkl\"\n",
    "\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "\n",
    "        print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf,client_brands=client_brands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72454168",
   "metadata": {},
   "source": [
    "# Value Uplift vs Discount depth By  Client Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "827dd3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Proximite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Proximite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Proximite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\value_uplift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Proximite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Proximite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche Hyper.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche Super.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Carrefour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\BW4SA\\AppData\\Local\\Temp\\ipykernel_11748\\787351652.py:130: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "All DataFrames for saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\value_uplift_manuf.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type,hierby, area,market, client,manuf):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {client} | {entity_name}\" if client else (f\"{entity_type} | {manuf} | {entity_name}\" if manuf else f\"{entity_name}\")\n",
    "    if normalized == True:\n",
    "        columns = ['Discount Depth (%)','Promo Price/Unit','Value Uplift (v. base) Normalized','Promo Sales']\n",
    "    else:\n",
    "        columns = ['Discount Depth (%)','Promo Price/Unit','Value Uplift (v. base)','Promo Sales']\n",
    "\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''    \n",
    "    manuf_filter=''\n",
    "    if manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{prodORitem}]),\n",
    "                VALUES('Calendar'[End of Week])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[{prodORitem}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}])\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            \n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_query)\n",
    "            parcol = [desc[0] for desc in cursor.description]\n",
    "            pardata = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        dp = pd.DataFrame(pardata, columns=parcol)\n",
    "        dp.columns = dp.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        dp = dp.loc[~(dp.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if dp.shape[1] > 1:\n",
    "            dp.iloc[:, 0] = dp.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if dp.empty:\n",
    "            outputdic[key] = dp\n",
    "            return outputdic\n",
    "        if not df.empty: \n",
    "            df = pd.concat([df, dp], ignore_index=True)\n",
    "\n",
    "        if not grand_tot.empty and grand_tot.dropna(axis=1, how='all').shape[1] > 0:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "\n",
    "            # Final safety check to avoid warning\n",
    "            if not grand_tot.dropna(how=\"all\").empty:\n",
    "                df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df\n",
    "            print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area,market, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            if client_manuf:\n",
    "                                for manuf in client_manuf:\n",
    "                                    key = f\"{value} | {manuf} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area,market, '', manuf\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "                            if client_brands:\n",
    "                                for client in client_brands:\n",
    "                                    key = f\"{value} | {client} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area,market, client, ''\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "    \n",
    "\n",
    "\n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        if client_brands:\n",
    "            filename = f\"value_uplift\"\n",
    "        else:\n",
    "            filename = f\"value_uplift_manuf\"\n",
    "\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames for saved to {output_file}.\")\n",
    "\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands)\n",
    "if client_manuf:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f22135",
   "metadata": {},
   "source": [
    "# Seasonality index dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61bf3d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "All DataFrames for Sector saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Sector_MonthYear.pkl.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "All DataFrames for Segment saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\Segment_MonthYear.pkl.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubSegment_MonthYear.pkl.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Intermarche Proxi.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate\\Promotion Datasets NewEX\\SubCategory_MonthYear.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type, area,market, hierby):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {entity_name}\"\n",
    "\n",
    "    columns = [\"Value Sales\"]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{hierby}]),\n",
    "                VALUES('Calendar'[MonthYear])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "            \n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "\n",
    "        hierarchy_dict = dict(hierarchy_levels)\n",
    "        for hierby, _ in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    "            dfs_results = {} \n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "            # Get the parent level name (e.g., 'Segment')\n",
    "            parent_level = direct_parent[hierby]\n",
    "\n",
    "            # Get the list of values associated with that parent level\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    "            if isinstance(parent_values, list):\n",
    "                for value in parent_values:\n",
    "                    for area,market, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{value} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query, entity, value, area,market, hierby\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "            \n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"{hierby}_MonthYear.pkl\"\n",
    "\n",
    "                 \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d09a0",
   "metadata": {},
   "source": [
    "# Seasonality index dic Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "292c1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Intermarche Hyper.\n",
      "Query executed successfully for Carrefour Proximite.\n",
      "Query executed successfully for Intermarche Super.\n",
      "Query executed successfully for Intermarche.\n",
      "Query executed successfully for Carrefour Supermarket + Drive.\n",
      "Query executed successfully for Carrefour Hyper + Drive.\n",
      "Query executed successfully for Carrefour.\n",
      "Query executed successfully for Intermarche Proxi.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area,market,manuf):\n",
    "    outputdic = {}\n",
    "    key =f\"{manuf} | {entity_name}\"\n",
    "    columns = ['Value Sales']\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    manuf_filter=''\n",
    "    if manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[Category]),\n",
    "                VALUES('Calendar'[MonthYear])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            FILTER('Market', 'Market'[Area] = \"{area}\"),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{market}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy,client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        \n",
    "        if client_manuf:\n",
    "            for area,market, entity_list in entity_hierarchy:\n",
    "                for entity in entity_list:\n",
    "                    for manuf in client_manuf:\n",
    "                        key = f\"{manuf} | {entity}\" \n",
    "                        ordered_keys.append(key)\n",
    "                        future = executor.submit(\n",
    "                            execute_dax_query, entity, area,market,manuf,\n",
    "                        )\n",
    "                        futures[future] = key               \n",
    "\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"Category_MonthYear\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "\n",
    "\n",
    "if client_manuf:\n",
    "    process_dax_queries(entity_hierarchy, client_manuf=client_manuf)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecca619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: Sun Sep 14 16:42:28 2025\n",
      "Script ended at: Sun Sep 14 16:55:35 2025\n",
      "Elapsed time: 786.70 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Script started at: {time.ctime(start_time)}\")\n",
    "print(f\"Script ended at: {time.ctime(end_time)}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
