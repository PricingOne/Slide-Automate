{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliha\\AppData\\Local\\Temp\\ipykernel_23140\\3742693690.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "%run \"..\\general_functions\\generalFunctions.ipynb\"\n",
    "%run \"..\\Promotion Slide Duplicate\\Promotion Replacement Function.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_type = True\n",
    "feature_share = False\n",
    "display_share = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_manuf = [\"Ontex\"]\n",
    "client_brands = ['Dada']\n",
    "currency = 'PLN'\n",
    "decimals = 2\n",
    "sign = \"before\"\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' ' \n",
    "\n",
    "period = 'P3M'\n",
    "prodORitem = 'Product'\n",
    "percent = 100000\n",
    "percentstr=\"'00 000\"\n",
    "ValueCutOff = 1000\n",
    "\n",
    "categories = [\"Baby Diapers/Pants\"]\n",
    "sectors = [\"Baby Diapers\",\"Baby Pants\"]\n",
    "segments = [\"Newborn\", \"Mini\",\"Midi\", \"Maxi\", \"Junior\", \"Extra Large\", \"Extra Extra Large\"]\n",
    "subsegments= []\n",
    "subcategories= []\n",
    "\n",
    "sectorsegments = [\"\"]\n",
    "sectorInScope = sectors\n",
    "segmentInScope = segments\n",
    "\n",
    "customareas= \"RETAILER REGION\"\n",
    "areas = ['NATIONAL', \"RETAILER\",customareas]#, \"CHANNEL\"\n",
    "\n",
    "national = True\n",
    "\n",
    "regions_RET  =  [\"Carrefour\"]\n",
    "channels_RET = [\"Carrefour Hypermarket\",\"Carrefour Supermarket\"]\n",
    "market_RET = []\n",
    "regions_CHAN = [] \n",
    "channels_CHAN = []\n",
    "market_CHAN = [\"Enseignes Hm\",\"Enseignes Sm\",\"Enseignes Drive\"]\n",
    "regions_POS = []\n",
    "channels_POS = []\n",
    "market_POS = []\n",
    "\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending May 2024\"\n",
    "years = ['2021', '2022','2023']\n",
    "brackets = ['[Products].[Size Bracket]','[Base Price Bracket].[Base Price\\xa0Bracket]']\n",
    "\n",
    "normalized = True\n",
    "\n",
    "promo_col = [\"[Measures].[Bundle Pack Sales]\",\"[Measures].[Loyalty Card Sales]\",\"[Measures].[Special Price Sales]\"] # Guidline Promo Columns ex :Volume Uplift >>> \"[Measures].[Volume Uplift IYA]\" using filter_dictionary_keys(fieldsNamePosition, 'Volume Upli')\n",
    "selectedBrands = client_brands + ['Private Brand','Granola','Prince','Nutella','Milka','Kinder', 'St Michel', 'Petit Ecolier', 'Delacre', 'Brossard', 'Savane', 'Napolitain', 'Lulu']\n",
    "marketList = regions_RET + channels_RET + market_RET + regions_CHAN + channels_CHAN + market_CHAN + regions_POS + channels_POS + market_POS\n",
    "start_date = '2023-06-30'\n",
    "end_date = '2024-05-31'\n",
    "notInScope = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_total_size = {\n",
    "        'Sc/Sb | Carrefour': 640,\n",
    "        'Sc/Sb | Carrefour Hypermarket': 640,\n",
    "        'Sc/Sb | Carrefour Supermarket': 640,\n",
    "        'Sc/Sb | Enseignes Hm': 663,\n",
    "        'Sc/Sb | Enseignes Sm': 663,\n",
    "        'Sc/Sb | Enseignes Drive': 663\n",
    "        ,'Carrefour | Gateaux Moelleux':526\n",
    "        ,'Carrefour Hypermarket | Gateaux Moelleux':526\n",
    "        ,'Carrefour Supermarket | Gateaux Moelleux':526\n",
    "\n",
    "        ,'Enseignes Hm | Gateaux Moelleux':551\n",
    "        ,'Enseignes Sm | Gateaux Moelleux':551\n",
    "        ,'Enseignes Drive | Gateaux Moelleux':551\n",
    "\n",
    "        ,'Carrefour | Biscuits Cerealiers':540\n",
    "        ,'Carrefour Hypermarket | Biscuits Cerealiers':540\n",
    "        ,'Carrefour Supermarket | Biscuits Cerealiers':540\n",
    "\n",
    "        ,'Enseignes Hm | Biscuits Cerealiers':554\n",
    "        ,'Enseignes Sm | Biscuits Cerealiers':554\n",
    "        ,'Enseignes Drive | Biscuits Cerealiers':554\n",
    "\n",
    "        ,'Carrefour | Gouter Fourre':526\n",
    "        ,'Carrefour Hypermarket | Gouter Fourre':526\n",
    "        ,'Carrefour Supermarket | Gouter Fourre':526\n",
    "\n",
    "        ,'Enseignes Hm | Gouter Fourre':551\n",
    "        ,'Enseignes Sm | Gouter Fourre':551\n",
    "        ,'Enseignes Drive | Gouter Fourre':551\n",
    " \n",
    "        ,'Carrefour | Recettes':526\n",
    "        ,'Carrefour Hypermarket | Recettes':526\n",
    "        ,'Carrefour Supermarket | Recettes':526\n",
    " \n",
    "        ,'Enseignes Hm | Recettes':551\n",
    "        ,'Enseignes Sm | Recettes':551\n",
    "        ,'Enseignes Drive | Recettes':551\n",
    "\n",
    "        ,'Carrefour | Nappe Choco':526\n",
    "        ,'Carrefour Hypermarket | Nappe Choco':526\n",
    "        ,'Carrefour Supermarket | Nappe Choco':526\n",
    "\n",
    "        ,'Enseignes Hm | Nappe Choco':551\n",
    "        ,'Enseignes Sm | Nappe Choco':551\n",
    "        ,'Enseignes Drive | Nappe Choco':551\n",
    " \n",
    "        ,'Carrefour | Petit Creux':526\n",
    "        ,'Carrefour Hypermarket | Petit Creux':526\n",
    "        ,'Carrefour Supermarket | Petit Creux':526\n",
    "    \n",
    "        ,'Enseignes Hm | Petit Creux':551\n",
    "        ,'Enseignes Sm | Petit Creux':551\n",
    "        ,'Enseignes Drive | Petit Creux':551\n",
    "   \n",
    "        ,'Carrefour | Cookies':526\n",
    "        ,'Carrefour Hypermarket | Cookies':526\n",
    "        ,'Carrefour Supermarket | Cookies':526\n",
    "      \n",
    "        ,'Enseignes Hm | Cookies':551\n",
    "        ,'Enseignes Sm | Cookies':551\n",
    "        ,'Enseignes Drive | Cookies':551\n",
    "    \n",
    "        ,'Carrefour | Multi Layer':540\n",
    "        ,'Carrefour Hypermarket | Multi Layer':540\n",
    "        ,'Carrefour Supermarket | Multi Layer':540\n",
    "\n",
    "        ,'Enseignes Hm | Multi Layer':554\n",
    "        ,'Enseignes Sm | Multi Layer':554\n",
    "        ,'Enseignes Drive | Multi Layer':554\n",
    "\n",
    "        ,'Carrefour | Pates Simples':540\n",
    "        ,'Carrefour Hypermarket | Pates Simples':540\n",
    "        ,'Carrefour Supermarket | Pates Simples':540\n",
    "\n",
    "        ,'Enseignes Hm | Pates Simples':554\n",
    "        ,'Enseignes Sm | Pates Simples':554\n",
    "        ,'Enseignes Drive | Pates Simples':554\n",
    "  \n",
    "        ,'Carrefour | Fourres':540\n",
    "        ,'Carrefour Hypermarket | Fourres':540\n",
    "        ,'Carrefour Supermarket | Fourres':540\n",
    "    \n",
    "        ,'Enseignes Hm | Fourres':554\n",
    "        ,'Enseignes Sm | Fourres':554\n",
    "        ,'Enseignes Drive | Fourres':554\n",
    "    \n",
    "        ,'Carrefour | Brownies':540\n",
    "        ,'Carrefour Hypermarket | Brownies':540\n",
    "        ,'Carrefour Supermarket | Brownies':540\n",
    "     \n",
    "        ,'Enseignes Hm | Brownies':554\n",
    "        ,'Enseignes Sm | Brownies':554\n",
    "        ,'Enseignes Drive | Brownies':554\n",
    "      \n",
    "        ,'Carrefour | Marbres':540\n",
    "        ,'Carrefour Hypermarket | Marbres':540\n",
    "        ,'Carrefour Supermarket | Marbres':540\n",
    "     \n",
    "        ,'Enseignes Hm | Marbres':554\n",
    "        ,'Enseignes Sm | Marbres':554\n",
    "        ,'Enseignes Drive | Marbres':554\n",
    "   \n",
    "\n",
    "\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "datasets_path = os.getcwd()+\"/Promotion Datasets/\"\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLeaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningData(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "        # Skip the first 11 rows if there are NaN values\n",
    "        df = data[key].iloc[11:]\n",
    "        if data[key].iloc[11,:].isna().any():\n",
    "            df = data[key].iloc[12:]\n",
    "        \n",
    "        # Set column names and skip the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        \n",
    "        # Perform specific cleaning operations based on the DataFrame columns and key\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if 'Top Brands' in df.columns and 'Product' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df['Product'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "            \n",
    "            elif 'Top Brands' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df[~df['Top Brands'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "                df['VSOD Evaluation vs YA']=df['VSOD IYA']-1\n",
    "                if normalized:\n",
    "                    df['Promo Value Uplift vs YA']=df['Value Uplift Normalized IYA']-1\n",
    "                else:\n",
    "                    df['Promo Value Uplift vs YA']=df['Value Uplift IYA']-1\n",
    "                \n",
    "                \n",
    "            elif 'End of Week' in df.columns and 'Product' in df.columns:\n",
    "                df['Product'] = df['Product'].fillna(method='ffill')\n",
    "                df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df['Product'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df = df[df['Promo Sales'] > 10000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-21|-22|-23|Jan-24')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "            \n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaned_data[modified_key] = df\n",
    "            else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaned_data[key] = df\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningdata_with_grand_total(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaningdata_with_grand_total = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "        # Skip the first 11 rows if there are NaN values\n",
    "        df = data[key].iloc[11:]\n",
    "        if data[key].iloc[11,:].isna().any():\n",
    "            df = data[key].iloc[12:]\n",
    "        \n",
    "        # Set column names and skip the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        \n",
    "        # Perform specific cleaning operations based on the DataFrame columns and key\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if 'Top Brands' in df.columns and 'Product' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df['Product'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "            \n",
    "            elif 'Top Brands' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                #df = df[~df['Top Brands'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "                df['VSOD Evaluation vs YA']=df['VSOD IYA']-1\n",
    "                if normalized:\n",
    "                    df['Promo Value Uplift vs YA']=df['Value Uplift Normalized IYA']-1\n",
    "                else:\n",
    "                    df['Promo Value Uplift vs YA']=df['Value Uplift IYA']-1\n",
    "                \n",
    "                \n",
    "            elif 'End of Week' in df.columns and 'Product' in df.columns:\n",
    "                df['Product'] = df['Product'].fillna(method='ffill')\n",
    "                df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df['Product'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df = df[df['Promo Sales'] > 10000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-21|-22|-23|Jan-24')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "            \n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[modified_key] = df\n",
    "            else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[key] = df\n",
    "    \n",
    "    return cleaningdata_with_grand_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning13New(data):\n",
    "    \"\"\"\n",
    "    Clean and process data for specific brands and regions.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing raw data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned and processed data.\n",
    "    \"\"\"\n",
    "    data_cleaned = {}\n",
    "    \n",
    "    # Define maximum total size for each combination of product type and region\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        # Skip processing if the region is 'NATIONAL' or 'National'\n",
    "        if 'NATIONAL' in areas or 'National' in key:\n",
    "            continue\n",
    "        \n",
    "        new_data = []\n",
    "        \n",
    "        # Skip first 12 rows as they are headers and metadata\n",
    "        df = df.iloc[12:]\n",
    "        \n",
    "        # Set columns names based on the first row, and skip the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        \n",
    "        # Fill missing values in 'Top Brands' column with the previous non-null value\n",
    "        df['Top Brands'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "        # Filter out rows where 'Top Brands' is 'Grand Total' or 'Other'\n",
    "        df = df[(df['Top Brands'] != 'Grand Total') & (df['Top Brands'] != 'Other')]\n",
    "        # Remove 'GR' suffix from 'Total Size' and convert it to integer\n",
    "        df['Total Size'] = df['Total Size'].str.extract('(\\d+)', expand=False)\n",
    "        df.fillna('0',inplace=True)\n",
    "        df['Total Size'] = df['Total Size'].astype(int)\n",
    "        \n",
    "        # Sort data by 'Value Share' in descending order\n",
    "        df = df.sort_values(by='Value Share', ascending=False).reset_index(drop=True)\n",
    "        for i, brand in enumerate(df['Top Brands'].unique()):\n",
    "            # Determine the product key based on the first two elements of the key\n",
    "            product_key = key.split('|')[0] + '|' + key.split('|')[1]\n",
    "            \n",
    "            # Get the maximum total size for the product key, if it exists\n",
    "            max_size = max_total_size.get(product_key, None)\n",
    "            # Filter rows for the current brand and check if total size is within the maximum allowed size\n",
    "            if max_size is not None:\n",
    "                brand_df = df[(df['Top Brands'] == brand) & (df['Total Size'] <= max_size)]\n",
    "            else:\n",
    "                brand_df = pd.DataFrame()\n",
    "                \n",
    "            # Calculate recruitment ratio if the brand has data and total size is greater than zero\n",
    "            #brand_total = df[(df['Top Brands'] == brand + ' Total')]['Promo Value'].values\n",
    "            brand_total = df[(df['Top Brands'].str.strip() == (brand + ' Total').strip())]['Promo Value'].values\n",
    "\n",
    "            if not brand_df.empty and brand_total.size > 0 and brand_total[0] > 0:\n",
    "                brand_sum = brand_df['Promo Value'].sum() / brand_total[0]\n",
    "                new_data.append({'Top Brands': brand, 'Recruitment': brand_sum, 'Consumption': 1 - brand_sum, 'Value Share': df['Value Share'][i], 'SUM':brand_df['Promo Value'].sum()})\n",
    "        \n",
    "        # Create a new DataFrame with cleaned data\n",
    "        new = pd.DataFrame(new_data)\n",
    "        new.fillna(0, inplace=True)\n",
    "        \n",
    "        # Add cleaned data to the dictionary if it contains non-zero rows\n",
    "        if new.shape[0] != 0:\n",
    "            data_cleaned[key] = new\n",
    "        \n",
    "    return data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VSOD_Clean(VSOD_Data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess VSOD data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - VSOD_Data (dict): Dictionary containing VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned VSOD DataFrames.\n",
    "    \"\"\"\n",
    "    VSOD_cleaned = {}\n",
    "    for key in VSOD_Data:\n",
    "        # Skip first 11 rows\n",
    "        df = VSOD_Data[key].iloc[11:]\n",
    "        # Set column names and skip first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        # Fill NaN values with 0\n",
    "        df['Sector'].fillna(method='ffill', inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        VSOD_cleaned[key] = df\n",
    "    return VSOD_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(VSODClient_Cleaned, VSODCleaned):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of DataFrames based on a common column.\n",
    "\n",
    "    Parameters:\n",
    "    - VSODClient_Cleaned (dict): Dictionary containing cleaned VSOD client DataFrames.\n",
    "    - VSODCleaned (dict): Dictionary containing cleaned VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing merged DataFrames.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for key in VSODClient_Cleaned:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        VSOD_total = VSODCleaned[key][VSODCleaned[key]['Sector'].str.contains('Total', case=False, na=False)]\n",
    "        VSODClient_total = VSODClient_Cleaned[key][VSODClient_Cleaned[key]['Sector'].str.contains('Total', case=False, na=False)]\n",
    "        merged_df = pd.merge(VSOD_total, VSODClient_total, on=['Sector','Segment'], how='left')\n",
    "        merged_df.fillna(0,inplace=True)\n",
    "        merged_dict[key] = merged_df     \n",
    "        if len(sectors)!=0:\n",
    "            for se in sectors:\n",
    "                dfVSOD_client = VSODClient_Cleaned[key][VSODClient_Cleaned[key]['Sector'].str.contains(se, case=False, na=False)]\n",
    "                dfVSOD = VSODCleaned[key][VSODCleaned[key]['Sector'].str.contains(se, case=False, na=False)]\n",
    "                merged_df = pd.merge(dfVSOD, dfVSOD_client, on=['Sector','Segment'], how='left')\n",
    "                merged_df.fillna(0,inplace=True)\n",
    "                new_key = key + ' | ' +se\n",
    "                merged_dict[new_key] = merged_df\n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSODClient_Cleaned = cleaningData(VSOD_Client)\n",
    "VSODCleaned = VSOD_Clean(VSOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionBrandsP12M = cleaningdata_with_grand_total(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionProductsP12M = cleaningData(promotions_products_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionEndOfWeek = cleaningData(promotions_EndOfWeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_valueUplift = cleaningData(value_uplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newModifiedBrands = cleaning13New(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSOD_merged = merging(VSODClient_Cleaned,VSODCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptydf={}\n",
    "promotionsBrandSortedTotal=dfSort(modified_promotionBrandsP12M, client_brands, \"Top Brands\", num=8,salesCol='Promo Value')\n",
    "keys = list(promotionsBrandSortedTotal)\n",
    "for key in keys:\n",
    "     df = promotionsBrandSortedTotal[key].reset_index(drop=True)\n",
    "     df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "     if len(df) ==0:\n",
    "          del promotionsBrandSortedTotal[key]\n",
    "          emptydf[key] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands_og = selectedBrands\n",
    "selectedBrands= selectedBrands + [\"Grand Total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsSelected={key:modified_promotionBrandsP12M[key][modified_promotionBrandsP12M[key]['Top Brands'].isin(selectedBrands)].sort_values(by='Promo Value',ascending=False) for key in modified_promotionBrandsP12M.keys()   if all(cat != key.split(' | ')[0] for cat in categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands=selectedBrands_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAttribute(dic, marketList):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames and a list of markets, and concatenates\n",
    "    the DataFrames by adding a 'SOURCE' column to each DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are strings in the format 'market | source', and\n",
    "                values are DataFrames containing market data.\n",
    "    marketList (list): A list of market names (strings).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are market names and values are concatenated DataFrames\n",
    "          with an added 'SOURCE' column.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict to store the resulting DataFrames\n",
    "    marketDic = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the list of markets\n",
    "    for market in marketList:\n",
    "        # Iterate through the items in the dictionary\n",
    "        for key, value in dic.items():\n",
    "            # Check if the market name matches the key's market part\n",
    "            if market == key.split(' | ')[0]:\n",
    "                # Extract the source part from the key and assign it to the 'SOURCE' column\n",
    "                value['SOURCE'] = list(set(key.split(' | ')) - set([market]))[0]\n",
    "                # Only include rows where 'SOURCE' is not 'National'\n",
    "                if (value['SOURCE'] != 'National').all():\n",
    "                    marketDic[market].append(value)\n",
    "\n",
    "        # Concatenate all DataFrames in the list for each market\n",
    "        if len(marketDic[market]) != 0:\n",
    "            marketDic[market] = pd.concat(marketDic[market])\n",
    "    \n",
    "    return marketDic\n",
    "\n",
    "def fillingMissingBrands(dic):\n",
    "    \"\"\"\n",
    "    This function fills in missing brands for each market and source combination in the\n",
    "    provided dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are market names and values are DataFrames\n",
    "                containing market data with 'Top Brands' and 'SOURCE' columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: The input dictionary with missing brands filled in each DataFrame.\n",
    "    \"\"\"\n",
    "    # Iterate through the dictionary items\n",
    "    for key, value in dic.items():\n",
    "        # Get the unique list of top brands in the DataFrame\n",
    "\n",
    "   \n",
    "        brandList = value['Top Brands'].unique().tolist()\n",
    " \n",
    "        # Iterate through the unique sources in the DataFrame\n",
    "        for source in value['SOURCE'].unique():\n",
    "            # Check if the number of unique brands for the source is less than the total unique brands\n",
    "            if value[value['SOURCE'] == source]['Top Brands'].nunique() != len(brandList):\n",
    "                # Find the missing brands for the source\n",
    "                missingBrand = list(set(brandList) - set(value[value['SOURCE'] == source]['Top Brands'].unique()))\n",
    "                # Create a DataFrame for the missing brands with the current source\n",
    "                missingBrand = pd.DataFrame({'Top Brands': missingBrand, 'SOURCE': source}).explode('Top Brands')\n",
    "                # Concatenate the missing brands DataFrame with the original DataFrame\n",
    "                value = pd.concat([value, missingBrand]).replace(np.nan, 0).reset_index(drop=True)\n",
    "\n",
    "        # Update the dictionary with the filled DataFrame\n",
    "        dic[key] = value\n",
    "    \n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsWithMarket=concatAttribute(promotionsBrandsSelected,marketList)\n",
    "promotionsBrandsWithMarket = fillingMissingBrands(promotionsBrandsWithMarket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAttributeNew(sorted):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames sorted by keys and concatenates the DataFrames\n",
    "    based on specified categories, sectors, and segments. It adds a 'SOURCE' column to each DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    sorted (dict): A dictionary where keys are strings in the format 'category | sector | segment | brand',\n",
    "                   and values are DataFrames containing market data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are the categories, sectors, and segments, and values are concatenated DataFrames\n",
    "          with an added 'SOURCE' column.\n",
    "    \"\"\"\n",
    "    # List of all categories, sectors, and segments to process\n",
    "    lis = categories + sectors + segments\n",
    "  \n",
    "    # Initialize a defaultdict to store the resulting DataFrames\n",
    "    marketDic = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the list of categories, sectors, and segments\n",
    "    for i in lis:\n",
    "        # Iterate through the items in the sorted dictionary\n",
    "        for key, value in sorted.items():\n",
    "            # Check if the current item is part of the key\n",
    "            if i in key:\n",
    "                # Split the key to extract individual parts              \n",
    "                parts = key.split(' | ')\n",
    "                # Identify the client brand (either 'Extra' or 'Kiwi')\n",
    "                if i in categories:\n",
    "                    markets = parts[1]\n",
    "                else:\n",
    "                    markets = parts[0]\n",
    "                \n",
    "                # Set the 'SOURCE' column to the client brand\n",
    "                value['SOURCE'] = markets\n",
    "                \n",
    "                marketDic[i].append(value)\n",
    "                    \n",
    "        # Concatenate all DataFrames in the list for each category/sector/segment\n",
    "        if marketDic[i]:\n",
    "            marketDic[i] = pd.concat(marketDic[i])\n",
    "    \n",
    "    return marketDic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = concatAttributeNew(modified_promotionBrandsP12M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        df = modified_promotionProductsP12M[key].sort_values(by=['Promo Share'], ascending=False)\n",
    "        \n",
    "        # Filter and sort the DataFrame\n",
    "        df = df[df['Product'] != '']\n",
    "        df['cumulative promo share'] = df['Promo Share'].cumsum()\n",
    "        df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "        df = df[df['VSOD'] >= 0.05]\n",
    "        df = df[df['cumulative promo share'] <= 0.8]\n",
    "        df = df.sort_values(by='Incr Value', ascending=False).reset_index(drop=True)\n",
    "        df = df.head(50)\n",
    "        df['index'] = str(df.index + 1)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] >0:\n",
    "            cleaned_data[key] = df\n",
    "        #else:\n",
    "            #print(key)\n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data3(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        for client in client_brands:\n",
    "            \n",
    "            df = modified_promotionProductsP12M[key]\n",
    "\n",
    "            # Filter the DataFrame for the current client brand\n",
    "            df = df[df['Product'] != '']\n",
    "            df = df[df['Top Brands'] == client]\n",
    "            df = df.sort_values(by='Top Brands')\n",
    "            df['cumulative promo share'] = df.groupby('Top Brands')['Promo Share'].cumsum()\n",
    "            df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "            df = df[df['VSOD'] >= 0.05]\n",
    "            df = df.sort_values(by='Incr Value', ascending=False).tail(20).reset_index(drop=True)\n",
    "            df = df.sort_values(by ='Incr Value', ascending= True).reset_index(drop=True)\n",
    "            if df.shape[0] >0:\n",
    "                cleaned_data[key+'_'+client] = df\n",
    "                \n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20clientonly = filter_data3(modified_promotionProductsP12M)\n",
    "bottom20clientonly = filter_data3(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_modified_promotionProductsP12M = filter_data(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotionsEndOfWeekCleaning(promotions_EndOfWeek, notInScope, col='Top Brands'):\n",
    "    \"\"\"\n",
    "    Clean promotions end of week data.\n",
    "\n",
    "    Parameters:\n",
    "    promotions_EndOfWeek (dict): Dictionary containing promotions end of week data.\n",
    "    notInScope (list): List of items not in scope to be excluded.\n",
    "    col (str): Column name to check for filtering. Default is 'Top Brands'.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing cleaned promotions end of week data.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store cleaned promotions end of week data\n",
    "    promotionsEndOfWeek = {}\n",
    "    \n",
    "    # Iterate over items in promotions_EndOfWeek dictionary\n",
    "    for key, value in promotions_EndOfWeek.items():\n",
    "        # Make a copy of the dataframe to avoid modifying the original data\n",
    "        df = value.copy()\n",
    "\n",
    "        # Check if the dataframe is not empty\n",
    "        if df.shape[0] != 0:\n",
    "            # Modify the key to match the desired format if applicable\n",
    "            modified_key = key\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "            \n",
    "            # Check if the key contains any item from the notInScope list\n",
    "            # If not, add the dataframe to the cleaned dictionary after filtering out 'Grand Total' rows\n",
    "            flag = False if any(element in modified_key for element in notInScope) else True\n",
    "            if flag:\n",
    "                promotionsEndOfWeek[modified_key] = df[df[col] != 'Grand Total'].reset_index(drop=True).replace(np.nan, 0)\n",
    "            \n",
    "        else:\n",
    "            print(key, ' Is empty')\n",
    "    \n",
    "    return promotionsEndOfWeek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=cleaningData(promotions_EndOfWeek)\n",
    "promotionsEndOfWeekCleaned=promotionsEndOfWeekCleaning(mod,notInScope,col='End of Week')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarket = list(set([key.split(' | ')[0]+' | '+key.split(' | ')[1] for key in promotionsEndOfWeekCleaned]))\n",
    "brandMarketCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat in key.split(' | ')[-1] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in subcategories )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeDates(dfList, promotionsEndOfWeekCleaned):\n",
    "    \"\"\"\n",
    "    Complete dates for each dataframe in dfList based on promotionsEndOfWeekCleaned.\n",
    "\n",
    "    Parameters:\n",
    "    dfList (list): List of dataframe keys.\n",
    "    promotionsEndOfWeekCleaned (dict): Dictionary containing cleaned promotions end of week data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing EndOfWeekcompletDate (dictionary), dfGroup (list), and dic (dictionary).\n",
    "    \"\"\"\n",
    "    # Create a list of unique brand-category combinations\n",
    "    brandCatList = list(set(key.split(' | ')[0] + ' | ' + key.split(' | ')[2] for key in dfList))\n",
    "    \n",
    "    # Initialize dictionaries and lists\n",
    "    EndOfWeekcompletDate = {}\n",
    "    dfGroup = []\n",
    "    dic = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each brand-category combination\n",
    "    for key in brandCatList:\n",
    "        for name in dfList:\n",
    "            if (key.split(' | ')[0] == name.split(' | ')[0]) and (key.split(' | ')[1] == name.split(' | ')[2]):\n",
    "                dic[key] += 1\n",
    "                \n",
    "    # Iterate over unique brand-category combinations\n",
    "    for name in dic.keys():\n",
    "        # Get dataframe keys associated with the current brand-category combination\n",
    "        dfName = [key for key in dfList if name == (key.split(' | ')[0] + ' | ' + key.split(' | ')[2])]\n",
    "        \n",
    "        # Extract unique dates from all associated dataframes\n",
    "        uniqueDates = pd.concat([promotionsEndOfWeekCleaned[key] for key in dfName])[['End of Week']].drop_duplicates()\n",
    "        \n",
    "        # Initialize dictionary for complete dates for each dataframe\n",
    "        dfCompleteDates = {}\n",
    "        \n",
    "        # Add dataframe keys to the group list\n",
    "        dfGroup.append(dfName)\n",
    "        \n",
    "        # Populate EndOfWeekcompletDate dictionary with dataframes merged on unique dates\n",
    "        for key in dfName:\n",
    "            EndOfWeekcompletDate[key] = pd.merge(uniqueDates, promotionsEndOfWeekCleaned[key], how='left').replace(np.nan, 0)\n",
    "   \n",
    "    return EndOfWeekcompletDate, dfGroup, dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory,catGroup,catDuplication=completeDates(brandMarketCategory,promotionsEndOfWeekCleaned)\n",
    "if len(sectors) != 0:\n",
    "    dfSector,secGroup,secDuplication=completeDates(brandMarketSector,promotionsEndOfWeekCleaned)\n",
    "if len(segments) != 0:\n",
    "    dfSegment,segGroup,segDuplication=completeDates(brandMarketSegment,promotionsEndOfWeekCleaned)\n",
    "\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment,subsegGroup,subsegDuplication=completeDates(brandMarketSubSegment,promotionsEndOfWeekCleaned)\n",
    "\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory,subcatGroup,subcatDuplication=completeDates(brandMarketSubCategory,promotionsEndOfWeekCleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplication Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "slidePromoValueIndex=[i+16 for i in catDuplication.values()],[i+16 for i in secDuplication.values()],[i+16 for i in segDuplication.values()]\n",
    "\n",
    "index = [0,1,2,3,4,5,6,7,8,9,10,11,12,13 if promo_type != False else None, 14 if feature_share != False else None,\n",
    "         15 if display_share != False else None, 16,*slidePromoValueIndex,21]\n",
    "\n",
    "index = [x for x in index if x is not None]\n",
    "\n",
    "\n",
    "len_brands = len(modified_promotionBrandsP12M)\n",
    "len_Prod = len(modified_promotionProductsP12M)\n",
    "len_modified_prod = len(new_modified_promotionProductsP12M)\n",
    "len_client_market = len(client_brands) * len(VSOD_merged)\n",
    "duplication = [ len_brands, len(promotionsBrandSortedTotal), len(promotionsBrandsWithMarket) if len(sectors)>0 else 0, len(promotionsBrandsWithMarket)*len(sectors) if len(segments)>0 else 0, len(concated),len_Prod, len_modified_prod, len_modified_prod,\n",
    "               len(top20clientonly), len(bottom20clientonly),len_client_market, len_brands, len(newModifiedBrands), \n",
    "               len(newModifiedBrands) if promo_type != False else None, len_brands if feature_share != False else None , \n",
    "               len_brands if display_share!= False else None,  len(modified_promotionEndOfWeek),\n",
    "               1 if len(categories)>0 and len(slidePromoValueIndex[0])!=0 else 0 ,1 if len(sectors)>0 and len(slidePromoValueIndex[1])!=0  else 0 ,1 if len(segments)>0 and len(slidePromoValueIndex[2])!=0  else 0, len(modified_valueUplift)]\n",
    "\n",
    "duplication = [x for x in duplication if x is not None]\n",
    "\n",
    "section_names = [\"Promo Value Sales\",\"Promo Evolution\",\"VSOD Summary by Sector\" if len(sectors)>0 else \"\",\"VSOD Summary by Segment\" if len(segments)>0 else \"\", \"Value uplift by retailer by brand\", \"Volume Uplift vs discount depth\",\n",
    "                 \"Value Uplift vs Promo Efficiency Quadrant\",\"Top 20 promotions\",\"Top 20 promotions CLIENT ONLY\",\"Bottom 20 promotions CLIENT ONLY\",\n",
    "                \"Volume Sold on Deal\",\"Promo share vs Value Share\",\"Promo Sales by total size\",\"Promo Sales by promo type\" if promo_type != False else None\n",
    "                 ,\"Feature Share vs Fair Share \"if feature_share != False else None, \"Display Share vs Fair Share\" if display_share!= False else None,\n",
    "                 \"Promo Frequency learnings\",\"Category Promo sales per retailer\" if len(categories)>0  and len(slidePromoValueIndex[0])!=0  else \"\", \"Sector Promo sales per retailer\"if len(sectors)>0  and len(slidePromoValueIndex[1])!=0 else \"\",\n",
    "                 \"Segment Promo sales per retailer\"if len(segments)>0  and len(slidePromoValueIndex[2])!=0  else \"\",\"Value Uplift vs discount depth\"]\n",
    "\n",
    "section_names = [x for x in section_names if x is not None]\n",
    "\n",
    "\n",
    "path = os.getcwd() + '//slide base.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated.pptx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation(new_pre)\n",
    "posItr = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "promoValueSales(prs,modified_promotionBrandsP12M,duplication[posItr],position=posItr)\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "promoEvolution(prs,promotionsBrandSortedTotal,duplication[posItr],position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors) !=0:\n",
    "    VSODSectors(prs,promotionsBrandsWithMarket,duplication[posItr],position=sum(duplication[:posItr]))\n",
    "    posItr +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(segments) !=0:\n",
    "    VSODSegments(prs,promotionsBrandsWithMarket,duplication[posItr],position=sum(duplication[:posItr]))\n",
    "    posItr +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subsegments) !=0:\n",
    "    VSODSubSegments(prs,promotionsBrandsWithMarket,duplication[posItr],position=sum(duplication[:posItr]))\n",
    "    posItr +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subcategories) !=0:\n",
    "    VSODSubCategories(prs,promotionsBrandsWithMarket,duplication[posItr],position=sum(duplication[:posItr]))\n",
    "    posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueUpliftRetailer(prs,concated,duplication[posItr],position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "VolumeUplift(prs,modified_promotionProductsP12M, duplication[posItr],position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueUpliftvsPromoEfficiencyQuadrant(prs,new_modified_promotionProductsP12M, duplication[posItr], position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20(prs,new_modified_promotionProductsP12M, duplication[posItr], position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20Client(prs,top20clientonly, duplication[posItr], position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot20Client(prs,bottom20clientonly, duplication[posItr], position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "VolumeSold(prs,VSOD_merged, duplication[posItr], position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoShare_vs_ValueShare(prs, modified_promotionBrandsP12M, duplication[posItr],position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoSalesTotalSize(prs,newModifiedBrands, duplication[posItr],position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "if promo_type != False:    \n",
    "    PromoSalesTypes(prs,modified_promotionBrandsP12M, duplication[posItr],position=sum(duplication[:posItr]))\n",
    "    posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_share != False:\n",
    "    featureShare(prs,modified_promotionBrandsP12M, duplication[posItr],position=sum(duplication[:posItr]))\n",
    "    posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_share != False:\n",
    "    displayShare(prs,modified_promotionBrandsP12M, duplication[posItr],position=sum(duplication[:posItr]))\n",
    "    posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoFrequency(prs, modified_promotionEndOfWeek, duplication[posItr],position=sum(duplication[:posItr]))\n",
    "posItr +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slide 17\n",
    "\n",
    "#Category Replace\n",
    "pos = sum([duplication[i]* (1 if type(index[i])==int else len(index[i])) for i in range(posItr)])\n",
    "\n",
    "promoSalesPerRetailer(prs,dfCategory,len(index[posItr]),catGroup,position=pos)\n",
    "posItr +=1\n",
    "\n",
    "#Sector Replace\n",
    "pos = sum([duplication[i]* (1 if type(index[i])==int else len(index[i])) for i in range(posItr)])\n",
    "\n",
    "promoSalesPerRetailer(prs,dfSector,len(index[posItr]),secGroup,position=pos)\n",
    "posItr +=1\n",
    "\n",
    "if len(segments) != 0:\n",
    "    pos = sum([duplication[i]* (1 if type(index[i])==int else len(index[i])) for i in range(posItr)])\n",
    "    promoSalesPerRetailer(prs,dfSegment,len(index[posItr]),segGroup,position=pos)\n",
    "    posItr +=1\n",
    "if len(subsegments) != 0:\n",
    "    pos = sum([duplication[i]* (1 if type(index[i])==int else len(index[i])) for i in range(posItr)])\n",
    "    promoSalesPerRetailer(prs,dfSubSegment,len(index[posItr]),subsegGroup,position=pos)\n",
    "    posItr +=1\n",
    "if len(subcategories) != 0:\n",
    "    pos = sum([duplication[i]* (1 if type(index[i])==int else len(index[i])) for i in range(posItr)])\n",
    "    promoSalesPerRetailer(prs,dfSubCategory,len(index[posItr]),subcatGroup,position=pos)\n",
    "    posItr +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slide 18\n",
    "# valueUplift(prs, modified_valueUplift, duplication[17],position=sum(duplication[:17]))\n",
    "#pos = sum(duplication[:14]) + duplication[14]*len(index[14]) +duplication[15]*len(index[15]) \n",
    "\n",
    "pos = sum([duplication[i]* (1 if type(index[i])==int else len(index[i])) for i in range(posItr)])\n",
    "\n",
    "valueUplift(prs, modified_valueUplift, duplication[posItr],position=pos)\n",
    "posItr +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath=os.getcwd() + \"\\\\Promotion doc output.pptx\"\n",
    "prs.save(outputPath)\n",
    "app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "presentation = app.Presentations.Open(outputPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
