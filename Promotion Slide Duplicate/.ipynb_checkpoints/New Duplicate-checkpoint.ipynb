{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15a454f5-c4e1-460c-a27d-40aa00dfcac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"..\\general_functions\\generalFunctions.ipynb\"\n",
    "%run \"..\\Promotion Slide Duplicate\\Promotion Replacement Function.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b6fbff-4bd3-4c15-8d5d-f3b65886bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_type = True\n",
    "feature_share = False\n",
    "display_share = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0249d53a-535b-4fd4-bad7-9d17f982fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = os.path.join(os.path.dirname(os.getcwd()),\"Live connection Edgewell 1.xlsx\")\n",
    "client_manuf = [\"Edgewell\"]\n",
    "client_brands = [\"Xtreme 3\",\"Extra 3\",\"Hydro\",\"Quattro Titanium\",\"Classic\",\"Quattro\",\"Extra 2\",\"Duplo\"]\n",
    "decimals = 2\n",
    "sign = \"After\"\n",
    "currency = 'â‚¬'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' ' \n",
    "\n",
    "categories = [\"Male Shaving\"]\n",
    "sectors = [\"Disposable\",\"System\"]\n",
    "segments = [\"Lame\",\"Rasoi\"]\n",
    "subsegments= [\"Rasoi No Sensitive\", \"Rasoi Sensitive\", \"Disposable No Sensitive\",\"Disposable Sensitive\"]\n",
    "subcategories= [\"1 Blades\",\t\"2 Blades\",\t\"3 Blades\",\t\"4 Blades\",\t\"5 Blades\"]\n",
    "\n",
    "national = True\n",
    "customareas= \"DRUGSTORES\"\n",
    "areas = ['NATIONAL', \"RETAILER\",\"CHANNEL\",customareas]#, \"CHANNEL\"\n",
    "\n",
    "regions_RET  = [\"Cesar\"]\n",
    "channels_RET = []\n",
    "market_RET = []\n",
    "\n",
    "regions_CHAN = []\n",
    "channels_CHAN = []#\"Discount\",\"Hm/Sm\",\"Small Format\"\n",
    "market_CHAN = []\n",
    "\n",
    "regions_CUST = []\n",
    "channels_CUST = []\n",
    "market_CUST = []\n",
    "\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending Feb 2024\"\n",
    "years = ['2021', '2022','2023']\n",
    "\n",
    "# Add one month to the original ending date (YYYY-MM-01)\n",
    "start_date = \"2020-01-05\"\t\n",
    "end_date = \"2024-03-01\"\n",
    "prodORitem = \"Product\"\n",
    "\n",
    "normalized = False\n",
    "national = False\n",
    "\n",
    "dispaly_share = True  # True if Available\n",
    "feature_share = False\n",
    "\n",
    "# Guidline Promo Columns ex :Volume Uplift >>> \"[Measures].[Volume Uplift IYA]\" using filter_dictionary_keys(fieldsNamePosition, 'Volume Upli')\n",
    "promo_col = ['[Measures].[Straight Discount 10-20 Sales]','[Measures].[Straight Discount 20-30 Sales]', '[Measures].[Straight Discount 30-40 Sales]','[Measures].[Straight Discount 40+ Sales]']\n",
    "selectedBrands = client_brands \n",
    "marketList = regions_RET + channels_RET + market_RET + regions_CHAN + channels_CHAN + market_CHAN \n",
    "notInScope = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461dc096-382d-4c3b-91c1-8610a7cd684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "datasets_path = os.getcwd()+\"/Promotion Datasets/\"\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568bfe77-8df7-454f-a650-f680c67c66da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d9ab22-4ebf-4b74-8859-4cad1e38a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningData(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "        # Skip the first 11 rows if there are NaN values\n",
    "        df = data[key].iloc[11:]\n",
    "        if data[key].iloc[11,:].isna().any():\n",
    "            df = data[key].iloc[12:]\n",
    "        \n",
    "        # Set column names and skip the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        \n",
    "        # Perform specific cleaning operations based on the DataFrame columns and key\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if 'Top Brands' in df.columns and 'Product' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df['Product'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df.reset_index(drop=True)\n",
    "            \n",
    "            elif 'Top Brands' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df[~df['Top Brands'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "                df['VSOD Evaluation vs YA']=df['VSOD IYA']-1\n",
    "                if normalized:\n",
    "                    df['Promo Value Uplift vs YA']=df['Value Uplift Normalized IYA']-1\n",
    "                else:\n",
    "                    df['Promo Value Uplift vs YA']=df['Value Uplift IYA']-1\n",
    "                \n",
    "                \n",
    "            elif 'End of Week' in df.columns and 'Product' in df.columns:\n",
    "                df['Product'] = df['Product'].fillna(method='ffill')\n",
    "                df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df['Product'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df = df[df['Promo Sales'] > 10000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-21|-22|-23|Jan-24')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "            \n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaned_data[modified_key] = df\n",
    "            else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaned_data[key] = df\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df05b63b-986d-473b-9a40-ef7798a932ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningdata_with_grand_total(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaningdata_with_grand_total = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "        # Skip the first 11 rows if there are NaN values\n",
    "        df = data[key].iloc[11:]\n",
    "        if data[key].iloc[11,:].isna().any():\n",
    "            df = data[key].iloc[12:]\n",
    "        \n",
    "        # Set column names and skip the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        \n",
    "        # Perform specific cleaning operations based on the DataFrame columns and key\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if 'Top Brands' in df.columns and 'Product' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df['Product'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "            \n",
    "            elif 'Top Brands' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                #df = df[~df['Top Brands'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "                df['VSOD Evaluation vs YA']=df['VSOD IYA']-1\n",
    "                if normalized:\n",
    "                    df['Promo Value Uplift vs YA']=df['Value Uplift Normalized IYA']-1\n",
    "                else:\n",
    "                    df['Promo Value Uplift vs YA']=df['Value Uplift IYA']-1\n",
    "                \n",
    "                \n",
    "            elif 'End of Week' in df.columns and 'Product' in df.columns:\n",
    "                df['Product'] = df['Product'].fillna(method='ffill')\n",
    "                df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df['Product'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df = df[df['Promo Sales'] > 10000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-21|-22|-23|Jan-24')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "            \n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[modified_key] = df\n",
    "            else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[key] = df\n",
    "    \n",
    "    return cleaningdata_with_grand_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc586ee-696d-43bb-b763-ecfe53252dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning13New(data):\n",
    "    \"\"\"\n",
    "    Clean and process data for specific brands and regions.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing raw data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned and processed data.\n",
    "    \"\"\"\n",
    "    data_cleaned = {}\n",
    "    \n",
    "    # Define maximum total size for each combination of product type and region\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        # Skip processing if the region is 'NATIONAL' or 'National'\n",
    "        if 'NATIONAL' in areas or 'National' in key:\n",
    "            continue\n",
    "        \n",
    "        new_data = []\n",
    "        \n",
    "        # Skip first 12 rows as they are headers and metadata\n",
    "        df = df.iloc[12:]\n",
    "        \n",
    "        # Set columns names based on the first row, and skip the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        \n",
    "        # Fill missing values in 'Top Brands' column with the previous non-null value\n",
    "        df['Top Brands'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "        # Filter out rows where 'Top Brands' is 'Grand Total' or 'Other'\n",
    "        df = df[(df['Top Brands'] != 'Grand Total') & (df['Top Brands'] != 'Other')]\n",
    "        # Remove 'GR' suffix from 'Total Size' and convert it to integer\n",
    "        df['Total Size'] = df['Total Size'].str.extract('(\\d+)', expand=False)\n",
    "        df.fillna('0',inplace=True)\n",
    "        df['Total Size'] = df['Total Size'].astype(int)\n",
    "        print(df)\n",
    "        # Sort data by 'Value Share' in descending order\n",
    "        df = df.sort_values(by='Value Share', ascending=False).reset_index(drop=True)\n",
    "        for i, brand in enumerate(df['Top Brands'].unique()):\n",
    "            # Determine the product key based on the first two elements of the key\n",
    "            product_key = key.split('|')[0] + '|' + key.split('|')[1]\n",
    "            \n",
    "            # Get the maximum total size for the product key, if it exists\n",
    "            max_size = max_total_size.get(product_key, None)\n",
    "            # Filter rows for the current brand and check if total size is within the maximum allowed size\n",
    "            if max_size is not None:\n",
    "                brand_df = df[(df['Top Brands'] == brand) & (df['Total Size'] <= max_size)]\n",
    "            else:\n",
    "                brand_df = pd.DataFrame()\n",
    "                \n",
    "            # Calculate recruitment ratio if the brand has data and total size is greater than zero\n",
    "            #brand_total = df[(df['Top Brands'] == brand + ' Total')]['Promo Value'].values\n",
    "            brand_total = df[(df['Top Brands'].str.strip() == (brand + ' Total').strip())]['Promo Value'].values\n",
    "\n",
    "            if not brand_df.empty and brand_total.size > 0 and brand_total[0] > 0:\n",
    "                brand_sum = brand_df['Promo Value'].sum() / brand_total[0]\n",
    "                new_data.append({'Top Brands': brand, 'Recruitment': brand_sum, 'Consumption': 1 - brand_sum, 'Value Share': df['Value Share'][i], 'SUM':brand_df['Promo Value'].sum()})\n",
    "        \n",
    "        # Create a new DataFrame with cleaned data\n",
    "        new = pd.DataFrame(new_data)\n",
    "        new.fillna(0, inplace=True)\n",
    "        \n",
    "        # Add cleaned data to the dictionary if it contains non-zero rows\n",
    "        if new.shape[0] != 0:\n",
    "            data_cleaned[key] = new\n",
    "        \n",
    "    return data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7a1182-ecdd-40a8-8d9e-a110ad873ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VSOD_Clean(VSOD_Data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess VSOD data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - VSOD_Data (dict): Dictionary containing VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned VSOD DataFrames.\n",
    "    \"\"\"\n",
    "    VSOD_cleaned = {}\n",
    "    for key in VSOD_Data:\n",
    "        # Skip first 11 rows\n",
    "        df = VSOD_Data[key].iloc[11:]\n",
    "        # Set column names and skip first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        # Fill NaN values with 0\n",
    "        df['Sector'].fillna(method='ffill', inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        VSOD_cleaned[key] = df\n",
    "    return VSOD_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32522c1-88ce-4f84-b71c-4e777b408673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(VSODClient_Cleaned, VSODCleaned):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of DataFrames based on a common column.\n",
    "\n",
    "    Parameters:\n",
    "    - VSODClient_Cleaned (dict): Dictionary containing cleaned VSOD client DataFrames.\n",
    "    - VSODCleaned (dict): Dictionary containing cleaned VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing merged DataFrames.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for key in VSODClient_Cleaned:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        VSOD_total = VSODCleaned[key][VSODCleaned[key]['Sector'].str.contains('Total', case=False, na=False)]\n",
    "        VSODClient_total = VSODClient_Cleaned[key][VSODClient_Cleaned[key]['Sector'].str.contains('Total', case=False, na=False)]\n",
    "        merged_df = pd.merge(VSOD_total, VSODClient_total, on=['Sector','Segment'], how='left')\n",
    "        merged_df.fillna(0,inplace=True)\n",
    "        merged_dict[key] = merged_df     \n",
    "        if len(sectors)!=0:\n",
    "            for se in sectors:\n",
    "                dfVSOD_client = VSODClient_Cleaned[key][VSODClient_Cleaned[key]['Sector'].str.contains(se, case=False, na=False)]\n",
    "                dfVSOD = VSODCleaned[key][VSODCleaned[key]['Sector'].str.contains(se, case=False, na=False)]\n",
    "                merged_df = pd.merge(dfVSOD, dfVSOD_client, on=['Sector','Segment'], how='left')\n",
    "                merged_df.fillna(0,inplace=True)\n",
    "                new_key = key + ' | ' +se\n",
    "                merged_dict[new_key] = merged_df\n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929bda61-bb8b-4a6b-ac3f-269bf1b416c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VSODClient_Cleaned = cleaningData(VSOD_Client)\n",
    "VSODCleaned = VSOD_Clean(VSOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e64cccc-6619-435f-b9f6-ac3f3e20efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionBrandsP12M = cleaningdata_with_grand_total(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad3da6e-fb47-4873-b875-bd146d89d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionProductsP12M = cleaningData(promotions_products_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b40a1e66-80f4-4dd0-9af8-a1a4e1cdc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionEndOfWeek = cleaningData(promotions_EndOfWeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c86f6a2a-90f7-40b1-8f9f-723f5578455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_valueUplift = cleaningData(value_uplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3253f1fa-b573-493b-b3c6-d825cee538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning13New(data):\n",
    "    \"\"\"\n",
    "    Clean and process data for specific brands and regions.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing raw data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned and processed data.\n",
    "    \"\"\"\n",
    "    data_cleaned = {}\n",
    "    \n",
    "    # Define maximum total size for each combination of product type and region\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        # Skip processing if the region is 'NATIONAL' or 'National'\n",
    "        if 'National' in key:\n",
    "           continue\n",
    "        \n",
    "        new_data = []\n",
    "        \n",
    "        # Skip first 12 rows as they are headers and metadata\n",
    "        df = df.iloc[12:]\n",
    "        \n",
    "        # Set columns names based on the first row, and skip the first row\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        # Fill missing values in 'Top Brands' column with the previous non-null value\n",
    "        df['Top Brands'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "        # Filter out rows where 'Top Brands' is 'Grand Total' or 'Other'\n",
    "        df = df[(df['Top Brands'] != 'Grand Total') & (df['Top Brands'] != 'Other')]\n",
    "        # Remove 'GR' suffix from 'Total Size' and convert it to integer\n",
    "        df['Total Size'] = df['Total Size'].str.extract('(\\d+)', expand=False)\n",
    "        df.fillna('0',inplace=True)\n",
    "        df['Total Size'] = df['Total Size'].astype(int)\n",
    "        \n",
    "        # Sort data by 'Value Share' in descending order\n",
    "        df = df.sort_values(by='Value Share', ascending=False).reset_index(drop=True)\n",
    "        for i, brand in enumerate(df['Top Brands'].unique()):\n",
    "            # Determine the product key based on the first two elements of the key\n",
    "            product_key = key.split('|')[0] + '|' + key.split('|')[1]\n",
    "            \n",
    "            # Get the maximum total size for the product key, if it exists\n",
    "            max_size = max_total_size.get(product_key, None)\n",
    "            # Filter rows for the current brand and check if total size is within the maximum allowed size\n",
    "            if max_size is not None:\n",
    "                brand_df = df[(df['Top Brands'] == brand) & (df['Total Size'] <= max_size)]\n",
    "            else:\n",
    "                brand_df = pd.DataFrame()\n",
    "                \n",
    "            # Calculate recruitment ratio if the brand has data and total size is greater than zero\n",
    "            #brand_total = df[(df['Top Brands'] == brand + ' Total')]['Promo Value'].values\n",
    "            brand_total = df[(df['Top Brands'].str.strip() == (brand + ' Total').strip())]['Promo Value'].values\n",
    "\n",
    "            if not brand_df.empty and brand_total.size > 0 and brand_total[0] > 0:\n",
    "                brand_sum = brand_df['Promo Value'].sum() / brand_total[0]\n",
    "                new_data.append({'Top Brands': brand, 'Recruitment': brand_sum, 'Consumption': 1 - brand_sum, 'Value Share': df['Value Share'][i], 'SUM':brand_df['Promo Value'].sum()})\n",
    "        \n",
    "        # Create a new DataFrame with cleaned data\n",
    "        new = pd.DataFrame(new_data)\n",
    "        new.fillna(0, inplace=True)\n",
    "        \n",
    "        # Add cleaned data to the dictionary if it contains non-zero rows\n",
    "        if new.shape[0] != 0:\n",
    "            data_cleaned[key] = new\n",
    "        \n",
    "    return data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef87a78-311f-4010-931d-7c0f0840565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_total_size = {\n",
    "        'Cesar | Disposable': 8.6,\n",
    "        'Cesar | System': 5.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da7fc3d-461c-4a38-8ae2-8d19805e5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newModifiedBrands = cleaning13New(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b2826c6-ecb1-4367-b9bc-4e8b09fc2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VSOD_merged = merging(VSODClient_Cleaned,VSODCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dd04ba3-5570-4ad4-8d61-252872480a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emptydf={}\n",
    "promotionsBrandSortedTotal=dfSort(modified_promotionBrandsP12M, client_brands, \"Top Brands\", num=8,salesCol='Promo Value')\n",
    "keys = list(promotionsBrandSortedTotal)\n",
    "for key in keys:\n",
    "     df = promotionsBrandSortedTotal[key].reset_index(drop=True)\n",
    "     df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "     if len(df) ==0:\n",
    "          del promotionsBrandSortedTotal[key]\n",
    "          emptydf[key] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75e0e42f-2500-4d43-9ed2-2b5ac567a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands_og = selectedBrands\n",
    "selectedBrands= selectedBrands + [\"Grand Total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab470824-4116-42ea-b87c-150650a1d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsSelected={key:modified_promotionBrandsP12M[key][modified_promotionBrandsP12M[key]['Top Brands'].isin(selectedBrands)].sort_values(by='Promo Value',ascending=False) for key in modified_promotionBrandsP12M.keys()   if all(cat != key.split(' | ')[0] for cat in categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5a2a31b-4190-44a7-b990-299b277941ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands=selectedBrands_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eabcdd35-a8e6-4667-8efe-17c5dea262ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAttribute(dic, marketList):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames and a list of markets, and concatenates\n",
    "    the DataFrames by adding a 'SOURCE' column to each DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are strings in the format 'market | source', and\n",
    "                values are DataFrames containing market data.\n",
    "    marketList (list): A list of market names (strings).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are market names and values are concatenated DataFrames\n",
    "          with an added 'SOURCE' column.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict to store the resulting DataFrames\n",
    "    marketDic = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the list of markets\n",
    "    for market in marketList:\n",
    "        # Iterate through the items in the dictionary\n",
    "        for key, value in dic.items():\n",
    "            # Check if the market name matches the key's market part\n",
    "            if market == key.split(' | ')[0]:\n",
    "                # Extract the source part from the key and assign it to the 'SOURCE' column\n",
    "                value['SOURCE'] = list(set(key.split(' | ')) - set([market]))[0]\n",
    "                # Only include rows where 'SOURCE' is not 'National'\n",
    "                if (value['SOURCE'] != 'National').all():\n",
    "                    marketDic[market].append(value)\n",
    "\n",
    "        # Concatenate all DataFrames in the list for each market\n",
    "        if len(marketDic[market]) != 0:\n",
    "            marketDic[market] = pd.concat(marketDic[market])\n",
    "    \n",
    "    return marketDic\n",
    "\n",
    "def fillingMissingBrands(dic):\n",
    "    \"\"\"\n",
    "    This function fills in missing brands for each market and source combination in the\n",
    "    provided dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are market names and values are DataFrames\n",
    "                containing market data with 'Top Brands' and 'SOURCE' columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: The input dictionary with missing brands filled in each DataFrame.\n",
    "    \"\"\"\n",
    "    # Iterate through the dictionary items\n",
    "    for key, value in dic.items():\n",
    "        # Get the unique list of top brands in the DataFrame\n",
    "\n",
    "   \n",
    "        brandList = value['Top Brands'].unique().tolist()\n",
    " \n",
    "        # Iterate through the unique sources in the DataFrame\n",
    "        for source in value['SOURCE'].unique():\n",
    "            # Check if the number of unique brands for the source is less than the total unique brands\n",
    "            if value[value['SOURCE'] == source]['Top Brands'].nunique() != len(brandList):\n",
    "                # Find the missing brands for the source\n",
    "                missingBrand = list(set(brandList) - set(value[value['SOURCE'] == source]['Top Brands'].unique()))\n",
    "                # Create a DataFrame for the missing brands with the current source\n",
    "                missingBrand = pd.DataFrame({'Top Brands': missingBrand, 'SOURCE': source}).explode('Top Brands')\n",
    "                # Concatenate the missing brands DataFrame with the original DataFrame\n",
    "                value = pd.concat([value, missingBrand]).replace(np.nan, 0).reset_index(drop=True)\n",
    "\n",
    "        # Update the dictionary with the filled DataFrame\n",
    "        dic[key] = value\n",
    "    \n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5825f852-d5a8-4717-863c-60bce7831474",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsWithMarket=concatAttribute(promotionsBrandsSelected,marketList)\n",
    "promotionsBrandsWithMarket = fillingMissingBrands(promotionsBrandsWithMarket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "762fb5e2-0b4d-442f-9531-040980af3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_market(data, Scope):\n",
    "    final = {}\n",
    "    for k,df in data.items():\n",
    "        for key, value in Scope.items():\n",
    "            df_market = df[df['SOURCE'].isin(value)]\n",
    "            df_market = df_market.reset_index(drop=True)\n",
    "            if df_market.shape[0] >0:\n",
    "                final[k + ' | ' + value[0]] = df_market\n",
    "    return final\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2a003a6-6da6-495c-a2d9-a9b0af5a7a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newpromotionsBrandsWithMarket = split_market(promotionsBrandsWithMarket,Scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e045f79-110e-4cb8-b237-b514151b9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAttributeNew(sorted):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames sorted by keys and concatenates the DataFrames\n",
    "    based on specified categories, sectors, and segments. It adds a 'SOURCE' column to each DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    sorted (dict): A dictionary where keys are strings in the format 'category | sector | segment | brand',\n",
    "                   and values are DataFrames containing market data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are the categories, sectors, and segments, and values are concatenated DataFrames\n",
    "          with an added 'SOURCE' column.\n",
    "    \"\"\"\n",
    "    # List of all categories, sectors, and segments to process\n",
    "    lis = categories + sectors + segments\n",
    "  \n",
    "    # Initialize a defaultdict to store the resulting DataFrames\n",
    "    marketDic = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the list of categories, sectors, and segments\n",
    "    for i in lis:\n",
    "        # Iterate through the items in the sorted dictionary\n",
    "        for key, value in sorted.items():\n",
    "            # Check if the current item is part of the key\n",
    "            if i in key:\n",
    "                # Split the key to extract individual parts              \n",
    "                parts = key.split(' | ')\n",
    "                # Identify the client brand (either 'Extra' or 'Kiwi')\n",
    "                if i in categories:\n",
    "                    markets = parts[1]\n",
    "                else:\n",
    "                    markets = parts[0]\n",
    "                \n",
    "                # Set the 'SOURCE' column to the client brand\n",
    "                value['SOURCE'] = markets\n",
    "                \n",
    "                marketDic[i].append(value)\n",
    "                    \n",
    "        # Concatenate all DataFrames in the list for each category/sector/segment\n",
    "        if marketDic[i]:\n",
    "            marketDic[i] = pd.concat(marketDic[i])\n",
    "    \n",
    "    return marketDic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28e29be2-2f65-4603-8858-1087e008b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = concatAttributeNew(modified_promotionBrandsP12M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3ee231f-9b67-4eef-ad0a-ae6e7d75eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        df = modified_promotionProductsP12M[key].sort_values(by=['Promo Share'], ascending=False)\n",
    "        \n",
    "        # Filter and sort the DataFrame\n",
    "        df = df[df['Product'] != '']\n",
    "        df['cumulative promo share'] = df['Promo Share'].cumsum()\n",
    "        df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "        df = df[df['VSOD'] >= 0.05]\n",
    "        df = df[df['cumulative promo share'] <= 0.8]\n",
    "        df = df.sort_values(by='Incr Value', ascending=False).reset_index(drop=True)\n",
    "        df = df.head(50)\n",
    "        df['index'] = str(df.index + 1)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] >0:\n",
    "            cleaned_data[key] = df\n",
    "        #else:\n",
    "            #print(key)\n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3df8ceaa-7083-4015-8ccb-6202d9ac4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data3(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        for client in client_brands:\n",
    "            \n",
    "            df = modified_promotionProductsP12M[key]\n",
    "\n",
    "            # Filter the DataFrame for the current client brand\n",
    "            df = df[df['Product'] != '']\n",
    "            df = df[df['Top Brands'] == client]\n",
    "            df = df.sort_values(by='Top Brands')\n",
    "            df['cumulative promo share'] = df.groupby('Top Brands')['Promo Share'].cumsum()\n",
    "            df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "            df = df[df['VSOD'] >= 0.05]\n",
    "            df = df.sort_values(by='Incr Value', ascending=False).tail(20).reset_index(drop=True)\n",
    "            df = df.sort_values(by ='Incr Value', ascending= True).reset_index(drop=True)\n",
    "            if df.shape[0] >0:\n",
    "                cleaned_data[key+' | '+client] = df\n",
    "                \n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5316486-6b68-430c-963f-11df7ef98c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20clientonly = filter_data3(modified_promotionProductsP12M)\n",
    "bottom20clientonly = filter_data3(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49e7f809-dc1b-40c3-b5ae-7e04543b2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_modified_promotionProductsP12M = filter_data(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f3e9198-c36a-4544-8a00-92a1e2ab89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotionsEndOfWeekCleaning(promotions_EndOfWeek, notInScope, col='Top Brands'):\n",
    "    \"\"\"\n",
    "    Clean promotions end of week data.\n",
    "\n",
    "    Parameters:\n",
    "    promotions_EndOfWeek (dict): Dictionary containing promotions end of week data.\n",
    "    notInScope (list): List of items not in scope to be excluded.\n",
    "    col (str): Column name to check for filtering. Default is 'Top Brands'.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing cleaned promotions end of week data.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store cleaned promotions end of week data\n",
    "    promotionsEndOfWeek = {}\n",
    "    \n",
    "    # Iterate over items in promotions_EndOfWeek dictionary\n",
    "    for key, value in promotions_EndOfWeek.items():\n",
    "        # Make a copy of the dataframe to avoid modifying the original data\n",
    "        df = value.copy()\n",
    "\n",
    "        # Check if the dataframe is not empty\n",
    "        if df.shape[0] != 0:\n",
    "            # Modify the key to match the desired format if applicable\n",
    "            modified_key = key\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "            \n",
    "            # Check if the key contains any item from the notInScope list\n",
    "            # If not, add the dataframe to the cleaned dictionary after filtering out 'Grand Total' rows\n",
    "            flag = False if any(element in modified_key for element in notInScope) else True\n",
    "            if flag:\n",
    "                promotionsEndOfWeek[modified_key] = df[df[col] != 'Grand Total'].reset_index(drop=True).replace(np.nan, 0)\n",
    "            \n",
    "        else:\n",
    "            print(key, ' Is empty')\n",
    "    \n",
    "    return promotionsEndOfWeek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3bdd031-7086-48bc-b9b6-d9e09be82bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=cleaningData(promotions_EndOfWeek)\n",
    "promotionsEndOfWeekCleaned=promotionsEndOfWeekCleaning(mod,notInScope,col='End of Week')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e7408ac-109a-48e1-83ee-1f3acda454ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarket = list(set([key.split(' | ')[0]+' | '+key.split(' | ')[1] for key in promotionsEndOfWeekCleaned]))\n",
    "brandMarketCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat in key.split(' | ')[-1] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in subcategories )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33070cde-4b18-410c-bf87-00d132014a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeDates(dfList, promotionsEndOfWeekCleaned):\n",
    "    \"\"\"\n",
    "    Complete dates for each dataframe in dfList based on promotionsEndOfWeekCleaned.\n",
    "\n",
    "    Parameters:\n",
    "    dfList (list): List of dataframe keys.\n",
    "    promotionsEndOfWeekCleaned (dict): Dictionary containing cleaned promotions end of week data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing EndOfWeekcompletDate (dictionary), dfGroup (list), and dic (dictionary).\n",
    "    \"\"\"\n",
    "    # Create a list of unique brand-category combinations\n",
    "    brandCatList = list(set(key.split(' | ')[0] + ' | ' + key.split(' | ')[2] for key in dfList))\n",
    "    \n",
    "    # Initialize dictionaries and lists\n",
    "    EndOfWeekcompletDate = {}\n",
    "    dfGroup = []\n",
    "    dic = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each brand-category combination\n",
    "    for key in brandCatList:\n",
    "        for name in dfList:\n",
    "            if (key.split(' | ')[0] == name.split(' | ')[0]) and (key.split(' | ')[1] == name.split(' | ')[2]):\n",
    "                dic[key] += 1\n",
    "                \n",
    "    # Iterate over unique brand-category combinations\n",
    "    for name in dic.keys():\n",
    "        # Get dataframe keys associated with the current brand-category combination\n",
    "        dfName = [key for key in dfList if name == (key.split(' | ')[0] + ' | ' + key.split(' | ')[2])]\n",
    "        \n",
    "        # Extract unique dates from all associated dataframes\n",
    "        uniqueDates = pd.concat([promotionsEndOfWeekCleaned[key] for key in dfName])[['End of Week']].drop_duplicates()\n",
    "        \n",
    "        # Initialize dictionary for complete dates for each dataframe\n",
    "        dfCompleteDates = {}\n",
    "        \n",
    "        # Add dataframe keys to the group list\n",
    "        dfGroup.append(dfName)\n",
    "        \n",
    "        # Populate EndOfWeekcompletDate dictionary with dataframes merged on unique dates\n",
    "        for key in dfName:\n",
    "            EndOfWeekcompletDate[key] = pd.merge(uniqueDates, promotionsEndOfWeekCleaned[key], how='left').replace(np.nan, 0)\n",
    "   \n",
    "    return EndOfWeekcompletDate, dfGroup, dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26bde262-eab5-4afc-ab44-9c3658cb974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory,catGroup,catDuplication=completeDates(brandMarketCategory,promotionsEndOfWeekCleaned)\n",
    "if len(sectors) != 0:\n",
    "    dfSector,secGroup,secDuplication=completeDates(brandMarketSector,promotionsEndOfWeekCleaned)\n",
    "if len(segments) != 0:\n",
    "    dfSegment,segGroup,segDuplication=completeDates(brandMarketSegment,promotionsEndOfWeekCleaned)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment,subsegGroup,subsegDuplication=completeDates(brandMarketSubSegment,promotionsEndOfWeekCleaned)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory,subcatGroup,subcatDuplication=completeDates(brandMarketSubCategory,promotionsEndOfWeekCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "43c6f94a-d00f-46ad-9011-678073e6bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoRet ={}\n",
    "first_key, first_value = next(iter(catDuplication.items()))\n",
    "sec_key, sec_value = next(iter(secDuplication.items()))\n",
    "third_key, third_value = next(iter(segDuplication.items()))\n",
    "fourth_key, fourth_value = next(iter(subsegDuplication.items()))\n",
    "fifth_key, fifth_value = next(iter(subcatDuplication.items()))\n",
    "\n",
    "PromoRet = {first_key: first_value,sec_key:sec_value,third_key: third_value, fourth_key:fourth_value,fifth_key:fifth_value }\n",
    "\n",
    "# PromoRet.update(secDuplication)\n",
    "# PromoRet.update(segDuplication)\n",
    "# PromoRet.update(subsegDuplication)\n",
    "# PromoRet.update(subcatDuplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "04d4dc2e-b239-4946-8c42-fb06ab1bab7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Xtreme 3 | Male Shaving': 1,\n",
       " 'Quattro | Disposable': 1,\n",
       " 'Quattro | Rasoi': 1,\n",
       " 'Quattro | Rasoi Sensitive': 1,\n",
       " 'Hydro | 3 Blades': 1}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromoRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "45326cc8-c1cb-4ef7-aa36-43b7b0cd0770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Xtreme 3 | Male Shaving': 1,\n",
       "             'Duplo | Male Shaving': 1,\n",
       "             'Extra 2 | Male Shaving': 1,\n",
       "             'Extra 3 | Male Shaving': 1,\n",
       "             'Classic | Male Shaving': 1,\n",
       "             'Quattro Titanium | Male Shaving': 1,\n",
       "             'Quattro | Male Shaving': 1,\n",
       "             'Hydro | Male Shaving': 1})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "300cafa7-0392-422e-984a-d4271ae22422",
   "metadata": {},
   "outputs": [],
   "source": [
    "slidePromoValueIndex=[i+16 for i in catDuplication.values()],[i+16 for i in secDuplication.values()],[i+16 for i in segDuplication.values(), ]\n",
    "\n",
    "index = [0,1,2,3,4,5,6,7,8,9,10,11,12,13 if promo_type != False else None, 14 if feature_share != False else None,\n",
    "         15 if display_share != False else None, 16,*slidePromoValueIndex,21]\n",
    "\n",
    "index = [x for x in index if x is not None]\n",
    "\n",
    "\n",
    "len_brands = len(modified_promotionBrandsP12M)\n",
    "len_Prod = len(modified_promotionProductsP12M)\n",
    "len_modified_prod = len(new_modified_promotionProductsP12M)\n",
    "len_client_market = len(client_brands) * len(VSOD_merged)\n",
    "duplication = [ len_brands, len(promotionsBrandSortedTotal), len(promotionsBrandsWithMarket) if len(sectors)>0 else 0, len(promotionsBrandsWithMarket)*len(sectors) if len(segments)>0 else 0, len(concated),len_Prod, len_modified_prod, len_modified_prod,\n",
    "               len(top20clientonly), len(bottom20clientonly),len_client_market, len_brands, len(newModifiedBrands), \n",
    "               len(newModifiedBrands) if promo_type != False else None, len_brands if feature_share != False else None , \n",
    "               len_brands if display_share!= False else None,  len(modified_promotionEndOfWeek),\n",
    "               1 if len(categories)>0 and len(slidePromoValueIndex[0])!=0 else 0 ,1 if len(sectors)>0 and len(slidePromoValueIndex[1])!=0  else 0 ,1 if len(segments)>0 and len(slidePromoValueIndex[2])!=0  else 0, len(modified_valueUplift)]\n",
    "\n",
    "duplication = [x for x in duplication if x is not None]\n",
    "\n",
    "section_names = [\"Promo Value Sales\",\"Promo Evolution\",\"VSOD Summary by Sector\" if len(sectors)>0 else \"\",\"VSOD Summary by Segment\" if len(segments)>0 else \"\", \"Value uplift by retailer by brand\", \"Volume Uplift vs discount depth\",\n",
    "                 \"Value Uplift vs Promo Efficiency Quadrant\",\"Top 20 promotions\",\"Top 20 promotions CLIENT ONLY\",\"Bottom 20 promotions CLIENT ONLY\",\n",
    "                \"Volume Sold on Deal\",\"Promo share vs Value Share\",\"Promo Sales by total size\",\"Promo Sales by promo type\" if promo_type != False else None\n",
    "                 ,\"Feature Share vs Fair Share \"if feature_share != False else None, \"Display Share vs Fair Share\" if display_share!= False else None,\n",
    "                 \"Promo Frequency learnings\",\"Category Promo sales per retailer\" if len(categories)>0  and len(slidePromoValueIndex[0])!=0  else \"\", \"Sector Promo sales per retailer\"if len(sectors)>0  and len(slidePromoValueIndex[1])!=0 else \"\",\n",
    "                 \"Segment Promo sales per retailer\"if len(segments)>0  and len(slidePromoValueIndex[2])!=0  else \"\",\"Value Uplift vs discount depth\"]\n",
    "\n",
    "section_names = [x for x in section_names if x is not None]\n",
    "\n",
    "\n",
    "path = os.getcwd() + '//slide base.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated.pptx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2dbf29db-9cdf-448a-9dec-6fd262106d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_df(data, lis):\n",
    "    matching_keys = [key for category in lis for key in data.keys() if category in key.split(' | ')]        \n",
    "    return matching_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e50f0cfb-8776-419f-9729-6397863b81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_duplications(Scope,count_df, data_frames):\n",
    "    combined_duplications = []\n",
    "    for data_frame in data_frames:\n",
    "        duplications = [len(count_df(data_frame, value if isinstance(value, list) else [value])) for key, value in Scope.items()]\n",
    "        combined_duplications.extend(duplications)\n",
    "    return combined_duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "198c34c6-7ecc-4956-85c0-e33814b353bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope = {\n",
    "    \"Category\": categories,\n",
    "    \"Sector\": sectors,\n",
    "    \"Segment\": segments,\n",
    "    \"Subsegment\": subsegments,\n",
    "    \"Subcategory\": subcategories\n",
    "}\n",
    "suffixes = [\"Category\", \"Sector\", \"Segment\", \"Subsegments\", \"Subcategory\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c947100e-92c3-4664-9c91-a67279d5f9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_duplications(Scope,count_df,[PromoRet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ceca67f7-038d-44e1-98aa-cbc9a32d8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [0,0,0,0,0,\n",
    "         1,1,1,1,1,\n",
    "         2,2,2,2,2,\n",
    "         4,4,4,4,4,\n",
    "         5,5,5,5,5,\n",
    "         6,6,6,6,6,\n",
    "         7,7,7,7,7,\n",
    "         8,8,8,8,8,\n",
    "         9,9,9,9,9,\n",
    "         11,11,11,11,11,\n",
    "         12,12,12,12,12,\n",
    "         13,13,13,13,13,\n",
    "         15,15,15,15,15,\n",
    "         16,16,16,16,16,\n",
    "         *slidePromoValueIndex,\n",
    "         21,21,21,21,21\n",
    "        ]\n",
    "duplication = combine_duplications(Scope,count_df,[modified_promotionBrandsP12M,\n",
    "                                                   promotionsBrandSortedTotal,\n",
    "                                                   newpromotionsBrandsWithMarket,\n",
    "                                                   concated,\n",
    "                                                   modified_promotionProductsP12M,\n",
    "                                                   new_modified_promotionProductsP12M,\n",
    "                                                   new_modified_promotionProductsP12M,\n",
    "                                                   top20clientonly,\n",
    "                                                   bottom20clientonly,\n",
    "                                                   modified_promotionBrandsP12M,\n",
    "                                                   newModifiedBrands,\n",
    "                                                   modified_promotionBrandsP12M,\n",
    "                                                   modified_promotionBrandsP12M,\n",
    "                                                   modified_promotionEndOfWeek,\n",
    "                                                   PromoRet,\n",
    "                                                   modified_valueUplift\n",
    "                                                  ])\n",
    "section_names = [\"Promo Value Sale\",\n",
    "                 \"Promo Evolution\",\n",
    "                 \"VSOD Summary by Sector\" ,\n",
    "                 \"Value uplift by retailer by brand\", \n",
    "                 \"Volume Uplift vs discount depth\",\n",
    "                 \"Value Uplift vs Promo Efficiency Quadrant\",\n",
    "                 \"Top 20 promotions\",\n",
    "                 \"Top 20 promotions CLIENT ONLY\",\n",
    "                 \"Bottom 20 promotions CLIENT ONLY\",\n",
    "                 \"Promo share vs Value Share\",\n",
    "                 \"Promo Sales by total size\",\n",
    "                 \"Promo Sales by promo type\",\n",
    "                 \"Display Share vs Fair Share\",\n",
    "                 \"Promo Frequency learnings\",\n",
    "                 \"Promo sales per retailer\",\n",
    "                 \"Value Uplift vs discount depth\"\n",
    "                ]\n",
    "\n",
    "\n",
    "section_names = [f\"{name} {suffix}\" for name in section_names for suffix in suffixes]\n",
    "\n",
    "path = os.getcwd() + '//slide base.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated.pptx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "328e3f76-edb0-4b0b-9c0f-28d2cc716e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "78376167-8225-4f4f-af41-cd366f3e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation(new_pre)\n",
    "posItr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2e21be34-0498-4c06-b4e4-bd68b94e38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind =0\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        promoValueSales(prs,filtered_dict,duplication[posItr],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2a49aea0-21ff-4bf4-9427-d2f7a6b8a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(promotionsBrandSortedTotal,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in promotionsBrandSortedTotal.items() if key in dict[key1]}\n",
    "        promoEvolution(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1cd4fcae-6d85-4244-82e5-9a19f05b6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newpromotionsBrandsWithMarket,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newpromotionsBrandsWithMarket.items() if key in dict[key1]}\n",
    "        VSOD1(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1b8dc777-7e02-4573-b8f9-000bb04fcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(concated,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in concated.items() if key in dict[key1]}\n",
    "        valueUpliftRetailer(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9c8ee2a2-5cdf-41f3-ad8a-95fb9baacc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        VolumeUplift(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "30f24f2e-b093-4fe6-8605-1ef06f69e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(new_modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in new_modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        ValueUpliftvsPromoEfficiencyQuadrant(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ba714fd4-a9da-45d9-9eb0-1a1889324bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(new_modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in new_modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        top20(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "df90b497-4ca3-4467-9965-3eeba68fef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(top20clientonly,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in top20clientonly.items() if key in dict[key1]}\n",
    "        top20Client(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4108f604-685c-4fd8-aab4-5c66967a67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(bottom20clientonly,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in bottom20clientonly.items() if key in dict[key1]}\n",
    "        bot20Client(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0ee42400-58ae-4311-8ac7-9914c0ae666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        PromoShare_vs_ValueShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "569341f7-9b94-4bf5-9114-fe10a6d0cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newModifiedBrands,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newModifiedBrands.items() if key in dict[key1]}\n",
    "        PromoSalesTotalSize(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "08a53a81-9e6b-4dd1-ada9-5405ea161d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_col = ['Straight Discount 10-20 Sales', 'Straight Discount 20-30 Sales', 'Straight Discount 30-40 Sales', 'Straight Discount 40+ Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2e6331b0-3674-471a-8da0-d10719174ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        PromoSalesTypes(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b2ecb5b3-8308-43ec-b4aa-16c2e7b0797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        displayShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b5ffadfa-5f94-4967-84f6-54c1d2ab7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionEndOfWeek,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionEndOfWeek.items() if key in dict[key1]}\n",
    "        PromoFrequency(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5febe962-91f9-4b04-80a0-986f63399c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slide 17\n",
    "\n",
    "\n",
    "promoSalesPerRetailer(prs,dfCategory,len(index[ind]),catGroup,position=posItr)\n",
    "posItr += len(index[ind])\n",
    "ind+=1\n",
    "\n",
    "#Sector Replace\n",
    "\n",
    "promoSalesPerRetailer(prs,dfSector,len(index[ind]),secGroup,position=posItr)\n",
    "posItr += len(index[ind])\n",
    "ind+=1\n",
    "\n",
    "if len(segments) != 0:\n",
    "    promoSalesPerRetailer(prs,dfSegment,len(index[ind]),segGroup,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "    ind+=1\n",
    "    \n",
    "if len(subsegments) != 0:\n",
    "    promoSalesPerRetailer(prs,dfSubSegment,len(index[ind]),subsegGroup,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "    ind+=1\n",
    "    \n",
    "if len(subcategories) != 0:\n",
    "    promoSalesPerRetailer(prs,dfSubCategory,len(index[ind]),subcatGroup,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "    ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4bdf01c1-6ec8-405c-bb46-928310996eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_valueUplift,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_valueUplift.items() if key in dict[key1]}\n",
    "        valueUplift(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c56e0982-087b-439b-a549-736abdbb54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath=os.getcwd() + \"\\\\test output.pptx\"\n",
    "prs.save(outputPath)\n",
    "app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "presentation = app.Presentations.Open(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "fae9547d-d140-444c-967d-6d8f59d452e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide = prs.slides[220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "7e890fbb-d739-4a79-b5cc-e728d0572cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 7/22/2024\n",
      "True \n",
      "True 221\n",
      "True Data Source | Nielsen | Ending Mar 2022\n",
      "True Promo Frequency learnings | Category | Brand | National\n",
      "True Promo Frequency learnings (Replace With SO WHAT)\n",
      "True \n",
      "True Value \n",
      "Sales (â‚¬)\n",
      "True % Promo \n",
      "Sales \n",
      "(Value)\n",
      "True Base Sales: What the Brand would have sold without Promotion\n",
      "Promo Value: Brand Sales on Promotion\n",
      "% Promo Value: % of Brand Sales sold on promotion\n"
     ]
    }
   ],
   "source": [
    "for s in slide.shapes:\n",
    "    if s.has_text_frame:\n",
    "        print(True,s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47380f6-7d9a-44c3-a76e-7f83198044b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
