{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a454f5-c4e1-460c-a27d-40aa00dfcac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"..\\general_functions\\generalFunctions.ipynb\"\n",
    "%run \"..\\Promotion Slide Duplicate\\Promotion Replacement Function.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999d128",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b6fbff-4bd3-4c15-8d5d-f3b65886bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_type = True\n",
    "\n",
    "normalized = True\n",
    "national = False \n",
    "\n",
    "display_share = False  # True if Available\n",
    "feature_share = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae417ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_total_size = {\n",
    "#         'Carrefour | Aperitif': 0.27,\n",
    "#         'Carrefour | Enfant': 0.35,\n",
    "#         'Carrefour | Frais A Tartiner': 0.25,\n",
    "#         'Carrefour | Ingredient A Chaud':0.3,\n",
    "#         'Carrefour | Salade':0.28\n",
    "#         }\n",
    "\n",
    "# custom_colors = [\n",
    "#     RGBColor(91, 159, 153),    # Darker teal\n",
    "#     RGBColor(131, 199, 193),   # Brighter medium teal\n",
    "#     RGBColor(168, 216, 212),   # Original light teal\n",
    "#     RGBColor(198, 236, 232),   # Very light teal\n",
    "#     RGBColor(111, 179, 173),\n",
    "#     RGBColor(121, 189, 183)\n",
    "# ]\n",
    "# custom_colors = [\n",
    "#     RGBColor(111, 179, 173),  \n",
    "#     RGBColor(121, 189, 183),  \n",
    "#     RGBColor(168, 216, 212),  \n",
    "#     RGBColor(178, 226, 222), \n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ff0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ManufOrTopC =\"Top Companies\"\n",
    "BrandOrTopB = \"Top Brands\"\n",
    " \n",
    "client_manuf = [\"Pbg\"]\n",
    "client_brands = [\"Schick\", \"Equate\", \"Cremo\"]\n",
    " \n",
    "decimals = 2\n",
    "sign = \"Before\"\n",
    "currency = '$'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    " \n",
    " \n",
    "categories = [\"Manual Shave Men\"]\n",
    "sectors = [\"System\",\"Disposables\"]\n",
    "segments = [\"Razors\", \"Refills\", \"Disposables\"]\n",
    "subsegments= []\n",
    "subcategories= []\n",
    " \n",
    "customareas='REGIONS'\n",
    "national = False\n",
    "areas = [\"RETAILER\", f\"{customareas}\"]\n",
    "regions_RET  = [\"Walmart\"]\n",
    "channels_RET = []\n",
    "market_RET = [\"Walmart Div1 Corp\", \"Walmart Nm Corp\", \"Walmart Sc Corp\"]\n",
    "regions_CHAN = []\n",
    "channels_CHAN = []\n",
    "market_CHAN = []\n",
    "regions_CUST = []\n",
    "channels_CUST = []\n",
    "market_CUST = [\"Walmart East\", \"Walmart North\", \"Walmart Southeast\", \"Walmart Southwest\",\t\"Walmart West\"]\n",
    "\n",
    "\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending March  2025\"\n",
    "years = ['2023', '2024', '2025']\n",
    "\n",
    "subcatg_parent = \"Segment\"\n",
    "subcatg_parent_list = segments\n",
    " \n",
    "percent = 1000000\n",
    "percentstr=\"'000 000\"\n",
    "\n",
    "\n",
    "#start_date is P3y from end date\n",
    "# Add one month to the original ending date (YYYY-MM-01)\n",
    "start_date = \"2022-04-01\"\t\n",
    "end_date = \"2025-04-01\"\n",
    "prodORitem = \"SKU\"\n",
    "\n",
    "\n",
    "\n",
    "# Guidline Promo Columns ex :Volume Uplift >>> \"[Measures].[Volume Uplift IYA]\" using filter_dictionary_keys(fieldsNamePosition, 'Volume Upli')\n",
    "#promo_col = ['[Measures].[Straight Discount 10-20 Sales]','[Measures].[Straight Discount 20-30 Sales]', '[Measures].[Straight Discount 30-40 Sales]','[Measures].[Straight Discount 40+ Sales]']\n",
    "promo_col = []\n",
    "selectedBrands = client_brands \n",
    "marketList = regions_RET + channels_RET + market_RET + regions_CHAN + channels_CHAN + market_CHAN \n",
    "notInScope = []\n",
    "OpenEditData=True\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64122dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d93407ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketList = regions_RET + channels_RET + market_RET + regions_CHAN + channels_CHAN + market_CHAN + regions_CUST + channels_CUST + market_CUST\n",
    "categoryList=categories +sectors+segments+subsegments+subcategories\n",
    "defaults = {\n",
    "    'Manual Shave Men': 6.69,\n",
    "    'Disposables': 7.98,\n",
    "    \"System\" : 5.03,\n",
    "    \"Razors\" : 2.80,\n",
    "    \"Refills\" : 6.58\n",
    "}\n",
    "diff_market_value = {\n",
    "\n",
    "}\n",
    "\n",
    "def totalsize (lis,defaultdic,diffmarketdic=[],cat1=True) :\n",
    "    if cat1:\n",
    "        max_total_size = {\n",
    "        f\"{category} | {market}\": diff_market_value.get(market.upper(), {}).get(category, defaults[category])\n",
    "        for market in lis\n",
    "        for category in defaults\n",
    "    }\n",
    "    else:\n",
    "        max_total_size = {\n",
    "        f\"{market} | {category}\": diff_market_value.get(market.upper(), {}).get(category, defaults[category])\n",
    "        for market in lis\n",
    "        for category in defaults\n",
    "    }    \n",
    "    return max_total_size  \n",
    "### if dafaults cat_hierarchy\n",
    "# max_total_size=totalsize(categoryList,defaults,diff_market_value,cat1=True)\n",
    "### if dafaults Market_hierarchy\n",
    "max_total_size=totalsize(marketList,defaults,diff_market_value,cat1=False)\n",
    "\n",
    "custom_colors = [\n",
    "    RGBColor(91, 159, 153),    # Darker teal\n",
    "    RGBColor(131, 199, 193),   # Brighter medium teal\n",
    "    RGBColor(168, 216, 212),   # Original light teal\n",
    "    RGBColor(198, 236, 232),   # Very light teal\n",
    "    RGBColor(111, 179, 173),\n",
    "    RGBColor(121, 189, 183)\n",
    "]\n",
    "# custom_colors = [\n",
    "#     RGBColor(111, 179, 173),  \n",
    "#     RGBColor(121, 189, 183),  \n",
    "#     RGBColor(168, 216, 212),  \n",
    "#     RGBColor(178, 226, 222),\n",
    "# ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3e1d5a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Walmart | Manual Shave Men': 6.69,\n",
       " 'Walmart | Disposables': 7.98,\n",
       " 'Walmart | System': 5.03,\n",
       " 'Walmart | Razors': 2.8,\n",
       " 'Walmart | Refills': 6.58,\n",
       " 'Walmart Div1 Corp | Manual Shave Men': 6.69,\n",
       " 'Walmart Div1 Corp | Disposables': 7.98,\n",
       " 'Walmart Div1 Corp | System': 5.03,\n",
       " 'Walmart Div1 Corp | Razors': 2.8,\n",
       " 'Walmart Div1 Corp | Refills': 6.58,\n",
       " 'Walmart Nm Corp | Manual Shave Men': 6.69,\n",
       " 'Walmart Nm Corp | Disposables': 7.98,\n",
       " 'Walmart Nm Corp | System': 5.03,\n",
       " 'Walmart Nm Corp | Razors': 2.8,\n",
       " 'Walmart Nm Corp | Refills': 6.58,\n",
       " 'Walmart Sc Corp | Manual Shave Men': 6.69,\n",
       " 'Walmart Sc Corp | Disposables': 7.98,\n",
       " 'Walmart Sc Corp | System': 5.03,\n",
       " 'Walmart Sc Corp | Razors': 2.8,\n",
       " 'Walmart Sc Corp | Refills': 6.58,\n",
       " 'Walmart East | Manual Shave Men': 6.69,\n",
       " 'Walmart East | Disposables': 7.98,\n",
       " 'Walmart East | System': 5.03,\n",
       " 'Walmart East | Razors': 2.8,\n",
       " 'Walmart East | Refills': 6.58,\n",
       " 'Walmart North | Manual Shave Men': 6.69,\n",
       " 'Walmart North | Disposables': 7.98,\n",
       " 'Walmart North | System': 5.03,\n",
       " 'Walmart North | Razors': 2.8,\n",
       " 'Walmart North | Refills': 6.58,\n",
       " 'Walmart Southeast | Manual Shave Men': 6.69,\n",
       " 'Walmart Southeast | Disposables': 7.98,\n",
       " 'Walmart Southeast | System': 5.03,\n",
       " 'Walmart Southeast | Razors': 2.8,\n",
       " 'Walmart Southeast | Refills': 6.58,\n",
       " 'Walmart Southwest | Manual Shave Men': 6.69,\n",
       " 'Walmart Southwest | Disposables': 7.98,\n",
       " 'Walmart Southwest | System': 5.03,\n",
       " 'Walmart Southwest | Razors': 2.8,\n",
       " 'Walmart Southwest | Refills': 6.58,\n",
       " 'Walmart West | Manual Shave Men': 6.69,\n",
       " 'Walmart West | Disposables': 7.98,\n",
       " 'Walmart West | System': 5.03,\n",
       " 'Walmart West | Razors': 2.8,\n",
       " 'Walmart West | Refills': 6.58}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7a8ab5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Scope = {\n",
    "    \"Category\": categories,\n",
    "    \"Sector\": sectors,\n",
    "    \"Segment\": segments,\n",
    "    \"Subsegment\": subsegments,\n",
    "    \"Subcategory\": subcategories\n",
    "}\n",
    "suffixes = [\"Category\", \"Sector\", \"Segment\",'SubSegment', 'SubCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0373cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectHeader(df):\n",
    "  # df = df.replace( np.nan, 0)\n",
    "  vals=df.values\n",
    "  vals=list(map(lambda x : all([type(i)==str for i in x ]),vals))\n",
    "  # print(vals)\n",
    "  break_point=0\n",
    "  for i,v in enumerate(vals):\n",
    "    if v:\n",
    "      break_point=i\n",
    "      # print(break_point)\n",
    "      break  \n",
    "  df.columns=df.iloc[break_point]\n",
    "  df=df.iloc[break_point+1:,:]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fd5a3",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "461dc096-382d-4c3b-91c1-8610a7cd684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "datasets_path = os.getcwd()+\"/Promotion Datasets/\"\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e341f",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94d9ab22-4ebf-4b74-8859-4cad1e38a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningData(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "      \n",
    "        df=DetectHeader(data[key])\n",
    "        # Set column names and skip the first row\n",
    "        if  df.columns.isna().any():\n",
    "            continue\n",
    "        # Perform specific cleaning operations based on the DataFrame columns and key\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if 'Top Brands' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df[f'{prodORitem}'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df.reset_index(drop=True)\n",
    "            \n",
    "            elif 'Top Brands' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                #df.fillna(0, inplace=True)\n",
    "                if normalized:\n",
    "                    df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])] = df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift Normalized IYA'].isna(), None, df['Value Uplift Normalized IYA'] - 1)\n",
    "                else:\n",
    "                    df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])] = df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift IYA'].isna(), None, df['Value Uplift IYA'] - 1)\n",
    "               \n",
    "                df['VSOD Evaluation vs YA'] = np.where(df['VSOD IYA'].isna(), None, df['VSOD IYA'] - 1)\n",
    "\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df[~df['Top Brands'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "\n",
    "       \n",
    "                \n",
    "            elif 'End of Week' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df[f'{prodORitem}'] = df[f'{prodORitem}'].fillna(method='ffill')\n",
    "                if normalized:\n",
    "                    df = df[(df['Value Uplift (v. base) Normalized'] >= 0)]\n",
    "                else:\n",
    "                    df = df[(df['Value Uplift (v. base)'] >= 0)]\n",
    "                df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                new_start = \"2023-07-31\"\n",
    "                df = df[(df['End of Week'] >= new_start) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df[f'{prodORitem}'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df = df[df['Promo Sales'] > 1000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                    df =  df[df['Value Uplift (v. base) Normalized']<10]\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                    df = df[df['Value Uplift (v. base']<10]\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-22|-23|-24|Jan-25')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                print(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                # df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                if 'Sector' == df.columns[1]:\n",
    "                    df[direct_parent[\"Sector\"]].fillna(method='ffill', inplace= True)\n",
    "                    df['Sector'] = df['Sector'].replace(0, np.nan)\n",
    "                    df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                    df['Sector'] = df.apply(lambda row: row[direct_parent[\"Sector\"]] if 'Total' in row[direct_parent[\"Sector\"]] and row[direct_parent[\"Sector\"]] != categories[0] else row['Sector'], axis=1)\n",
    "\n",
    "                elif 'Segment' == df.columns[1]:\n",
    "                    df['Segment'] = df['Segment'].replace(0, np.nan)  \n",
    "                    df[direct_parent[\"Segment\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['Segment'] = df.apply(lambda row: row[direct_parent[\"Segment\"]] if 'Total' in row[direct_parent[\"Segment\"]] and row[direct_parent[\"Segment\"]] != categories[0] else row['Segment'], axis=1)\n",
    "                    df['Segment'].fillna(method='ffill', inplace=True)\n",
    "                elif 'SubSegment' == df.columns[1]:\n",
    "                    df['SubSegment'] = df['SubSegment'].replace(0, np.nan)\n",
    "                    df[direct_parent[\"SubSegment\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['SubSegment'] = df.apply(lambda row: row[direct_parent[\"SubSegment\"]] if 'Total' in row[direct_parent[\"SubSegment\"]] and row[direct_parent[\"SubSegment\"]] != categories[0] else row['SubSegment'], axis=1)\n",
    "                    df['SubSegment'].fillna(method='ffill', inplace=True)\n",
    "                elif 'SubCategory' == df.columns[1]:\n",
    "                    df['SubCategory'] = df['SubCategory'].replace(0, np.nan)\n",
    "                    df[direct_parent[\"SubCategory\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['SubCategory'] = df.apply(lambda row: row[direct_parent[\"SubCategory\"]] if 'Total' in row[direct_parent[\"SubCategory\"]] and row[direct_parent[\"SubCategory\"]] != categories[0] else row['SubCategory'], axis=1)\n",
    "                    df['SubCategory'].fillna(method='ffill', inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "        if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaned_data[modified_key] = df\n",
    "        else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaned_data[key] = df\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df05b63b-986d-473b-9a40-ef7798a932ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningdata_with_grand_total(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaningdata_with_grand_total = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "   \n",
    "        df=DetectHeader(data[key])\n",
    "\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if 'Top Brands' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df[f'{prodORitem}'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "            \n",
    "            elif 'Top Brands' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                if normalized:\n",
    "                    df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])] = df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift Normalized IYA'].isna(), None, df['Value Uplift Normalized IYA'] - 1)\n",
    "                else:\n",
    "                    df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])] = df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift IYA'].isna(), None, df['Value Uplift IYA'] - 1)\n",
    "               \n",
    "                df['VSOD Evaluation vs YA'] = np.where(df['VSOD IYA'].isna(), None, df['VSOD IYA'] - 1)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                #df = df[~df['Top Brands'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "\n",
    "            elif 'End of Week' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df[f'{prodORitem}'] = df[f'{prodORitem}'].fillna(method='ffill')\n",
    "                df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df[f'{prodORitem}'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                \n",
    "                df = df[df['Promo Sales'] > 1000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-21|-22|-23|Jan-24')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "            \n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[modified_key] = df\n",
    "            else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[key] = df\n",
    "    \n",
    "    return cleaningdata_with_grand_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07630d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VSOD_Clean(VSOD_Data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess VSOD data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - VSOD_Data (dict): Dictionary containing VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned VSOD DataFrames.\n",
    "    \"\"\"\n",
    "    VSOD_cleaned = {}\n",
    "    for key in VSOD_Data:\n",
    " \n",
    "        df=DetectHeader(VSOD_Data[key])\n",
    "        # Fill NaN values with 0\n",
    "        # print(key)\n",
    "        if 'Sector' == df.columns[1]:\n",
    "            df[direct_parent[\"Sector\"]] = df[direct_parent[\"Sector\"]].replace(0, np.nan)\n",
    "            df[direct_parent[\"Sector\"]].fillna(method='ffill', inplace = True)\n",
    "            df['Sector'] = df['Sector'].replace(0, np.nan)\n",
    "            df['Sector'] = df.apply(lambda row: row[direct_parent[\"Sector\"]] if 'Total' in row[direct_parent[\"Sector\"]] and row[direct_parent[\"Sector\"]] != categories[0] else row['Sector'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"Sector\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"Sector\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"Sector\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['Sector'].fillna(method='ffill', inplace=True)\n",
    "            df.fillna(0, inplace=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] > 0:\n",
    "            VSOD_cleaned[key] = df\n",
    "    return VSOD_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd7a1182-ecdd-40a8-8d9e-a110ad873ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VSOD_Clean(VSOD_Data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess VSOD data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - VSOD_Data (dict): Dictionary containing VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned VSOD DataFrames.\n",
    "    \"\"\"\n",
    "    VSOD_cleaned = {}\n",
    "    for key in VSOD_Data:\n",
    " \n",
    "        df=DetectHeader(VSOD_Data[key])\n",
    "        # Fill NaN values with 0\n",
    "        # print(key)\n",
    "        if 'Sector' == df.columns[1]:\n",
    "            df[direct_parent[\"Sector\"]] = df[direct_parent[\"Sector\"]].replace(0, np.nan)\n",
    "            df[direct_parent[\"Sector\"]].fillna(method='ffill', inplace = True)\n",
    "            df['Sector'] = df['Sector'].replace(0, np.nan)\n",
    "            df['Sector'] = df.apply(lambda row: row[direct_parent[\"Sector\"]] if 'Total' in row[direct_parent[\"Sector\"]] and row[direct_parent[\"Sector\"]] != categories[0] else row['Sector'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"Sector\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"Sector\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"Sector\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['Sector'].fillna(method='ffill', inplace=True)\n",
    "        elif 'Segment' == df.columns[1]:\n",
    "            df[direct_parent[\"Segment\"]].fillna(method='ffill', inplace=True)\n",
    "            df['Segment'] = df['Segment'].replace(0, np.nan)\n",
    "            # df=df[~df['Sector'].str.contains(r'\\btotal\\b', case=False) | df['Sector'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['Segment'] = df.apply(lambda row: row[direct_parent[\"Segment\"]] if 'Total' in row[direct_parent[\"Segment\"]] and row[direct_parent[\"Segment\"]] != categories[0] else row['Segment'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"Segment\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"Segment\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"Segment\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['Segment'].fillna(method='ffill', inplace=True)    \n",
    "        elif 'SubSegment' == df.columns[1]:\n",
    "            df['SubSegment'] = df['SubSegment'].replace(0, np.nan)\n",
    "            df[direct_parent[\"SubSegment\"]].fillna(method='ffill', inplace=True)\n",
    "            # df=df[~df['Segment'].str.contains(r'\\btotal\\b', case=False) | df['Segment'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubSegment'] = df.apply(lambda row: row[direct_parent[\"SubSegment\"]] if 'Total' in row[direct_parent[\"SubSegment\"]] and row[direct_parent[\"SubSegment\"]] != categories[0] else row['SubSegment'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"SubSegment\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"SubSegment\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"SubSegment\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubSegment'].fillna(method='ffill', inplace=True)\n",
    "        elif 'SubCategory' == df.columns[1]:\n",
    "            df['SubCategory'] = df['SubCategory'].replace(0, np.nan)\n",
    "            df[direct_parent[\"SubCategory\"]].fillna(method='ffill', inplace=True)\n",
    "            # df=df[~df['Segment'].str.contains(r'\\btotal\\b', case=False) | df['Segment'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubCategory'] = df.apply(lambda row: row[direct_parent[\"SubCategory\"]] if 'Total' in row[direct_parent[\"SubCategory\"]] and row[direct_parent[\"SubCategory\"]] != categories[0] else row['SubCategory'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"SubCategory\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"SubCategory\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"SubCategory\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubCategory'].fillna(method='ffill', inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] > 0:\n",
    "            VSOD_cleaned[key] = df\n",
    "    return VSOD_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d32522c1-88ce-4f84-b71c-4e777b408673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(VSODClient_Cleaned, VSODCleaned, col):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of DataFrames based on a common column.\n",
    "\n",
    "    Parameters:\n",
    "    - VSODClient_Cleaned (dict): Dictionary containing cleaned VSOD client DataFrames.\n",
    "    - VSODCleaned (dict): Dictionary containing cleaned VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing merged DataFrames.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for key in VSODClient_Cleaned:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        #merged_df = pd.merge(VSODCleaned[key], VSODClient_Cleaned[key], on=col, how='left')\n",
    "        merged_df = pd.merge(VSODClient_Cleaned[key],VSODCleaned[key], on=col, how='left')\n",
    "        #merged_df = merged_df.dropna(subset=['Grand Total'])\n",
    "        merged_df['Grand Total'] = merged_df['Grand Total'].fillna(0)\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            merged_dict[key] = merged_df     \n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7caef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified_promotionBrandsP12M_display = cleaningData(display_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1c2b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionBrandsP12M = cleaningData(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77d32fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionBrandsP12M_total = cleaningdata_with_grand_total(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ad3da6e-fb47-4873-b875-bd146d89d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionProductsP12M = cleaningData(promotions_products_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5159e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionProductsP12M_updated = {}\n",
    "for key, df in modified_promotionProductsP12M.items():\n",
    "    df = df.copy()\n",
    "    df = df[df[f'{prodORitem}'] != '']\n",
    "    df = df[df['Promo Sales'] >= 10000]\n",
    "    df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "    if not df.empty:\n",
    "        modified_promotionProductsP12M_updated[key] = df\n",
    "modified_promotionProductsP12M_volumeuplift = modified_promotionProductsP12M_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b40a1e66-80f4-4dd0-9af8-a1a4e1cdc795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "82   2024-12-15\n",
      "83   2024-12-22\n",
      "84   2025-01-12\n",
      "85   2025-01-19\n",
      "86   2025-01-26\n",
      "Name: End of Week, Length: 87, dtype: datetime64[ns]\n",
      "0   2023-11-12\n",
      "1   2023-11-26\n",
      "2   2024-02-04\n",
      "3   2024-02-11\n",
      "4   2024-03-10\n",
      "5   2024-06-23\n",
      "6   2024-08-11\n",
      "7   2024-09-29\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "90   2024-12-01\n",
      "91   2025-01-05\n",
      "92   2025-01-12\n",
      "93   2025-01-19\n",
      "94   2025-01-26\n",
      "Name: End of Week, Length: 95, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "87   2024-10-20\n",
      "88   2025-01-05\n",
      "89   2025-01-12\n",
      "90   2025-01-19\n",
      "91   2025-01-26\n",
      "Name: End of Week, Length: 92, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "92   2024-12-29\n",
      "93   2025-01-05\n",
      "94   2025-01-12\n",
      "95   2025-01-19\n",
      "96   2025-01-26\n",
      "Name: End of Week, Length: 97, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "82   2024-12-15\n",
      "83   2024-12-22\n",
      "84   2025-01-12\n",
      "85   2025-01-19\n",
      "86   2025-01-26\n",
      "Name: End of Week, Length: 87, dtype: datetime64[ns]\n",
      "0   2023-11-12\n",
      "1   2023-11-26\n",
      "2   2024-02-04\n",
      "3   2024-02-11\n",
      "4   2024-03-10\n",
      "5   2024-06-23\n",
      "6   2024-08-11\n",
      "7   2024-09-29\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "90   2024-12-01\n",
      "91   2025-01-05\n",
      "92   2025-01-12\n",
      "93   2025-01-19\n",
      "94   2025-01-26\n",
      "Name: End of Week, Length: 95, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "87   2024-10-20\n",
      "88   2025-01-05\n",
      "89   2025-01-12\n",
      "90   2025-01-19\n",
      "91   2025-01-26\n",
      "Name: End of Week, Length: 92, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "92   2024-12-29\n",
      "93   2025-01-05\n",
      "94   2025-01-12\n",
      "95   2025-01-19\n",
      "96   2025-01-26\n",
      "Name: End of Week, Length: 97, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "67   2024-11-10\n",
      "68   2024-11-17\n",
      "69   2025-01-12\n",
      "70   2025-01-19\n",
      "71   2025-01-26\n",
      "Name: End of Week, Length: 72, dtype: datetime64[ns]\n",
      "0    2023-01-22\n",
      "1    2023-01-29\n",
      "2    2023-02-05\n",
      "3    2023-02-12\n",
      "4    2023-02-19\n",
      "        ...    \n",
      "74   2024-12-15\n",
      "75   2024-12-22\n",
      "76   2025-01-12\n",
      "77   2025-01-19\n",
      "78   2025-01-26\n",
      "Name: End of Week, Length: 79, dtype: datetime64[ns]\n",
      "0   2023-11-12\n",
      "1   2023-11-26\n",
      "2   2024-02-04\n",
      "3   2024-02-11\n",
      "4   2024-03-10\n",
      "5   2024-06-23\n",
      "6   2024-08-11\n",
      "7   2024-09-29\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "89   2024-12-29\n",
      "90   2025-01-05\n",
      "91   2025-01-12\n",
      "92   2025-01-19\n",
      "93   2025-01-26\n",
      "Name: End of Week, Length: 94, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "101   2024-12-29\n",
      "102   2025-01-05\n",
      "103   2025-01-12\n",
      "104   2025-01-19\n",
      "105   2025-01-26\n",
      "Name: End of Week, Length: 106, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "77   2024-11-17\n",
      "78   2025-01-05\n",
      "79   2025-01-12\n",
      "80   2025-01-19\n",
      "81   2025-01-26\n",
      "Name: End of Week, Length: 82, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "85   2024-11-17\n",
      "86   2024-12-01\n",
      "87   2025-01-12\n",
      "88   2025-01-19\n",
      "89   2025-01-26\n",
      "Name: End of Week, Length: 90, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "77   2024-12-29\n",
      "78   2025-01-05\n",
      "79   2025-01-12\n",
      "80   2025-01-19\n",
      "81   2025-01-26\n",
      "Name: End of Week, Length: 82, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "91   2024-12-29\n",
      "92   2025-01-05\n",
      "93   2025-01-12\n",
      "94   2025-01-19\n",
      "95   2025-01-26\n",
      "Name: End of Week, Length: 96, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "75   2024-10-13\n",
      "76   2025-01-05\n",
      "77   2025-01-12\n",
      "78   2025-01-19\n",
      "79   2025-01-26\n",
      "Name: End of Week, Length: 80, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "78   2024-09-29\n",
      "79   2024-10-20\n",
      "80   2025-01-12\n",
      "81   2025-01-19\n",
      "82   2025-01-26\n",
      "Name: End of Week, Length: 83, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "72   2024-12-29\n",
      "73   2025-01-05\n",
      "74   2025-01-12\n",
      "75   2025-01-19\n",
      "76   2025-01-26\n",
      "Name: End of Week, Length: 77, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "92   2024-12-29\n",
      "93   2025-01-05\n",
      "94   2025-01-12\n",
      "95   2025-01-19\n",
      "96   2025-01-26\n",
      "Name: End of Week, Length: 97, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "73   2024-12-29\n",
      "74   2025-01-05\n",
      "75   2025-01-12\n",
      "76   2025-01-19\n",
      "77   2025-01-26\n",
      "Name: End of Week, Length: 78, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "89   2024-12-29\n",
      "90   2025-01-05\n",
      "91   2025-01-12\n",
      "92   2025-01-19\n",
      "93   2025-01-26\n",
      "Name: End of Week, Length: 94, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "modified_promotionEndOfWeek = cleaningData(promotions_EndOfWeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c86f6a2a-90f7-40b1-8f9f-723f5578455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_valueUplift = cleaningData(value_uplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3253f1fa-b573-493b-b3c6-d825cee538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning13New(data, size_fix=1):\n",
    "    \"\"\"\n",
    "    Clean and process data for specific brands and regions.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing raw data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned and processed data.\n",
    "    \"\"\"\n",
    "    data_cleaned = {}\n",
    "    \n",
    "    # Define maximum total size for each combination of product type and region\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        # Skip processing if the region is 'NATIONAL' or 'National'\n",
    "        if 'NATIONAL' in areas or 'National' in key:\n",
    "            continue\n",
    "        \n",
    "        new_data = []\n",
    "        \n",
    "        # Skip first 12 rows as they are headers and metadata\n",
    "        # df = df.iloc[12:]\n",
    "        df=DetectHeader(data[key])\n",
    "\n",
    "        df['Top Brands'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "        # Filter out rows where 'Top Brands' is 'Grand Total' or 'Other'\n",
    "        df = df[(df['Top Brands'] != 'Grand Total') & (df['Top Brands'] != 'Other')]\n",
    "        # Remove 'GR' suffix from 'Total Size' and convert it to integer\n",
    "        df['Total Size'] = df['Total Size'].str.extract('(\\d+)', expand=False)\n",
    "        df.fillna('0',inplace=True)\n",
    "        df['Total Size'] = df['Total Size'].astype(int)\n",
    "        # Sort data by 'Value Share' in descending order\n",
    "        df = df.sort_values(by='Value Share', ascending=False).reset_index(drop=True)\n",
    "        for i, brand in enumerate(df['Top Brands'].unique()):\n",
    "            # Determine the product key based on the first two elements of the key\n",
    "            if key.split(' |')[0] in categories:\n",
    "                product_key = key.split('| ')[1] + ' | ' + key.split(' |')[0]\n",
    "            else:\n",
    "                product_key = key.split('|')[0] + '|' + key.split('|')[1]\n",
    "            max_size = max_total_size.get(product_key, None)\n",
    "            # Filter rows for the current brand and check if total size is within the maximum allowed size\n",
    "            if max_size is not None:\n",
    "                \n",
    "                brand_df = df[(df['Top Brands'] == brand) & (df['Total Size'] <= max_size*size_fix)]\n",
    "            else:\n",
    "                brand_df = pd.DataFrame()\n",
    "                \n",
    "            # Calculate recruitment ratio if the brand has data and total size is greater than zero\n",
    "            #brand_total = df[(df['Top Brands'] == brand + ' Total')]['Promo Value'].values\n",
    "            brand_total = df[(df['Top Brands'].str.strip() == (brand + ' Total').strip())]['Promo Value'].values\n",
    "            \n",
    "            \n",
    "            if not brand_df.empty and brand_total.size > 0 and brand_total[0] > 0:\n",
    "                \n",
    "                brand_sum = brand_df['Promo Value'].sum() / brand_total[0]\n",
    "     \n",
    "                new_data.append({'Top Brands': brand, 'Recruitment': brand_sum, 'Consumption': 1 - brand_sum, 'Value Share': df['Value Share'][i], 'SUM':brand_df['Promo Value'].sum()})\n",
    "        \n",
    "        # Create a new DataFrame with cleaned data\n",
    "        new = pd.DataFrame(new_data)\n",
    "        new.fillna(0, inplace=True)\n",
    "        # Add cleaned data to the dictionary if it contains non-zero rows\n",
    "        if new.shape[0] != 0:\n",
    "            data_cleaned[key] = new\n",
    "        \n",
    "    return data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7da7fc3d-461c-4a38-8ae2-8d19805e5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newModifiedBrands = cleaning13New(promotions_brands_P12M,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "586375bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(VSODClient_Cleaned, VSODCleaned, col):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of DataFrames based on a common column.\n",
    "\n",
    "    Parameters:\n",
    "    - VSODClient_Cleaned (dict): Dictionary containing cleaned VSOD client DataFrames.\n",
    "    - VSODCleaned (dict): Dictionary containing cleaned VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing merged DataFrames.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for key in VSODClient_Cleaned:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        #merged_df = pd.merge(VSODCleaned[key], VSODClient_Cleaned[key], on=col, how='left')\n",
    "        merged_df = pd.merge(VSODClient_Cleaned[key],VSODCleaned[key], on=col, how='left')\n",
    "        #merged_df = merged_df.dropna(subset=['Grand Total'])\n",
    "        merged_df['Grand Total'] = merged_df['Grand Total'].fillna(0)\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            merged_dict[key] = merged_df     \n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d18ef6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors) >0:\n",
    "    a = VSOD_Clean(Sector_VSOD)\n",
    "    if len(Sector_client_VSOD) >0:\n",
    "        b = cleaningData(Sector_client_VSOD)\n",
    "        sect_vsod_merged = merging(b,a, col=[direct_parent[\"Sector\"],'Sector'])\n",
    "    else:\n",
    "        sect_vsod_merged = a\n",
    "    c = cleaningData(Sector_manuf_VSOD)\n",
    "    for key in sect_vsod_merged:\n",
    "        merged_df = pd.merge(sect_vsod_merged[key], c[key], on=[direct_parent[\"Sector\"],'Sector'], how='left')\n",
    "        if merged_df.shape[0]>0:\n",
    "            sect_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(segments) >0:\n",
    "    a = VSOD_Clean(Segment_VSOD)\n",
    "    if len(Segment_client_VSOD) > 0:\n",
    "        b = cleaningData(Segment_client_VSOD)\n",
    "        seg_vsod_merged = merging(a,b, col=[direct_parent[\"Segment\"],'Segment'])\n",
    "    else:\n",
    "        seg_vsod_merged = a\n",
    "    \n",
    "    c = cleaningData(Segment_manuf_VSOD)\n",
    "    for key in seg_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(seg_vsod_merged[key], c[key], on=[direct_parent[\"Segment\"],'Segment'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            seg_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(subsegments) >0:\n",
    "    a = VSOD_Clean(SubSegment_VSOD)\n",
    "    if len(SubSegment_client_VSOD) > 0 :\n",
    "        b = cleaningData(SubSegment_client_VSOD)\n",
    "        subseg_vsod_merged = merging(a,b, col=[direct_parent[\"SubSegment\"],'SubSegment'])\n",
    "    else:\n",
    "        subseg_vsod_merged = a\n",
    "    c = cleaningData(SubSegment_manuf_VSOD)\n",
    "    for key in subseg_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(subseg_vsod_merged[key], c[key], on=[direct_parent[\"SubSegment\"],'SubSegment'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            subseg_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(subcategories) >0:\n",
    "    a = VSOD_Clean(SubCategory_VSOD)\n",
    "    if len(SubCategory_client_VSOD) > 0 :\n",
    "        b = cleaningData(SubCategory_client_VSOD)\n",
    "        subcat_vsod_merged = merging(a,b, col=[direct_parent[\"SubCategory\"],'SubCategory'])\n",
    "    else:\n",
    "        subcat_vsod_merged = a\n",
    "    c = cleaningData(SubCategory_manuf_VSOD)\n",
    "    for key in subcat_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(subcat_vsod_merged[key], c[key], on=[direct_parent[\"SubCategory\"],'SubCategory'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            subcat_vsod_merged[key] = merged_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5f75fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitkeys(dic, lis, parent,clientlist):\n",
    "    \"\"\"\n",
    "    Splits the keys of a dictionary into new keys based on unique values in a specified column.\n",
    "    Parameters:\n",
    "    dic (dict): The input dictionary with DataFrames as values.\n",
    "    lis (list): A list of sector names to filter by (if needed).\n",
    "    parent (str): The column name used for splitting (e.g., 'Sector').\n",
    "    \n",
    "    Returns:\n",
    "    dict: A new dictionary with updated keys and filtered DataFrames.\n",
    "    \"\"\"\n",
    "    splitvsod = {}\n",
    "    for key in dic.keys():\n",
    "        for key in dic.keys():\n",
    "            # Split the key into parts and check if all parts are in the valid list\n",
    "    \n",
    "                # Get a copy of the current DataFrame\n",
    "            s = dic[key].copy()        \n",
    "            for value in s[parent].unique():\n",
    "                if isinstance(value, str) and not value.endswith(\"Total\"):\n",
    "                    new_key = f\"{key} | {value}\"                \n",
    "                    filtered_df = s[s[parent].isin([value, f\"{value} Total\"])]   \n",
    "                    for cli in clientlist:\n",
    "                        needed_col = [filtered_df.columns[0], filtered_df.columns[1], \"VSOD\", cli]\n",
    "                        missing_cols = [col for col in needed_col if col not in filtered_df.columns]\n",
    "                        \n",
    "                        if not missing_cols:\n",
    "                            filtered_dfnew = filtered_df[needed_col]\n",
    "                            splitvsod[new_key + \" | \" + cli] = filtered_dfnew\n",
    "                        else:\n",
    "                            print(f\"Skipping {cli}: missing columns {missing_cols}\")\n",
    "\n",
    "    keys_to_remove = [\n",
    "        k for k in splitvsod.keys() \n",
    "        if k.split(\" | \")[-2] not in lis\n",
    "    ]\n",
    "    for k in keys_to_remove:\n",
    "        del splitvsod[k]\n",
    "    return splitvsod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e97a11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=client_brands+client_manuf\n",
    "if len(sectors)!=0:\n",
    "    sect_vsod_merged=splitkeys(sect_vsod_merged,categories,parent=direct_parent['Sector'],clientlist=client)\n",
    "if len(segments)!=0:\n",
    "    seg_vsod_merged=splitkeys(seg_vsod_merged,sectors,parent=direct_parent['Segment'],clientlist=client)\n",
    "if len(subsegments)!=0:\n",
    "    subseg_vsod_merged=splitkeys(subseg_vsod_merged,segments,parent=direct_parent['SubSegment'],clientlist=client)\n",
    "if len(subcategories)!=0:\n",
    "    subcat_vsod_merged=splitkeys(subcat_vsod_merged,segments,parent=direct_parent['SubCategory'],clientlist=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b271ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors):\n",
    "    sect_vsod_count =0\n",
    "    for key,df in sect_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                sect_vsod_count +=1\n",
    "    sect_vsod_count = sect_vsod_count *len(categories)\n",
    " \n",
    "if len(segments):\n",
    "    seg_vsod_count =0\n",
    "    for key,df in seg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                seg_vsod_count +=1\n",
    "    #seg_vsod_count = seg_vsod_count * len(sectors) \n",
    "    seg_vsod_count = seg_vsod_count           \n",
    " \n",
    "if len(subsegments) >0:\n",
    "    subseg_vsod_count =0\n",
    "    for key,df in subseg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subseg_vsod_count +=1\n",
    "    #subseg_vsod_count =subseg_vsod_count *len(segments)\n",
    "    subseg_vsod_count = subseg_vsod_count\n",
    " \n",
    "if len(subcategories) >0:\n",
    "    subcat_vsod_count =0\n",
    "    for key,df in subcat_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subcat_vsod_count +=1\n",
    "    #subcat_vsod_count = subcat_vsod_count * len(subsegments)\n",
    "    subcat_vsod_count = subcat_vsod_count\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6dd04ba3-5570-4ad4-8d61-252872480a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandSortedTotalFinal={}\n",
    "promotionsBrandSortedTotal=dfSort(modified_promotionBrandsP12M, client_brands, \"Top Brands\", num=8,salesCol='Promo Value')\n",
    "for key,df in promotionsBrandSortedTotal.items():\n",
    "     df_client = selectClientBrands(promotionsBrandSortedTotal[key],'Top Brands', 'Promo Value')\n",
    "     number_of_brands_needed = max(6 - len(df_client),0)\n",
    "     df = df[~df['Top Brands'].isin(client_brands)]\n",
    "     df = df.sort_values(by='Promo Value', ascending=False).head(number_of_brands_needed)\n",
    "     df = pd.concat([df, df_client], ignore_index=True)\n",
    "     df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "     df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "     df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    "     df = df[df['Value Share'] > 0.01]\n",
    "        \n",
    "     df['VSOD Evaluation vs YA'] = df['VSOD Evaluation vs YA'].astype(float)\n",
    "     df['Promo Value Uplift vs YA'] = df['Promo Value Uplift vs YA'].astype(float)\n",
    "     \n",
    "     if df.shape[0] >0:\n",
    "          promotionsBrandSortedTotalFinal[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e66555e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandNOTSortedTotalFinal={}\n",
    "promotionsBrandNOTSortedTotalFinal=dfSort(modified_promotionBrandsP12M, client_brands, \"Top Brands\", num=8,salesCol='Promo Value')\n",
    "for key,df in modified_promotionBrandsP12M.items():\n",
    "     df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "     df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "     df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    "     df = df[df['Value Share'] > 0.01]\n",
    "     df['VSOD Evaluation vs YA'] = df['VSOD Evaluation vs YA'].astype(float)\n",
    "     df['Promo Value Uplift vs YA'] = df['Promo Value Uplift vs YA'].astype(float)\n",
    "     if df.shape[0] >0:\n",
    "          promotionsBrandNOTSortedTotalFinal[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "75e0e42f-2500-4d43-9ed2-2b5ac567a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands_og = selectedBrands\n",
    "selectedBrands= selectedBrands + [\"Grand Total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ab470824-4116-42ea-b87c-150650a1d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsSelected={key:modified_promotionBrandsP12M_total[key][modified_promotionBrandsP12M_total[key]['Top Brands'].isin(selectedBrands)].sort_values(by='Promo Value',ascending=False) for key in modified_promotionBrandsP12M_total.keys()   if all(cat != key.split(' | ')[0] for cat in categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5deb8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in promotionsBrandsSelected:\n",
    "    # Identify the Grand Total row (adjust this if necessary to match your data)\n",
    "    grand_total_row = promotionsBrandsSelected[key].loc[promotionsBrandsSelected[key]['Top Brands'] == 'Grand Total']\n",
    "    # Remove the Grand Total row from the dataframe\n",
    "    sorted_df = promotionsBrandsSelected[key].loc[promotionsBrandsSelected[key]['Top Brands'] != 'Grand Total']\n",
    "    # Concatenate the Grand Total row to the top of the dataframe\n",
    "    promotionsBrandsSelected[key] = pd.concat([grand_total_row, sorted_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ea88f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands = selectedBrands_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6d4a6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not including client brands\n",
    "promotionsNotBrandsSelected = {\n",
    "    key: modified_promotionBrandsP12M_total[key][\n",
    "        ~modified_promotionBrandsP12M_total[key]['Top Brands'].isin(selectedBrands)\n",
    "    ].sort_values(by='Value Share', ascending=False)\n",
    "    for key in modified_promotionBrandsP12M_total.keys()\n",
    "    if all(cat != key.split(' | ')[0] for cat in categories)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eabcdd35-a8e6-4667-8efe-17c5dea262ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAttribute(dic, marketList):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames and a list of markets, and concatenates\n",
    "    the DataFrames by adding a 'SOURCE' column to each DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are strings in the format 'market | source', and\n",
    "                values are DataFrames containing market data.\n",
    "    marketList (list): A list of market names (strings).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are market names and values are concatenated DataFrames\n",
    "          with an added 'SOURCE' column.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict to store the resulting DataFrames\n",
    "    marketDic = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the list of markets\n",
    "    for market in marketList:\n",
    "        # Iterate through the items in the dictionary\n",
    "        for key, value in dic.items():\n",
    "            # Check if the market name matches the key's market part\n",
    "            if market == key.split(' | ')[0]:\n",
    "                # Extract the source part from the key and assign it to the 'SOURCE' column\n",
    "                value['SOURCE'] = list(set(key.split(' | ')) - set([market]))[0]\n",
    "                value = value[value['Value Share'] >0.01]\n",
    "                value = value[~value['Top Brands'].str.contains('Other')].reset_index(drop=True)\n",
    "            \n",
    "                # Only include rows where 'SOURCE' is not 'National'\n",
    "                if (value['SOURCE'] != 'National').all():\n",
    "                    marketDic[market].append(value)\n",
    "\n",
    "        # Concatenate all DataFrames in the list for each market\n",
    "        if len(marketDic[market]) != 0:\n",
    "            marketDic[market] = pd.concat(marketDic[market])\n",
    "    \n",
    "    return marketDic\n",
    "\n",
    "def fillingMissingBrands(dic):\n",
    "    \"\"\"\n",
    "    This function fills in missing brands for each market and source combination in the\n",
    "    provided dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are market names and values are DataFrames\n",
    "                containing market data with 'Top Brands' and 'SOURCE' columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: The input dictionary with missing brands filled in each DataFrame.\n",
    "    \"\"\"\n",
    "    # Iterate through the dictionary items\n",
    "    for key, value in dic.items():\n",
    "        # Get the unique list of top brands in the DataFrame\n",
    "        brandList = value['Top Brands'].unique().tolist()\n",
    "        # Iterate through the unique sources in the DataFrame\n",
    "        for source in value['SOURCE'].unique():\n",
    "            # Check if the number of unique brands for the source is less than the total unique brands\n",
    "            if value[value['SOURCE'] == source]['Top Brands'].nunique() != len(brandList):\n",
    "                # Find the missing brands for the source\n",
    "                missingBrand = list(set(brandList) - set(value[value['SOURCE'] == source]['Top Brands'].unique()))\n",
    "                # Create a DataFrame for the missing brands with the current source\n",
    "                missingBrand = pd.DataFrame({'Top Brands': missingBrand, 'SOURCE': source}).explode('Top Brands')\n",
    "                # Concatenate the missing brands DataFrame with the original DataFrame\n",
    "                value = pd.concat([value, missingBrand]).replace(np.nan, 0).reset_index(drop=True)\n",
    "        # Update the dictionary with the filled DataFrame\n",
    "        dic[key] = value\n",
    "    \n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5825f852-d5a8-4717-863c-60bce7831474",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsWithMarket=concatAttribute(promotionsBrandsSelected,marketList)\n",
    "promotionsBrandsWithMarket = fillingMissingBrands(promotionsBrandsWithMarket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "993b4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsNotBrandsWithMarket=concatAttribute(promotionsNotBrandsSelected,marketList)\n",
    "promotionsNotBrandsWithMarket = fillingMissingBrands(promotionsNotBrandsWithMarket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "762fb5e2-0b4d-442f-9531-040980af3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_market(data, Scope):\n",
    "    final = {}\n",
    "    for k,df in data.items():\n",
    "        for key, value in Scope.items():\n",
    "            df_market = df[df['SOURCE'].isin(value)]\n",
    "            df_market = df_market.reset_index(drop=True)\n",
    "            if df_market.shape[0] >0:\n",
    "                final[k + ' | ' + value[0]] = df_market\n",
    "    return final        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b2a003a6-6da6-495c-a2d9-a9b0af5a7a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newpromotionsBrandsWithMarket = split_market(promotionsBrandsWithMarket,Scope)\n",
    "newpromotionsNotBrandsWithMarket = split_market(promotionsNotBrandsWithMarket,Scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6e045f79-110e-4cb8-b237-b514151b9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def concatAttributeNew(sorted):\n",
    "    \"\"\"\n",
    "    Concatenate DataFrames from a sorted dictionary based on exact matches of categories, sectors, segments,\n",
    "    subsegments, and subcategories. Adds a 'SOURCE' column to each DataFrame indicating its market.\n",
    "\n",
    "    Parameters:\n",
    "    sorted (dict): Dictionary with keys like 'category | sector | segment | brand' and values as DataFrames.\n",
    "    categories, sectors, segments, subsegments, subcategories (list): Lists of strings to match exactly.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with matched group names as keys and concatenated DataFrames as values.\n",
    "    \"\"\"\n",
    "    # Combine all lists and preserve order without duplicates\n",
    "    lis = list(dict.fromkeys(categories + sectors + segments + subsegments + subcategories))\n",
    "\n",
    "    marketDic = defaultdict(list)\n",
    "    concatenatedDic = {}\n",
    "\n",
    "    for i in lis:\n",
    "        for key, df in sorted.items():\n",
    "            parts = key.split(' | ')\n",
    "            if i in parts:\n",
    "                # Determine market label\n",
    "                if i in categories:\n",
    "                    market_label = parts[1]  # sector\n",
    "                else:\n",
    "                    market_label = parts[0]  # category\n",
    "\n",
    "                df = df.copy()  # Avoid modifying original\n",
    "                df['SOURCE'] = market_label\n",
    "                marketDic[i].append(df)\n",
    "\n",
    "        if marketDic[i]:\n",
    "            concatenatedDic[i] = pd.concat(marketDic[i], ignore_index=True)\n",
    "\n",
    "    return concatenatedDic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "28e29be2-2f65-4603-8858-1087e008b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = concatAttributeNew(modified_promotionBrandsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d3ee231f-9b67-4eef-ad0a-ae6e7d75eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        df = modified_promotionProductsP12M[key]\n",
    "        df = df[df[f'{prodORitem}'] != '']\n",
    "        df=df.sort_values(by=['Promo Share'], ascending=False)\n",
    "        # Filter and sort the DataFrame\n",
    "        df['cumulative promo share'] = df['Promo Share'].cumsum()\n",
    "        df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "        df = df[df['VSOD'] >= 0.05]\n",
    "        df = df[df['cumulative promo share'] <= 0.8]\n",
    "        df = df.sort_values(by='Incr Value', ascending=False).reset_index(drop=True)\n",
    "        df = df.head(50)\n",
    "        df['index'] = str(df.index + 1)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] >0:\n",
    "            cleaned_data[key] = df\n",
    "        #else:\n",
    "            #print(key)\n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8b8c6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_modified_promotionProductsP12M = filter_data(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3df8ceaa-7083-4015-8ccb-6202d9ac4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_Top(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        combined_df = pd.DataFrame() \n",
    "        for client in client_brands:\n",
    "            \n",
    "            df = modified_promotionProductsP12M[key]\n",
    "            # Filter the DataFrame for the current client brand\n",
    "            df = df[df[f'{prodORitem}'] != '']\n",
    "            df = df[df['Top Brands'] == client]\n",
    "            df = df.sort_values(by='Top Brands')\n",
    "            df['Promo Share'] = pd.to_numeric(df['Promo Share'], errors='coerce')\n",
    "            df['cumulative promo share'] = df.groupby('Top Brands')['Promo Share'].cumsum()\n",
    "            df = df[df['cumulative promo share'] <= 0.8]\n",
    "\n",
    "            df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "            df = df[df['VSOD'] >= 0.05]\n",
    "            if df.shape[0] >0:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "                combined_df = combined_df.sort_values(by='Incr Value', ascending=False).head(20).reset_index(drop=True)\n",
    "        if combined_df.shape[0] > 0:\n",
    "            cleaned_data[key] = combined_df.reset_index(drop=True)  # Store the combined DataFrame for the current key\n",
    "                \n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ec81127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_promotionProductsP12M_updated = {}\n",
    "# for key, df in modified_promotionProductsP12M.items():\n",
    "#     df = df.copy()\n",
    "#     df = df[df['Product'] != '']\n",
    "#     df = df[df['Promo Sales'] >= 10000]\n",
    "#     df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "#     if not df.empty:\n",
    "#         modified_promotionProductsP12M_updated[key] = df\n",
    "# modified_promotionProductsP12M = modified_promotionProductsP12M_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0241c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_Bot(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        combined_df = pd.DataFrame() \n",
    "        for client in client_brands:\n",
    "            df = modified_promotionProductsP12M[key]\n",
    "            # Filter the DataFrame for the current client brand\n",
    "            df = df[df[f'{prodORitem}'] != '']\n",
    "            df = df[df['Top Brands'] == client]\n",
    "            df = df.sort_values(by='Top Brands')\n",
    "            df['Promo Share'] = pd.to_numeric(df['Promo Share'], errors='coerce')\n",
    "            df['cumulative promo share'] = df.groupby('Top Brands')['Promo Share'].cumsum()\n",
    "            df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "            df = df[df['VSOD'] >= 0.05]\n",
    "            if df.shape[0] >0:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "                combined_df = combined_df.sort_values(by='Incr Value', ascending=False).tail(20).reset_index(drop=True)\n",
    "                combined_df = combined_df.sort_values(by ='Incr Value', ascending= True).reset_index(drop=True)\n",
    "        if combined_df.shape[0] > 0:\n",
    "            cleaned_data[key] = combined_df.reset_index(drop=True)  # Store the combined DataFrame for the current key\n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d5316486-6b68-430c-963f-11df7ef98c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20clientonly = filter_data_Top(modified_promotionProductsP12M)\n",
    "\n",
    "bottom20clientonly = filter_data_Bot(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4f3e9198-c36a-4544-8a00-92a1e2ab89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotionsEndOfWeekCleaning(promotions_EndOfWeek, notInScope, col='Top Brands'):\n",
    "    \"\"\"\n",
    "    Clean promotions end of week data.\n",
    "\n",
    "    Parameters:\n",
    "    promotions_EndOfWeek (dict): Dictionary containing promotions end of week data.\n",
    "    notInScope (list): List of items not in scope to be excluded.\n",
    "    col (str): Column name to check for filtering. Default is 'Top Brands'.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing cleaned promotions end of week data.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store cleaned promotions end of week data\n",
    "    promotionsEndOfWeek = {}\n",
    "    # Iterate over items in promotions_EndOfWeek dictionary\n",
    "    for key, value in promotions_EndOfWeek.items():\n",
    "        # Make a copy of the dataframe to avoid modifying the original data\n",
    "        df = value.copy()\n",
    "        # Check if the dataframe is not empty\n",
    "        if df.shape[0] != 0:\n",
    "            # Modify the key to match the desired format if applicable\n",
    "            modified_key = key\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "            # Check if the key contains any item from the notInScope list\n",
    "            # If not, add the dataframe to the cleaned dictionary after filtering out 'Grand Total' rows\n",
    "            flag = False if any(element in modified_key for element in notInScope) else True\n",
    "            if flag:\n",
    "                promotionsEndOfWeek[modified_key] = df[df[col] != 'Grand Total'].reset_index(drop=True).replace(np.nan, 0)\n",
    "        else:\n",
    "            print(key, ' Is empty')\n",
    "    \n",
    "    return promotionsEndOfWeek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "58cb2bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-04-01'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c3bdd031-7086-48bc-b9b6-d9e09be82bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "82   2024-12-15\n",
      "83   2024-12-22\n",
      "84   2025-01-12\n",
      "85   2025-01-19\n",
      "86   2025-01-26\n",
      "Name: End of Week, Length: 87, dtype: datetime64[ns]\n",
      "0   2023-11-12\n",
      "1   2023-11-26\n",
      "2   2024-02-04\n",
      "3   2024-02-11\n",
      "4   2024-03-10\n",
      "5   2024-06-23\n",
      "6   2024-08-11\n",
      "7   2024-09-29\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "90   2024-12-01\n",
      "91   2025-01-05\n",
      "92   2025-01-12\n",
      "93   2025-01-19\n",
      "94   2025-01-26\n",
      "Name: End of Week, Length: 95, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "87   2024-10-20\n",
      "88   2025-01-05\n",
      "89   2025-01-12\n",
      "90   2025-01-19\n",
      "91   2025-01-26\n",
      "Name: End of Week, Length: 92, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "92   2024-12-29\n",
      "93   2025-01-05\n",
      "94   2025-01-12\n",
      "95   2025-01-19\n",
      "96   2025-01-26\n",
      "Name: End of Week, Length: 97, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "82   2024-12-15\n",
      "83   2024-12-22\n",
      "84   2025-01-12\n",
      "85   2025-01-19\n",
      "86   2025-01-26\n",
      "Name: End of Week, Length: 87, dtype: datetime64[ns]\n",
      "0   2023-11-12\n",
      "1   2023-11-26\n",
      "2   2024-02-04\n",
      "3   2024-02-11\n",
      "4   2024-03-10\n",
      "5   2024-06-23\n",
      "6   2024-08-11\n",
      "7   2024-09-29\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "90   2024-12-01\n",
      "91   2025-01-05\n",
      "92   2025-01-12\n",
      "93   2025-01-19\n",
      "94   2025-01-26\n",
      "Name: End of Week, Length: 95, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "87   2024-10-20\n",
      "88   2025-01-05\n",
      "89   2025-01-12\n",
      "90   2025-01-19\n",
      "91   2025-01-26\n",
      "Name: End of Week, Length: 92, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "92   2024-12-29\n",
      "93   2025-01-05\n",
      "94   2025-01-12\n",
      "95   2025-01-19\n",
      "96   2025-01-26\n",
      "Name: End of Week, Length: 97, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "67   2024-11-10\n",
      "68   2024-11-17\n",
      "69   2025-01-12\n",
      "70   2025-01-19\n",
      "71   2025-01-26\n",
      "Name: End of Week, Length: 72, dtype: datetime64[ns]\n",
      "0    2023-01-22\n",
      "1    2023-01-29\n",
      "2    2023-02-05\n",
      "3    2023-02-12\n",
      "4    2023-02-19\n",
      "        ...    \n",
      "74   2024-12-15\n",
      "75   2024-12-22\n",
      "76   2025-01-12\n",
      "77   2025-01-19\n",
      "78   2025-01-26\n",
      "Name: End of Week, Length: 79, dtype: datetime64[ns]\n",
      "0   2023-11-12\n",
      "1   2023-11-26\n",
      "2   2024-02-04\n",
      "3   2024-02-11\n",
      "4   2024-03-10\n",
      "5   2024-06-23\n",
      "6   2024-08-11\n",
      "7   2024-09-29\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "89   2024-12-29\n",
      "90   2025-01-05\n",
      "91   2025-01-12\n",
      "92   2025-01-19\n",
      "93   2025-01-26\n",
      "Name: End of Week, Length: 94, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "101   2024-12-29\n",
      "102   2025-01-05\n",
      "103   2025-01-12\n",
      "104   2025-01-19\n",
      "105   2025-01-26\n",
      "Name: End of Week, Length: 106, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "77   2024-11-17\n",
      "78   2025-01-05\n",
      "79   2025-01-12\n",
      "80   2025-01-19\n",
      "81   2025-01-26\n",
      "Name: End of Week, Length: 82, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "85   2024-11-17\n",
      "86   2024-12-01\n",
      "87   2025-01-12\n",
      "88   2025-01-19\n",
      "89   2025-01-26\n",
      "Name: End of Week, Length: 90, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "77   2024-12-29\n",
      "78   2025-01-05\n",
      "79   2025-01-12\n",
      "80   2025-01-19\n",
      "81   2025-01-26\n",
      "Name: End of Week, Length: 82, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "91   2024-12-29\n",
      "92   2025-01-05\n",
      "93   2025-01-12\n",
      "94   2025-01-19\n",
      "95   2025-01-26\n",
      "Name: End of Week, Length: 96, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "75   2024-10-13\n",
      "76   2025-01-05\n",
      "77   2025-01-12\n",
      "78   2025-01-19\n",
      "79   2025-01-26\n",
      "Name: End of Week, Length: 80, dtype: datetime64[ns]\n",
      "0    2023-01-15\n",
      "1    2023-01-22\n",
      "2    2023-01-29\n",
      "3    2023-02-05\n",
      "4    2023-02-12\n",
      "        ...    \n",
      "78   2024-09-29\n",
      "79   2024-10-20\n",
      "80   2025-01-12\n",
      "81   2025-01-19\n",
      "82   2025-01-26\n",
      "Name: End of Week, Length: 83, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "72   2024-12-29\n",
      "73   2025-01-05\n",
      "74   2025-01-12\n",
      "75   2025-01-19\n",
      "76   2025-01-26\n",
      "Name: End of Week, Length: 77, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "92   2024-12-29\n",
      "93   2025-01-05\n",
      "94   2025-01-12\n",
      "95   2025-01-19\n",
      "96   2025-01-26\n",
      "Name: End of Week, Length: 97, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "73   2024-12-29\n",
      "74   2025-01-05\n",
      "75   2025-01-12\n",
      "76   2025-01-19\n",
      "77   2025-01-26\n",
      "Name: End of Week, Length: 78, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "89   2024-12-29\n",
      "90   2025-01-05\n",
      "91   2025-01-12\n",
      "92   2025-01-19\n",
      "93   2025-01-26\n",
      "Name: End of Week, Length: 94, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "mod=cleaningData(promotions_EndOfWeek)\n",
    "promotionsEndOfWeekCleaned=promotionsEndOfWeekCleaning(mod,notInScope,col='End of Week')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4e7408ac-109a-48e1-83ee-1f3acda454ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarket = list(set([key.split(' | ')[0]+' | '+key.split(' | ')[1] for key in promotionsEndOfWeekCleaned]))\n",
    "brandMarketCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat in key.split(' | ')[-1] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in subcategories )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "33070cde-4b18-410c-bf87-00d132014a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeDates(dfList, promotionsEndOfWeekCleaned):\n",
    "    \"\"\"\n",
    "    Complete dates for each dataframe in dfList based on promotionsEndOfWeekCleaned.\n",
    "\n",
    "    Parameters:\n",
    "    dfList (list): List of dataframe keys.\n",
    "    promotionsEndOfWeekCleaned (dict): Dictionary containing cleaned promotions end of week data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing EndOfWeekcompletDate (dictionary), dfGroup (list), and dic (dictionary).\n",
    "    \"\"\"\n",
    "    # Create a list of unique brand-category combinations\n",
    "    brandCatList = sorted(set(key.split(' | ')[0] + ' | ' + key.split(' | ')[2] for key in dfList))\n",
    "    \n",
    "    # Initialize dictionaries and lists\n",
    "    EndOfWeekcompletDate = {}\n",
    "    dfGroup = []\n",
    "    dic = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each brand-category combination\n",
    "    for key in brandCatList:\n",
    "        for name in dfList:\n",
    "            if (key.split(' | ')[0] == name.split(' | ')[0]) and (key.split(' | ')[1] == name.split(' | ')[2]):\n",
    "                dic[key] += 1\n",
    "                \n",
    "    # Iterate over unique brand-category combinations\n",
    "    for name in dic.keys():\n",
    "        # Get dataframe keys associated with the current brand-category combination\n",
    "        dfName = [key for key in dfList if name == (key.split(' | ')[0] + ' | ' + key.split(' | ')[2])]\n",
    "        \n",
    "        # Extract unique dates from all associated dataframes\n",
    "        uniqueDates = pd.concat([promotionsEndOfWeekCleaned[key] for key in dfName])[['End of Week']].drop_duplicates()\n",
    "        if uniqueDates.shape[0] > 0:\n",
    "            # Initialize dictionary for complete dates for each dataframe\n",
    "            dfCompleteDates = {}\n",
    "            \n",
    "            # Add dataframe keys to the group list\n",
    "            dfGroup.append(dfName)\n",
    "            \n",
    "            # Populate EndOfWeekcompletDate dictionary with dataframes merged on unique dates\n",
    "            for key in dfName:\n",
    "                EndOfWeekcompletDate[key] = pd.merge(uniqueDates, promotionsEndOfWeekCleaned[key], how='left').replace(np.nan, 0).sort_values(by='End of Week').reset_index(drop = True)\n",
    "    return EndOfWeekcompletDate, dfGroup, dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "26bde262-eab5-4afc-ab44-9c3658cb974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory,catGroup,catDuplication=completeDates(brandMarketCategory,promotionsEndOfWeekCleaned)\n",
    "if len(sectors) != 0:\n",
    "    dfSector,secGroup,secDuplication=completeDates(brandMarketSector,promotionsEndOfWeekCleaned)\n",
    "if len(segments) != 0:\n",
    "    dfSegment,segGroup,segDuplication=completeDates(brandMarketSegment,promotionsEndOfWeekCleaned)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment,subsegGroup,subsegDuplication=completeDates(brandMarketSubSegment,promotionsEndOfWeekCleaned)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory,subcatGroup,subcatDuplication=completeDates(brandMarketSubCategory,promotionsEndOfWeekCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4108f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors):\n",
    "    sect_vsod_count =0\n",
    "    for key,df in sect_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                sect_vsod_count +=1\n",
    "    sect_vsod_count = sect_vsod_count *len(categories)\n",
    " \n",
    "if len(segments):\n",
    "    seg_vsod_count =0\n",
    "    for key,df in seg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                seg_vsod_count +=1\n",
    "    #seg_vsod_count = seg_vsod_count * len(sectors) \n",
    "    seg_vsod_count = seg_vsod_count           \n",
    " \n",
    "if len(subsegments) >0:\n",
    "    subseg_vsod_count =0\n",
    "    for key,df in subseg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subseg_vsod_count +=1\n",
    "    #subseg_vsod_count =subseg_vsod_count *len(segments)\n",
    "    subseg_vsod_count = subseg_vsod_count\n",
    " \n",
    "if len(subcategories) >0:\n",
    "    subcat_vsod_count =0\n",
    "    for key,df in subcat_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subcat_vsod_count +=1\n",
    "    #subcat_vsod_count = subcat_vsod_count * len(subsegments)\n",
    "    subcat_vsod_count = subcat_vsod_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "43c6f94a-d00f-46ad-9011-678073e6bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoRet ={}\n",
    "if len(categories)!=0:\n",
    "    first_key, first_value = next(iter(catDuplication.items()))\n",
    "    PromoRet.update({first_key: first_value})\n",
    "if len(sectors)!=0:\n",
    "    sec_key, sec_value = next(iter(secDuplication.items()))\n",
    "    PromoRet.update({sec_key:sec_value})\n",
    "if len(segments)!=0:\n",
    "    third_key, third_value = next(iter(segDuplication.items()))\n",
    "    PromoRet.update({third_key: third_value})\n",
    "if len(subsegments)!=0:\n",
    "    fourth_key, fourth_value = next(iter(subsegDuplication.items()))\n",
    "    PromoRet.update({fourth_key:fourth_value})\n",
    "if len(subcategories)!=0:\n",
    "    fifth_key, fifth_value = next(iter(subcatDuplication.items()))\n",
    "    PromoRet.update({fifth_key:fifth_value })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c094ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromoSalesTypes_data = {}\n",
    "# for key, df in modified_promotionBrandsP12M.items():\n",
    "#     df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "#     df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    "#     df = df[df['Value Share'] > 0.01]\n",
    "#     df = df[df['Promo Value'] > 0]\n",
    "#     # Select client brands and additional brands needed to make 10 brands\n",
    "#     df_client = selectClientBrands(modified_promotionBrandsP12M[key],'Top Brands', 'Value Share')\n",
    "#     number_of_brands_needed = max(10 - len(df_client),0)\n",
    "#     df = df[~df['Top Brands'].isin(client_brands)]\n",
    "#     df = df.sort_values(by='Value Share', ascending=False).head(number_of_brands_needed)\n",
    "#     # Concatenate client brands and additional brands\n",
    "#     df = pd.concat([df_client, df], ignore_index=True)\n",
    "#     df = df[df['Promo Value'] > 0]\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     if df.shape[0]:\n",
    "#         modified_promotionBrandsP12M[key] =df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a728a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoSalesTypes_data = {}\n",
    "for key, df in brands_promo_type.items():\n",
    "    df = DetectHeader(df)\n",
    "    df['Top Brands'] = df['Top Brands'].ffill()\n",
    "    df[\"Promo Sales\"] = pd.to_numeric(df[\"Promo Sales\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"Value Share\"] = pd.to_numeric(df[\"Value Share\"], errors=\"coerce\").fillna(0)\n",
    "    df = df[df['Promo Type'].notna()]\n",
    "    brand_totals = df.groupby(\"Top Brands\")['Promo Sales'].sum()\n",
    "\n",
    "    # df[\"Base Brand\"] = df[\"Top Brands\"].str.replace(\" Total\", \"\", regex=False)\n",
    "    # brand_totals = df[df[\"Top Brands\"].str.endswith(\"Total\")].set_index(\"Base Brand\")[\"Promo Sales\"].to_dict()\n",
    "    df[\"Brand Total Sales\"] = df[\"Top Brands\"].map(brand_totals)\n",
    "    df[\"% Promo Sales\"] = df[\"Promo Sales\"] / df[\"Brand Total Sales\"]\n",
    "\n",
    "    df = df[~df['Top Brands'].str.contains('Others|Grand Total', case=False)]\n",
    "    df = df[df['Value Share'] > 0.01]\n",
    "    df = df[df['Promo Sales'] > 0]\n",
    "    # Select client brands and additional brands needed to make 10 brands\n",
    "    df_client = selectClientBrands(brands_promo_type[key],'Top Brands', 'Value Share')\n",
    "    comp_brand = df[~df['Top Brands'].isin(cb for cb in client_brands)].drop_duplicates(\"Top Brands\")\n",
    "    if not df_client.empty:\n",
    "        comp_brand = comp_brand.nlargest(10-df_client[\"Top Brands\"].nunique(), \"Value Share\")[\"Top Brands\"].to_list()\n",
    "        # Concatenate client brands and additional brands\n",
    "        df = df[df[\"Top Brands\"].isin(comp_brand + client_brands)]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.sort_values(\"Value Share\", ascending=False).reset_index(drop=True)\n",
    "        df = df[~df['Promo Type'].fillna('').str.contains('NONE/PL|Undefined|Nan', na=False)]\n",
    "        # print(comp_brand)\n",
    "        if df.shape[0]:\n",
    "            PromoSalesTypes_data[key] =df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1b23654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.getcwd() + '//Promotion base Oct 2024.pptx'\n",
    "# prs = Presentation(path)\n",
    "# slide = prs.slides[12]\n",
    "# brands = list(dataa['Top Brands'].unique())\n",
    "# promotype = dataa['Promo Type'].unique().tolist()\n",
    "# tables, charts = createTableAndChart(slide.shapes)\n",
    "# # Update table with category data\n",
    "# table = tables[0].table\n",
    "# num_rows_to_remove = len(table.rows) - len(brands)\n",
    "# #table_height = 3.88\n",
    "# table = removeRowFromTable(table, num_rows_to_remove, rowToExclude=0)\n",
    "# for row_number, row in enumerate(table.rows, start=0):\n",
    "#     for column_num, cell in enumerate(row.cells):\n",
    "#         if column_num == 0:\n",
    "#             cell.text = str(brands[row_number])\n",
    "#             set_cell_font(cell, 'Nexa Bold', 9)\n",
    "#             cell.text_frame.paragraphs[0].alignment = PP_ALIGN.LEFT\n",
    "# chart = charts[0].chart\n",
    "# chart_data = CategoryChartData()\n",
    "# chart_data.categories = brands\n",
    "# for promo_type in promotype:\n",
    "#     brand_values = {brand: 0 for brand in brands}\n",
    "    \n",
    "#     promo_data = dataa[dataa['Promo Type'] == promo_type]\n",
    "#     for _, row in promo_data.iterrows():\n",
    "#         brand_values[row['Top Brands']] = row['% Promo Sales']\n",
    "#     series_values = [value if value != 0 else None for value in [brand_values[brand] for brand in brands]]\n",
    "\n",
    "#     if any(value is not None for value in series_values):\n",
    "#         chart_data.add_series(promo_type, series_values)\n",
    "\n",
    "\n",
    "# chart.replace_data(chart_data)\n",
    "# chart.chart_style = 3\n",
    "# for i, series in enumerate(chart.series):\n",
    "#     fill = series.format.fill\n",
    "#     fill.solid()\n",
    "#     fill.fore_color.rgb = custom_colors[i]\n",
    "\n",
    "# outputPath=os.getcwd() + \"\\\\Promotion EdgeWell test.pptx\"\n",
    "# prs.save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "88c1ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lis = []\n",
    "cat_lis = []\n",
    "if categories:\n",
    "    for i in range(len(catGroup)):\n",
    "        cat_lis += genrateIndexList(catGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(cat_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup)):\n",
    "        sec_lis += genrateIndexList(secGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(sec_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "seg_lis=[]\n",
    "if segments:\n",
    "    for i in range(len(segGroup)):\n",
    "        seg_lis += genrateIndexList(segGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(seg_lis)\n",
    "\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "subseg_lis =[]\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup)):\n",
    "        subseg_lis +=  genrateIndexList(subsegGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(subseg_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "subcat_lis =[]\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup)):\n",
    "        subcat_lis +=  genrateIndexList(subcatGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(subcat_lis)\n",
    "else:\n",
    "    final_lis.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd4fa9",
   "metadata": {},
   "source": [
    "### New Slide 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "15c05226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = Sector_MonthYear['Boots']\n",
    "# d = DetectHeader(d).reset_index(drop=True)\n",
    "# d['Sector'] = d['Sector'].fillna(method = 'ffill')\n",
    "# d = d[~d['Sector'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "# d['year'] = pd.to_datetime(d['MonthYear'], format='%b-%y').dt.year\n",
    "# #yearly_avg_sales = d.groupby(['year', 'Sector'])['Value Sales'].mean().reset_index(drop=True) \n",
    "# yearly_avg_sales = d.groupby(['year', 'Sector'])['Value Sales'].transform('mean').reset_index(drop=True) \n",
    "\n",
    "# print(yearly_avg_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b0c9e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonthYear_clean(data,column):\n",
    "    month_year_data={}\n",
    "    for key,df in data.items():\n",
    "        df=DetectHeader(data[key]).reset_index(drop=True)\n",
    "        df[column] = df[column].fillna(method = 'ffill')\n",
    "        df = df[~((df[column].str.contains('Total', case=False)) & (df[column] != categories[0]))].reset_index(drop=True)\n",
    "\n",
    "        # df = df[~df[column].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "        df['year'] = pd.to_datetime(df['MonthYear'], format='%b-%y').dt.year\n",
    "        df['year'] = df['year'].astype(int)\n",
    "        yearly_avg_sales = df.groupby(['year', column])['Value Sales'].transform('mean').reset_index(drop=True) \n",
    "        yearly_avg_sales = yearly_avg_sales.replace(0, float('nan'))\n",
    "        # Calculate 'Sales index' and handle NaN values gracefully\n",
    "        df['Sales index'] = (df['Value Sales'] / yearly_avg_sales * 100).fillna(0).astype(int)\n",
    "        # df['Sales index'] = (df['Value Sales'] / yearly_avg_sales * 100).astype(int)\n",
    "        if df.shape[0]>0:\n",
    "            # if column =='Sector':\n",
    "            #     newkey =key\n",
    "            #     month_year_data[newkey] = df\n",
    "            # elif column == 'Segment':\n",
    "            #     newkey= key.split(' | ')[0] + ' | ' + segments[-1]\n",
    "            #     month_year_data[newkey] = df\n",
    "            # elif column == 'SubSegment':\n",
    "            #     newkey= key.split(' | ')[0] + ' | ' + subsegments[-1]\n",
    "            #     month_year_data[newkey] = df\n",
    "            # else:\n",
    "            #     newkey= key.split(' | ')[0] + ' | ' + subcategories[-1]\n",
    "            month_year_data[key] = df\n",
    "    return month_year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "800863e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sector_MonthYear = {\n",
    "    f\"{key} | {categories[0]}\": value for key, value in Sector_MonthYear.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "db1737be",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_month_year=MonthYear_clean(Category_MonthYear,column='Category')\n",
    "sector_month_year = MonthYear_clean(Sector_MonthYear,column='Sector')\n",
    "segment_month_year = MonthYear_clean(Segment_MonthYear,column='Segment')\n",
    "subcat_month_year = MonthYear_clean(SubCategory_MonthYear,column='SubCategory')\n",
    "subseg_month_year = MonthYear_clean(SubSegment_MonthYear,column='SubSegment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6ac48172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_month_year(data, column):\n",
    "    final_month_year ={}\n",
    "    for key,df in data.items():\n",
    "        for sec in df[column].unique():\n",
    "            newkey = key + ' | ' + sec\n",
    "            new_df = df[df[column] == sec].reset_index(drop=True)\n",
    "            if new_df.shape[0] > 0:\n",
    "                final_month_year[newkey] = new_df\n",
    "    return final_month_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "265d60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_month_year1=split_month_year(category_month_year,'Category')\n",
    "sector_month_year1 = split_month_year(sector_month_year,'Sector')\n",
    "segment_month_year1 = split_month_year(segment_month_year,'Segment')\n",
    "subseg_month_year1 = split_month_year(subseg_month_year,'SubSegment')\n",
    "subcat_month_year1 = split_month_year(subcat_month_year,'SubCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b92a3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_year1 = {}\n",
    "month_year1.update(sector_month_year1)\n",
    "month_year1.update(segment_month_year1)\n",
    "month_year1.update(subcat_month_year1)\n",
    "month_year1.update(subseg_month_year1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a94833af",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarketCategory = [key for key in category_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in sector_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in segment_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in subseg_month_year1.keys() if any(cat == key.split(' | ')[2] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in subcat_month_year1.keys() if any(cat == key.split(' | ')[2] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "17cb72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeDates1(dfList, promotionsEndOfWeekCleaned,column=\"Sector\"):\n",
    "    \"\"\"\n",
    "    Complete dates for each dataframe in dfList based on promotionsEndOfWeekCleaned.\n",
    "\n",
    "    Parameters:\n",
    "    dfList (list): List of dataframe keys.\n",
    "    promotionsEndOfWeekCleaned (dict): Dictionary containing cleaned promotions end of week data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing EndOfWeekcompletDate (dictionary), dfGroup (list), and dic (dictionary).\n",
    "    \"\"\"\n",
    "    # Create a list of unique brand-category combinations\n",
    "    brandCatList = sorted(set(key.split(' | ')[0]  for key in dfList))\n",
    "    # Initialize dictionaries and lists\n",
    "    EndOfWeekcompletDate = {}\n",
    "    dfGroup = []\n",
    "    dic = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each brand-category combination\n",
    "    for key in brandCatList:\n",
    "        for name in dfList:\n",
    "            if (key.split(' | ')[0] == name.split(' | ')[0]):\n",
    "                \n",
    "                dic[key] += 1\n",
    "                \n",
    "    # Iterate over unique brand-category combinations\n",
    "    for name in dic.keys():\n",
    "\n",
    "        if column == \"Sector\" :\n",
    "            \n",
    "            # Filter dataframe keys associated with the current brand-category combination\n",
    "            dfName = [key for key in dfList if name == key.split(' | ')[0] and len(name.split(' | ')) == 1]\n",
    "        else:\n",
    "            \n",
    "            dfName = [key for key in dfList if name == key.split(' | ')[0]  ]\n",
    "        \n",
    "        # Extract unique dates from all associated dataframes\n",
    "        uniqueDates = pd.concat([promotionsEndOfWeekCleaned[key] for key in dfName])[['MonthYear']].drop_duplicates()\n",
    "        # Initialize dictionary for complete dates for each dataframe\n",
    "        dfCompleteDates = {}\n",
    "        # Add dataframe keys to the group list\n",
    "        dfGroup.append(dfName)\n",
    "        # Populate EndOfWeekcompletDate dictionary with dataframes merged on unique dates\n",
    "        for key in dfName:\n",
    "            EndOfWeekcompletDate[key] = pd.merge(uniqueDates, promotionsEndOfWeekCleaned[key], how='left')#.replace(np.nan, 0)\n",
    "            column = EndOfWeekcompletDate[key].columns[1]\n",
    "            year = EndOfWeekcompletDate[key].columns[3]\n",
    "            monthyear = EndOfWeekcompletDate[key].columns[0]\n",
    "            EndOfWeekcompletDate[key][column] = EndOfWeekcompletDate[key][column].fillna(method='ffill')      \n",
    "            EndOfWeekcompletDate[key][year] = pd.to_datetime(EndOfWeekcompletDate[key][monthyear], format='%b-%y').dt.year\n",
    "            EndOfWeekcompletDate[key] = EndOfWeekcompletDate[key].fillna(0)\n",
    "    return EndOfWeekcompletDate, dfGroup, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3b1814ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCategory0,categoryGroup0,categoryDuplication0=completeDates1(brandMarketCategory,category_month_year1,column=\"Category\")\n",
    "if len(sectors) != 0:\n",
    "    dfSector0,secGroup0,secDuplication0=completeDates1(brandMarketSector,sector_month_year1,column=\"Sector\")\n",
    "if len(segments) != 0:\n",
    "    dfSegment0,segGroup0,segDuplication0=completeDates1(brandMarketSegment,segment_month_year1,column=\"Segment\")\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment0,subsegGroup0,subsegDuplication0=completeDates1(brandMarketSubSegment,subseg_month_year1,column=\"Subsegment\")\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory0,subcatGroup0,subcatDuplication0=completeDates1(brandMarketSubCategory,subcat_month_year1,column=\"Subcategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "99912278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupingkeys(data):\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    for sublist in data:\n",
    "        for entry in sublist:\n",
    "            prefix = \" | \".join(entry.split(\" | \")[:2])  # Extract first two parts\n",
    "            grouped[prefix].append(entry)\n",
    "    result = list(grouped.values())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "55074d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryGroup0=groupingkeys(categoryGroup0)\n",
    "if len(sectors) != 0:\n",
    "    secGroup0=groupingkeys(secGroup0)\n",
    "if len(segments) != 0:\n",
    "    segGroup0=groupingkeys(segGroup0)\n",
    "if len(subsegments) != 0:\n",
    "    subsegGroup0=groupingkeys(subsegGroup0)\n",
    "if len(subcategories) != 0:\n",
    "    subcatGroup0=groupingkeys(subcatGroup0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 20, 20, 20, 20, 20, 20, 20, 20], [21, 21, 21, 21, 21, 21, 21, 21, 21]]\n",
      "[[20, 20, 20, 20, 20, 20, 20, 20, 20], [21, 21, 21, 21, 21, 21, 21, 21, 21], [21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20]]\n",
      "[[20, 20, 20, 20, 20, 20, 20, 20, 20], [21, 21, 21, 21, 21, 21, 21, 21, 21], [21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20], []]\n"
     ]
    }
   ],
   "source": [
    "final_lis0 = []\n",
    "category_lis = []\n",
    "if categories:\n",
    "    for i in range(len(categoryGroup0)):\n",
    "        category_lis += genrateIndexList(categoryGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(category_lis)  # Append empty list if sectors is False\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup0)):\n",
    "        sec_lis += genrateIndexList(secGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(sec_lis)  # Append empty list if sectors is False\n",
    "print(final_lis0)\n",
    "\n",
    "seg_lis = []\n",
    "if segments:\n",
    "    for i in range(len(segGroup0)):\n",
    "        seg_lis += genrateIndexList(segGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(seg_lis)  # Append empty list if segments is False\n",
    "print(final_lis0)\n",
    "\n",
    "subseg_lis = []\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup0)):\n",
    "        subseg_lis += genrateIndexList(subsegGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(subseg_lis)  # Append empty list if subsegments is False\n",
    "print(final_lis0)\n",
    "\n",
    "subcat_lis = []\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup0)):\n",
    "        subcat_lis += genrateIndexList(subcatGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(subcat_lis)  # Append empty list if subcategories is False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc50143",
   "metadata": {},
   "source": [
    "### New slide 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3653fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_endofweek_P12M = {}\n",
    "past_12_months = pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n",
    "for key in modified_promotionEndOfWeek.keys():\n",
    "    df=modified_promotionEndOfWeek[key].copy()\n",
    "    df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "    filtered_df = df[df['End of Week'].dt.strftime('%b-%y').isin(past_12_months)]\n",
    "    if filtered_df.shape[0] >0:\n",
    "        modified_endofweek_P12M[key] = filtered_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "06d18d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarket = list(set([key.split(' | ')[0]+' | '+key.split(' | ')[1] for key in modified_endofweek_P12M]))\n",
    "brandMarketCategory= [key for key in modified_endofweek_P12M.keys() if any(cat in key.split(' | ')[-1] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[-1] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[-1] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[-1] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[-1] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3ace0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory1,catGroup1,catDuplication1=completeDates(brandMarketCategory,modified_endofweek_P12M)\n",
    "if len(sectors) != 0:\n",
    "    dfSector1,secGroup1,secDuplication1=completeDates(brandMarketSector,modified_endofweek_P12M)\n",
    "if len(segments) != 0:\n",
    "    dfSegment1,segGroup1,segDuplication1=completeDates(brandMarketSegment,modified_endofweek_P12M)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment1,subsegGroup1,subsegDuplication1=completeDates(brandMarketSubSegment,modified_endofweek_P12M)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory1,subcatGroup1,subcatDuplication1=completeDates(brandMarketSubCategory,modified_endofweek_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8479ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promofrequencyclean(data):        \n",
    "        modified_dfCategory1 = {}\n",
    "        for k in data.keys():\n",
    "                chart_df=data[k].copy()\n",
    "                chart_df['Weekly VSOD'] = np.where((chart_df['VSOD']>.2)&(chart_df['Value Uplift (v. base) Normalized'] != ''),1,None)\n",
    "                chart_df['try'] = 0\n",
    "                chart_df['New Uplift'] = 0\n",
    "                chart_df['try'] = np.where((chart_df['Value Uplift (v. base) Normalized']>=2),1.8,chart_df['Value Uplift (v. base) Normalized'])\n",
    "                chart_df['New Uplift'] = np.where((chart_df['Weekly VSOD']==1)&(chart_df['Value Uplift (v. base) Normalized']>0.05),chart_df['try'],None)\n",
    "                if not chart_df['Weekly VSOD'].isnull().all():\n",
    "                        modified_dfCategory1[k]= chart_df \n",
    "        return modified_dfCategory1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9ad97599",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories)!=0: \n",
    "    modified_dfCategory1=promofrequencyclean(dfCategory1)\n",
    "if len(sectors)!=0: \n",
    "    modified_dfSector1=promofrequencyclean(dfSector1)\n",
    "if len(segments)!=0: \n",
    "    modified_dfSegment1=promofrequencyclean(dfSegment1)\n",
    "if len(subsegments)!=0: \n",
    "    modified_dfSubSegment1=promofrequencyclean(dfSubSegment1)\n",
    "if len(subcategories)!=0: \n",
    "    modified_dfSubCategory1=promofrequencyclean(dfSubCategory1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec62b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2f0aae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarket = list(set([key.split(' | ')[0]+' | '+key.split(' | ')[1] for key in modified_endofweek_P12M]))\n",
    "brandMarketCategory= [key for key in modified_dfCategory1.keys() if any(cat in key.split(' | ')[-1] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in modified_dfSector1.keys() if any(cat == key.split(' | ')[-1] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in modified_dfSegment1.keys() if any(cat == key.split(' | ')[-1] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in modified_dfSubSegment1.keys() if any(cat == key.split(' | ')[-1] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in modified_dfSubCategory1.keys() if any(cat == key.split(' | ')[-1] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "31f3763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory1,catGroup1,catDuplication1=completeDates(brandMarketCategory,modified_endofweek_P12M)\n",
    "if len(sectors) != 0:\n",
    "    dfSector1,secGroup1,secDuplication1=completeDates(brandMarketSector,modified_endofweek_P12M)\n",
    "if len(segments) != 0:\n",
    "    dfSegment1,segGroup1,segDuplication1=completeDates(brandMarketSegment,modified_endofweek_P12M)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment1,subsegGroup1,subsegDuplication1=completeDates(brandMarketSubSegment,modified_endofweek_P12M)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory1,subcatGroup1,subcatDuplication1=completeDates(brandMarketSubCategory,modified_endofweek_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c2bbe9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_region(data):\n",
    "    # Define categories for grouping\n",
    "    market_groups = {\n",
    "        \"RETAILER_REGIONS\": regions_RET,\n",
    "        \"RETAILER_CHANNELS\": channels_RET,\n",
    "        \"RETAILER_MARKET\": market_RET,\n",
    "        \"CHANNEL_REGIONS\": regions_CHAN,\n",
    "        \"CHANNEL_CHANNELS\": channels_CHAN,\n",
    "        \"CHANNEL_MARKET\": market_CHAN,\n",
    "        f\"{customareas}_REGIONS\": regions_CUST,\n",
    "        f\"{customareas}_CHANNELS\": channels_CUST,\n",
    "        f\"{customareas}_MARKET\": market_CUST,\n",
    "    }\n",
    "    result = []\n",
    "    for sublist in data:\n",
    "        for category, keywords in market_groups.items():\n",
    "            # Filter items matching the current category\n",
    "            base_category = category.split(\"_\")[0]\n",
    "\n",
    "            group = [\n",
    "                f\"{item} | {base_category}\" for item in sublist if item.split(\" | \")[1] in keywords\n",
    "            ]\n",
    "            if group:  # Append only non-empty groups\n",
    "                result.append(group)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "016f5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories)>0:\n",
    "    catGroup1 = group_by_region(catGroup1)\n",
    "if len(sectors)>0:\n",
    "    secGroup1 = group_by_region(secGroup1)\n",
    "if len(segments)>0:\n",
    "    segGroup1 = group_by_region(segGroup1)\n",
    "if len(subsegments)>0:\n",
    "    subsegGroup1 = group_by_region(subsegGroup1)\n",
    "if len(subcategories)>0:\n",
    "    subcatGroup1 = group_by_region(subcatGroup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "95b943e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lis1 = []\n",
    "cat_lis = []\n",
    "if categories:\n",
    "    for i in range(len(catGroup1)):\n",
    "        cat_lis += genrateIndexList(catGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(cat_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup1)):\n",
    "        sec_lis += genrateIndexList(secGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(sec_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "seg_lis=[]\n",
    "if segments:\n",
    "    for i in range(len(segGroup1)):\n",
    "        seg_lis += genrateIndexList(segGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(seg_lis)\n",
    "\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "subseg_lis =[]\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup1)):\n",
    "        subseg_lis +=  genrateIndexList(subsegGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(subseg_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "subcat_lis =[]\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup1)):\n",
    "        subcat_lis +=  genrateIndexList(subcatGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(subcat_lis)\n",
    "else:\n",
    "    final_lis1.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "21faef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "retailer=regions_RET+channels_RET+market_RET\n",
    "channels=regions_CHAN+channels_CHAN+channels_CHAN\n",
    "customarea=regions_CUST+channels_CUST+market_CUST\n",
    "def addarea(modified_dfCategory1,retailer,market=\"RETAILER\"):\n",
    "    keys_to_modify = [k for k in modified_dfCategory1.keys() if k.split(\" | \")[1] in retailer]\n",
    "    for k in keys_to_modify:\n",
    "        new_key = k + \" | \"+ market\n",
    "        modified_dfCategory1[new_key] = modified_dfCategory1[k]  \n",
    "        del modified_dfCategory1[k]       \n",
    "    return modified_dfCategory1      \n",
    "if len(categories)>0:            \n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,retailer,market=\"RETAILER\")\n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,channels,market=\"CHANNELS\")\n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,customarea,market=f\"{customareas}\")\n",
    "\n",
    "if len(sectors)>0:            \n",
    "    modified_dfSector1=addarea(modified_dfSector1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSector1=addarea(modified_dfSector1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSector1=addarea(modified_dfSector1,customarea,market=f\"{customareas}\")\n",
    "if len(segments)>0:            \n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,customarea,market=f\"{customareas}\")\n",
    "if len(subsegments)>0:            \n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,customarea,market=f\"{customareas}\")\n",
    "if len(subcategories)>0:            \n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,customarea,market=f\"{customareas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ec5211df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(modified_promotionBrandsP12M.keys()):  # Convert to list to avoid runtime errors\n",
    "    df = modified_promotionBrandsP12M[k].copy()\n",
    "    # Filter rows based on 'Top Brands'\n",
    "    df = df[~df['Top Brands'].str.contains('Others', case=False, na=False)]\n",
    "    df = df[~df['Top Brands'].str.contains('Grand Total', case=False, na=False)]\n",
    "    df = df[df['Value Share'] > 0.01]\n",
    "    if not df.empty:\n",
    "        modified_promotionBrandsP12M[k] = df\n",
    "    else:\n",
    "        del modified_promotionBrandsP12M[k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee2a74",
   "metadata": {},
   "source": [
    "\n",
    "## Slide duplication: index, duplication and section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a330545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [*[2]*5,*[11]*5,*[2]*5]\n",
    "duplication = combine_duplications(Scope,count_df,[concated,PromoSalesTypes_data if promo_type else None,concated])\n",
    "section_names = [\"Value uplift by retailer by brand\",\"Promo Sales by promo type\",\"Value uplift by retailer by brand no client prio\"]\n",
    "section_names = [f\"{name} {suffix}\" for name in section_names for suffix in suffixes]\n",
    "\n",
    "\n",
    "path = os.getcwd() + '//Promotion base.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated.pptx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ceca67f7-038d-44e1-98aa-cbc9a32d8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [*[0]*5,\n",
    "         #*[1]*5,\n",
    "         *[1]*5,\n",
    "         *[2]*5,\n",
    "         *[3]*5,\n",
    "         *[4]*5,\n",
    "         *[5]*5,\n",
    "         *[6]*5,\n",
    "         *[7]*5,\n",
    "         *[8]*4,\n",
    "         *[9]*5,\n",
    "         *[10]*5,\n",
    "         *[11]*5,\n",
    "         *[12]*5,\n",
    "         *[13]*5,\n",
    "         *[14]*5,\n",
    "         *final_lis,\n",
    "         *[19]*5,\n",
    "         *final_lis0,\n",
    "         *final_lis1,\n",
    "         *[0]*5,\n",
    "         *[1]*5,\n",
    "         *[2]*5,\n",
    "         *[9]*5,\n",
    "         *[10]*5,\n",
    "        #  *[11]*5,\n",
    "         *[12]*5,\n",
    "         *[13]*5\n",
    "         #*[14]*5\n",
    "        ]\n",
    "duplication = combine_duplications(Scope,count_df,[#modified_promotionBrandsP12M, #0\n",
    "                                                   promotionsBrandSortedTotalFinal, #1\n",
    "                                                   newpromotionsNotBrandsWithMarket, #2\n",
    "                                                   concated, #3\n",
    "                                                   modified_promotionProductsP12M_volumeuplift, #4\n",
    "                                                   new_modified_promotionProductsP12M, #5\n",
    "                                                   new_modified_promotionProductsP12M, #6\n",
    "                                                   top20clientonly, #7\n",
    "                                                   bottom20clientonly,#8\n",
    "                                                   modified_promotionBrandsP12M, #10\n",
    "                                                   newModifiedBrands, #11\n",
    "                                                   PromoSalesTypes_data if promo_type else None,#12\n",
    "                                                   modified_promotionBrandsP12M if feature_share else None, #13\n",
    "                                                   modified_promotionBrandsP12M if display_share else None, #14\n",
    "                                                   modified_promotionEndOfWeek,#15\n",
    "                                                   PromoRet, #16-19\n",
    "                                                   modified_valueUplift, #20\n",
    "                                                   #month_year1,#21\n",
    "                                                   #modified_endofweek_P12M, #22\n",
    "                                                   #modified_promotionBrandsP12M, #0 with no client\n",
    "                                                   promotionsBrandNOTSortedTotalFinal, #1 with no client\n",
    "                                                   newpromotionsNotBrandsWithMarket, #2 with no client\n",
    "                                                   concated, #3 with no client\n",
    "                                                   modified_promotionBrandsP12M, # 10 with no client\n",
    "                                                   newModifiedBrands, #11 with no client\n",
    "                                                   #PromoSalesTypes_data if promo_type else None,#12  with no client\n",
    "                                                   modified_promotionBrandsP12M if feature_share else None, #13  with no client\n",
    "                                                   modified_promotionBrandsP12M if display_share else None #14 with no client\n",
    "                                                  ])\n",
    "section_names = [#\"Promo Value Sale\",#0\n",
    "                 \"Promo Evolution\", #1\n",
    "                 \"VSOD Summary by Sector\" , #2\n",
    "                 \"Value uplift by retailer by brand\", #3 \n",
    "                 \"Volume Uplift vs discount depth\",#4\n",
    "                 \"Value Uplift vs Promo Efficiency Quadrant\", #5\n",
    "                 \"Top 20 promotions\", #6\n",
    "                 \"Top 20 promotions CLIENT ONLY\", #7\n",
    "                 \"Bottom 20 promotions CLIENT ONLY\", #8\n",
    "                 \"Promo share vs Value Share\", #10\n",
    "                 \"Promo Sales by total size\",#11\n",
    "                 \"Promo Sales by promo type\", #12\n",
    "                 \"Feature Share vs Fair Share\", #13\n",
    "                 \"Display Share vs Fair Share\", #14\n",
    "                 \"Promo Frequency learnings\", #15\n",
    "                 \"Promo sales per retailer\", #16-19\n",
    "                 \"Value Uplift vs discount depth\" ,#20\n",
    "                 #\"Seasonality Index\",#21\n",
    "                 #\"Promotional Frequency Analysis\", #22\n",
    "                 #\"Promo Value Sale no client prio\",\n",
    "                 \"Promo Evolution no client prio\",\n",
    "                 \"VSOD Summary by Sector no client prio\",\n",
    "                 \"Value uplift by retailer by brand no client prio\",\n",
    "                \"Promo share vs Value Share no client prio\", #10\n",
    "                 \"Promo Sales by total size no client prio\",#11\n",
    "                 #\"Promo Sales by promo type no client prio\", #12\n",
    "                 \"Feature Share vs Fair Share no client prio\", #13\n",
    "                 \"Display Share vs Fair Share no client prio\" #14\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "#duplication.insert(89, 0)\n",
    "\n",
    "if len(sectors) > 0:\n",
    "       #duplication.insert(45,(len(client_manuf)+len(client_brands))*len(categories)* len(marketList))\n",
    "       duplication.insert(40, sect_vsod_count)\n",
    "if len(segments) > 0:\n",
    "        #duplication.insert(46,(len(client_manuf)+len(client_brands))*len(sectors)* len(marketList)) \n",
    "         duplication.insert(41, seg_vsod_count)\n",
    " \n",
    "else:\n",
    "    duplication.insert(41,0)  \n",
    "  \n",
    "if len(subsegments) > 0:\n",
    "        #duplication.insert(47,(len(client_manuf)+len(client_brands))*len(segments)* len(marketList))\n",
    "        duplication.insert(42, subseg_vsod_count)\n",
    "\n",
    "else:\n",
    "    duplication.insert(42,0)\n",
    "\n",
    "if len(subcategories) > 0:\n",
    "        #duplication.insert(48,(len(client_manuf)+len(client_brands))*len(segments)* len(marketList))\n",
    "        duplication.insert(43, subcat_vsod_count)\n",
    "\n",
    "else:\n",
    "    duplication.insert(43,0)\n",
    "\n",
    "\n",
    "duplication.insert(84,1)\n",
    "duplication.insert(85, 1)\n",
    "duplication.insert(86, 1)\n",
    "duplication.insert(87, 1)\n",
    "duplication.insert(88, 0)\n",
    "duplication.insert(89, 1)\n",
    "duplication.insert(90, 1)\n",
    "duplication.insert(91, 1)\n",
    "duplication.insert(92, 1)\n",
    "duplication.insert(93, 0)\n",
    "\n",
    "section_names = [f\"{name} {suffix}\" for name in section_names for suffix in suffixes]\n",
    "\n",
    "section_names.insert(40,'Volume Sold on Deal Sector')\n",
    "section_names.insert(41,'Volume Sold on Deal Segment')\n",
    "section_names.insert(42,'Volume Sold on Deal SubSegment')\n",
    "section_names.insert(43,'Volume Sold on Deal SubCategory')\n",
    "\n",
    "section_names.insert(84,'Seasonality Index Category')\n",
    "section_names.insert(85,'Seasonality Index Sector')\n",
    "section_names.insert(86,'Seasonality Index Segment')\n",
    "section_names.insert(87,'Seasonality Index Subsegment')\n",
    "section_names.insert(88,'Seasonality Index Subcategory')\n",
    "\n",
    "section_names.insert(89,'Promotional Frequency Analysis Category')\n",
    "section_names.insert(90,'Promotional Frequency Analysis Sector')\n",
    "section_names.insert(91,'Promotional Frequency Analysis Segment')\n",
    "section_names.insert(92,'Promotional Frequency Analysis Subsegment')\n",
    "section_names.insert(93,'Promotional Frequency Analysis Subcategory')\n",
    "#section_names.insert(94,'Promotional Frequency Analysis Subcategory')\n",
    "\n",
    "duplication[77]=1\n",
    "#index = [i for i in index if i != []]\n",
    "# duplication = [i for i in duplication if i != []]\n",
    "#\n",
    "\n",
    "path = os.getcwd() + '//Promotion base.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated.pptx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "69ddaa2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newModifiedBrands.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9511d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = [ *[10]*5,*[10]*5]\n",
    "# duplication = combine_duplications(Scope,count_df,[newModifiedBrands,newModifiedBrands])\n",
    "# section_names = [\"Promo Sales by total size\",\"Promo Sales by total size no client prio\"]\n",
    "# section_names = [f\"{name} {suffix}\" for name in section_names for suffix in suffixes]\n",
    "\n",
    "# path = os.getcwd() + '//Promotion base.pptx'\n",
    "# new_pre = os.getcwd() + '//slide duplicated.pptx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bbcf3",
   "metadata": {},
   "source": [
    "### Deck 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1a93e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 11, 11, 11, 11, 11, 2, 2, 2, 2, 2]\n",
      "[1, 2, 3, 0, 0, 4, 8, 12, 0, 0, 1, 2, 3, 0, 0]\n",
      "['Value uplift by retailer by brand Category', 'Value uplift by retailer by brand Sector', 'Value uplift by retailer by brand Segment', 'Value uplift by retailer by brand SubSegment', 'Value uplift by retailer by brand SubCategory', 'Promo Sales by promo type Category', 'Promo Sales by promo type Sector', 'Promo Sales by promo type Segment', 'Promo Sales by promo type SubSegment', 'Promo Sales by promo type SubCategory', 'Value uplift by retailer by brand no client prio Category', 'Value uplift by retailer by brand no client prio Sector', 'Value uplift by retailer by brand no client prio Segment', 'Value uplift by retailer by brand no client prio SubSegment', 'Value uplift by retailer by brand no client prio SubCategory']\n",
      "15\n",
      "15\n",
      "15\n",
      "[[18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15], [18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15], [18, 18, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15], [], []]\n",
      "[[26, 28, 29, 26, 26, 27, 26, 27, 26, 28, 29, 26], [26, 28, 29, 26, 26, 28, 29, 26, 28, 29, 26, 26, 28, 29, 26, 28, 29, 26, 26, 28, 29, 26, 26, 28, 29, 26], [26, 27, 29, 26, 26, 28, 29, 26, 26, 28, 29, 26, 28, 29, 26, 26, 28, 29, 26, 28, 29, 26, 26, 28, 29, 26, 26, 28, 29, 26, 26, 28, 29, 26], [], []]\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(index)\n",
    "print(duplication)\n",
    "print(section_names)\n",
    "print(len(index))\n",
    "print(len(duplication))\n",
    "print(len(section_names))\n",
    "print(final_lis)\n",
    "print(final_lis1)\n",
    "print(sum(duplication))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "474f4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb24e2",
   "metadata": {},
   "source": [
    "## If we want specific slides duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e14224cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slides_name = ['Promo Frequency learnings']\n",
    "# indices = [i for i, s in enumerate(section_names) if any(sub.lower() in s.lower() for sub in slides_name)]\n",
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "016df9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_section_names = [section_names[i] for i in indices]\n",
    "# filtered_duplication = [duplication[i] for i in indices]\n",
    "# filtered_index = [index[i] for i in indices]\n",
    "\n",
    "# print(filtered_section_names)\n",
    "# print(filtered_duplication)\n",
    "# print(filtered_index)\n",
    "\n",
    "# print(len(filtered_section_names))\n",
    "# print(len(filtered_duplication))\n",
    "# print(len(filtered_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c8ba8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slideDuplication(filtered_index,filtered_duplication,filtered_section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7a50ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prs = Presentation(new_pre)\n",
    "# posItr = 0\n",
    "# ind =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b8412c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,value in Scope.items():\n",
    "#     dict = {key: count_df(modified_promotionEndOfWeek,value) }\n",
    "#     for key1,value1 in dict.items():\n",
    "#         filtered_dict = {key: value for key, value in modified_promotionEndOfWeek.items() if key in dict[key1]}\n",
    "#         if filtered_dict:\n",
    "#             PromoFrequency(prs,filtered_dict,filtered_duplication[ind],position=sum(filtered_duplication[:ind]))\n",
    "#         posItr += len(filtered_dict)\n",
    "#         ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cb30cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputPath=os.getcwd() + f\"\\\\Promotion {client_manuf[0]}.pptx\"\n",
    "# prs.save(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830c75f",
   "metadata": {},
   "source": [
    "## Replace duplicated slides with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "78376167-8225-4f4f-af41-cd366f3e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation(new_pre)\n",
    "posItr = 0\n",
    "ind =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c562c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promoValueSales(prs, promotionsBrandDF, numOfDuplicates, position=0):\n",
    "    \"\"\"\n",
    "    Generate PowerPoint slides for promo value sales.\n",
    "\n",
    "    Args:\n",
    "    - prs (pptx.presentation): PowerPoint presentation object.\n",
    "    - promotionsBrandDF (dict): Dictionary of DataFrames containing promotion data for different markets.\n",
    "    - numOfDuplicates (int): Number of slides to duplicate for different markets.\n",
    "    - position (int): Position to start adding slides in the presentation.\n",
    "\n",
    "    Returns:\n",
    "    - Replace the slides with new data\n",
    "    \"\"\"\n",
    "    # Loop through each slide number\n",
    "    slidenum = 0\n",
    "    for key,df in promotionsBrandDF.items():\n",
    "        # Retrieve DataFrame for the current market\n",
    "        df = promotionsBrandDF[key].reset_index(drop=True)\n",
    "        \n",
    "        # Remove rows with 'Others' in 'Top Brands' column and filter by 'Value Share'\n",
    "        df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "        df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    "        df = df[df['Value Share'] > 0.01]\n",
    "     \n",
    "  \n",
    "        # Select client brands\n",
    "        df_client = selectClientBrands(df,'Top Brands', 'Promo Value')\n",
    "        number_of_brands_needed = max(6 - len(df_client),0)\n",
    "     \n",
    "        # Filter top brands and concatenate with client brands\n",
    "        df = df[~df['Top Brands'].isin(client_brands)]\n",
    "        df = df.sort_values(by='Promo Value', ascending=False).head(number_of_brands_needed)\n",
    "        df = pd.concat([df, df_client], ignore_index=True)\n",
    "        df = df.sort_values(by='Promo Value', ascending=False)\n",
    "    \n",
    "        \n",
    "        # Update title\n",
    "        shapes = prs.slides[slidenum + position].shapes\n",
    "        titlNumber = get_shape_number(shapes, \"Promo Value Sales | Category | National | P12M\")\n",
    "        headerNumber = get_shape_number(shapes, \"Promo Value Sales (Replace With SO WHAT)\")\n",
    "        shapes[titlNumber - 1].text = data_source\n",
    "        shapes[headerNumber].text_frame.paragraphs[0].font.size = Pt(16)\n",
    "        shapes[headerNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "        shapes[titlNumber].text = shapes[titlNumber].text.replace('Category', key.split(' | ')[0]).replace(\n",
    "            'National', key.split(' | ')[1])\n",
    "        shapes[titlNumber].text_frame.paragraphs[0].font.size = Pt(12)\n",
    "        shapes[titlNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "        \n",
    "        # Create table and chart\n",
    "        tables, charts = createTableAndChart(shapes)\n",
    "        table = tables[0].table\n",
    "        \n",
    "        # Remove unnecessary rows\n",
    "        num_rows_to_remove = len(table.rows) - df['Top Brands'].nunique() - 1\n",
    "        table_height = get_table_height(table)\n",
    "        for _ in range(num_rows_to_remove):\n",
    "            if len(table.rows) > 1:\n",
    "                row = table.rows[1]\n",
    "                remove_row(table, row)\n",
    "        \n",
    "        # Adjust row heights\n",
    "        total_row_height = table_height - table.rows[0].height\n",
    "        num_rows = len(table.rows) - 1\n",
    "        if num_rows > 0:\n",
    "            cell_height = total_row_height / num_rows\n",
    "            for row in range(1, table.rows.__len__()):\n",
    "                table.rows[row].height = int(cell_height)\n",
    "        \n",
    "        # Populate table cells\n",
    "        for i, row in enumerate(table.rows):\n",
    "            for j, cell in enumerate(row.cells):\n",
    "                if i == 0:  # Header row\n",
    "                    continue\n",
    "                if j == 0:  # Brand column\n",
    "                    cell.text = df['Top Brands'].iloc[i - 1]\n",
    "                    cell.text_frame.paragraphs[0].font.name = 'Nexa Bold'\n",
    "                elif j == 1:  # Promo Value sales column\n",
    "                    value = df['Promo Value'].iloc[i - 1]\n",
    "                    if len(str(value)) > 3:\n",
    "                        formatted_value = '{:,}'.format(int(value))\n",
    "                        cell.text = str(formatted_value)\n",
    "                        cell.text_frame.paragraphs[0].font.name = 'Nexa Book'\n",
    "                    else:\n",
    "                        cell.text = str(df['Promo Value'].iloc[i - 1])\n",
    "                        cell.text_frame.paragraphs[0].font.name = 'Nexa Book'\n",
    "                elif j == 2:  # Volume Sold on Deal (VSOD) column\n",
    "                    cell.text = str(int(round(df['VSOD'].replace(np.nan, 0).iloc[i - 1] * 100, 0))) + '%'\n",
    "                    cell.text_frame.paragraphs[0].font.name = 'Nexa Book'\n",
    "                else:  # VSOD IYA column\n",
    "                    cell.text = str(int(round(df['VSOD IYA'].replace(np.nan, 0).iloc[i - 1] * 100, 0)))\n",
    "                    cell.text_frame.paragraphs[0].font.name = 'Nexa Book'\n",
    "                # Set font size and alignment\n",
    "                cell.text_frame.paragraphs[0].font.size = Pt(8)\n",
    "                cell.text_frame.paragraphs[0].alignment = PP_ALIGN.CENTER\n",
    "        slidenum +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2e21be34-0498-4c06-b4e4-bd68b94e38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slide 1\n",
    "# for key,value in Scope.items():\n",
    "#     dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "#     for key1,value1 in dict.items():\n",
    "#         filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "#         if filtered_dict:\n",
    "#             promoValueSales(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "#             posItr += len(filtered_dict)\n",
    "#         ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f3e5ba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2a49aea0-21ff-4bf4-9427-d2f7a6b8a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 2\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(promotionsBrandSortedTotalFinal,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in promotionsBrandSortedTotalFinal.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            promoEvolutionNew(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ec6850ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 98\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1cd4fcae-6d85-4244-82e5-9a19f05b6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 3\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newpromotionsBrandsWithMarket,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newpromotionsBrandsWithMarket.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VSOD1(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "92914c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 110\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1b8dc777-7e02-4573-b8f9-000bb04fcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 4\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(concated,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in concated.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUpliftRetailer(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "27312e22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mind\u001b[49m,posItr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ind' is not defined"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9c8ee2a2-5cdf-41f3-ad8a-95fb9baacc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 5\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionProductsP12M_volumeuplift,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionProductsP12M_volumeuplift.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VolumeUplift(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cd0b667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "30f24f2e-b093-4fe6-8605-1ef06f69e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 6\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(new_modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in new_modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            ValueUpliftvsPromoEfficiencyQuadrant(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "569c3cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 317\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ba714fd4-a9da-45d9-9eb0-1a1889324bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 7\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(new_modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in new_modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            top20(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "477805d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 404\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "df90b497-4ca3-4467-9965-3eeba68fef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 8\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(top20clientonly,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in top20clientonly.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            top20Client(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e578ca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 484\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4108f604-685c-4fd8-aab4-5c66967a67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 9\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(bottom20clientonly,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in bottom20clientonly.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            bot20Client(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "16765a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 566\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "762a2079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566\n"
     ]
    }
   ],
   "source": [
    "if len(sectors)>0:\n",
    "    newVolumeSold(prs, sect_vsod_merged, position=posItr, parent=direct_parent['Sector'], child = 'Sector')\n",
    "    print(posItr)\n",
    "    posItr += sect_vsod_count\n",
    "    ind +=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "244a0620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 620\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d41024fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(segments)>0:\n",
    "    newVolumeSold(prs, seg_vsod_merged, position=posItr, parent=direct_parent['Segment'], child = 'Segment')\n",
    "    posItr += seg_vsod_count\n",
    "    ind +=1\n",
    "    \n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "66bc536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 830\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subsegments)>0:\n",
    "    newVolumeSold(prs, subseg_vsod_merged, position=posItr, parent=direct_parent['SubSegment'], child = 'SubSegment')\n",
    "    posItr += subseg_vsod_count\n",
    "    ind+=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "780b7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 1340\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2298d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subcategories)>0:\n",
    "    newVolumeSold(prs, subcat_vsod_merged, position=posItr, parent=direct_parent['SubCategory'], child = 'SubCategory')\n",
    "    posItr += subcat_vsod_count\n",
    "    ind+=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5677352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 1340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0ee42400-58ae-4311-8ac7-9914c0ae666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 11\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoShare_vs_ValueShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7f7b6a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 1447\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "569341f7-9b94-4bf5-9114-fe10a6d0cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yogurt | Ahorramas\n",
      "6\n",
      "Yogurt | Carrefour\n",
      "6\n",
      "Yogurt | Mercadona\n",
      "6\n",
      "Ahorramas | Functionals\n",
      "6\n",
      "Ahorramas | Every Day Nutrition\n",
      "6\n",
      "Ahorramas | Plant Based\n",
      "6\n",
      "Ahorramas | Protein\n",
      "6\n",
      "Carrefour | Functionals\n",
      "6\n",
      "Carrefour | Every Day Nutrition\n",
      "6\n",
      "Carrefour | Plant Based\n",
      "6\n",
      "Carrefour | Protein\n",
      "6\n",
      "Mercadona | Functionals\n",
      "6\n",
      "Mercadona | Every Day Nutrition\n",
      "6\n",
      "Mercadona | Plant Based\n",
      "6\n",
      "Ahorramas | Plant Based\n",
      "6\n",
      "Ahorramas | Protein\n",
      "6\n",
      "Carrefour | Plant Based\n",
      "6\n",
      "Carrefour | Protein\n",
      "6\n",
      "Mercadona | Plant Based\n",
      "6\n",
      "Ahorramas | Bifidus\n",
      "6\n",
      "Ahorramas | Cholesterol\n",
      "6\n",
      "Ahorramas | Essential\n",
      "6\n",
      "Ahorramas | Greek\n",
      "6\n",
      "Ahorramas | Immunity\n",
      "6\n",
      "Ahorramas | Kefir\n",
      "6\n",
      "Ahorramas | Kids\n",
      "6\n",
      "Ahorramas | Light\n",
      "6\n",
      "Carrefour | Bifidus\n",
      "6\n",
      "Carrefour | Cholesterol\n",
      "6\n",
      "Carrefour | Essential\n",
      "6\n",
      "Carrefour | Greek\n",
      "6\n",
      "Carrefour | Immunity\n",
      "6\n",
      "Carrefour | Kefir\n",
      "6\n",
      "Carrefour | Kids\n",
      "6\n",
      "Carrefour | Light\n",
      "6\n",
      "Mercadona | Bifidus\n",
      "6\n",
      "Mercadona | Cholesterol\n",
      "6\n",
      "Mercadona | Essential\n",
      "6\n",
      "Ahorramas | Essential Drink\n",
      "6\n",
      "Ahorramas | Essential Spoon\n",
      "6\n",
      "Ahorramas | Greek Spoon\n",
      "6\n",
      "Ahorramas | Kids Drink\n",
      "6\n",
      "Ahorramas | Kids Spoon\n",
      "6\n",
      "Ahorramas | Light Spoon\n",
      "6\n",
      "Carrefour | Essential Drink\n",
      "6\n",
      "Carrefour | Essential Spoon\n",
      "6\n",
      "Carrefour | Greek Spoon\n",
      "6\n",
      "Carrefour | Kids Drink\n",
      "6\n",
      "Carrefour | Kids Spoon\n",
      "6\n",
      "Carrefour | Light Drink\n",
      "6\n",
      "Carrefour | Light Spoon\n",
      "6\n",
      "Mercadona | Essential Spoon\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# slide 12\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newModifiedBrands,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newModifiedBrands.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoSalesTotalSize(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0b4e2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 52\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2e6331b0-3674-471a-8da0-d10719174ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Shave Men | Walmart\n",
      "Manual Shave Men | Walmart Div1 Corp\n",
      "Manual Shave Men | Walmart Nm Corp\n",
      "Manual Shave Men | Walmart Sc Corp\n",
      "Walmart | System\n",
      "Walmart | Disposables\n",
      "Walmart Div1 Corp | System\n",
      "Walmart Div1 Corp | Disposables\n",
      "Walmart Nm Corp | System\n",
      "Walmart Nm Corp | Disposables\n",
      "Walmart Sc Corp | System\n",
      "Walmart Sc Corp | Disposables\n",
      "Walmart | Disposables\n",
      "Walmart Div1 Corp | Disposables\n",
      "Walmart Nm Corp | Disposables\n",
      "Walmart Sc Corp | Disposables\n",
      "Walmart | Razors\n",
      "Walmart | Refills\n",
      "Walmart Div1 Corp | Razors\n",
      "Walmart Div1 Corp | Refills\n",
      "Walmart Nm Corp | Razors\n",
      "Walmart Nm Corp | Refills\n",
      "Walmart Sc Corp | Razors\n",
      "Walmart Sc Corp | Refills\n"
     ]
    }
   ],
   "source": [
    "# slide 13\n",
    "if promo_type:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(PromoSalesTypes_data,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in PromoSalesTypes_data.items() if key in dict[key1]}\n",
    "            if filtered_dict:\n",
    "                PromoSalesTypes(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5192201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 91\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "555338fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 14\n",
    "if feature_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                featureShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b2ecb5b3-8308-43ec-b4aa-16c2e7b0797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# slide 15\n",
    "if display_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                displayShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f16eefd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 1589\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b5ffadfa-5f94-4967-84f6-54c1d2ab7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 16\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionEndOfWeek,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionEndOfWeek.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoFrequency(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b84ad318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 1892\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43caf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if categories:\n",
    "    catFinal = sorted(splitDfsPromo(dfCategory,(client_manuf) ,genrateIndexList(catGroup[0])[0]))\n",
    "    catFinal = catFinal+sorted(splitDfsPromo(dfCategory,(client_brands) ,genrateIndexList(catGroup[0])[0]))\n",
    "    catFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "56f6fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 1892\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "05117abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sectors:\n",
    "    secFinal = sorted(splitDfsPromo(dfSector,(client_manuf)  ,genrateIndexList(secGroup[0])[0]))\n",
    "    secFinal = secFinal + sorted(splitDfsPromo(dfSector,(client_brands)  ,genrateIndexList(secGroup[0])[0]))\n",
    "    secFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9f5855f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 1892\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c788ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if segments:\n",
    "    segFinal = sorted(splitDfsPromo(dfSegment,(client_manuf)  ,genrateIndexList(segGroup[0])[0]))\n",
    "    segFinal = segFinal+sorted(splitDfsPromo(dfSegment,(client_brands)  ,genrateIndexList(segGroup[0])[0]))\n",
    "    segFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "889d3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if subsegments:\n",
    "    subsegFinal = sorted(splitDfsPromo(dfSubSegment,(client_manuf)  ,genrateIndexList(subsegGroup[0])[0]))\n",
    "    subsegFinal = subsegFinal + sorted(splitDfsPromo(dfSubSegment,(client_brands)  ,genrateIndexList(subsegGroup[0])[0]))\n",
    "    subsegFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f3153b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if subcategories:\n",
    "    subcatFinal = sorted(splitDfsPromo(dfSubCategory,(client_manuf) ,genrateIndexList(subcatGroup[0])[0]))\n",
    "    subcatFinal = subcatFinal+sorted(splitDfsPromo(dfSubCategory,(client_brands) ,genrateIndexList(subcatGroup[0])[0]))\n",
    "    subcatFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "42f04b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 1892\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5febe962-91f9-4b04-80a0-986f63399c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Actimel | Ahorramas | Yogurt', 'Actimel | Carrefour | Yogurt', 'Actimel | Carrefour Hyper | Yogurt', 'Actimel | Carrefour Super | Yogurt'], ['Actimel | Mercadona | Yogurt'], ['Activia | Ahorramas | Yogurt', 'Activia | Carrefour | Yogurt', 'Activia | Carrefour Hyper | Yogurt', 'Activia | Carrefour Super | Yogurt'], ['Activia | Mercadona | Yogurt'], ['Danacol | Ahorramas | Yogurt', 'Danacol | Carrefour | Yogurt', 'Danacol | Carrefour Hyper | Yogurt', 'Danacol | Carrefour Super | Yogurt'], ['Danacol | Mercadona | Yogurt'], ['Danone Griego | Ahorramas | Yogurt', 'Danone Griego | Carrefour | Yogurt', 'Danone Griego | Carrefour Hyper | Yogurt', 'Danone Griego | Carrefour Super | Yogurt'], ['Danone Group | Ahorramas | Yogurt', 'Danone Group | Carrefour | Yogurt', 'Danone Group | Carrefour Hyper | Yogurt', 'Danone Group | Carrefour Super | Yogurt'], ['Danone Group | Mercadona | Yogurt'], ['Danone Original | Ahorramas | Yogurt', 'Danone Original | Carrefour | Yogurt', 'Danone Original | Carrefour Hyper | Yogurt', 'Danone Original | Carrefour Super | Yogurt'], ['Danone | Ahorramas | Yogurt', 'Danone | Carrefour | Yogurt', 'Danone | Carrefour Hyper | Yogurt', 'Danone | Carrefour Super | Yogurt'], ['Danone | Mercadona | Yogurt'], ['Danonino | Ahorramas | Yogurt', 'Danonino | Carrefour | Yogurt', 'Danonino | Carrefour Hyper | Yogurt', 'Danonino | Carrefour Super | Yogurt'], ['Danonino | Mercadona | Yogurt']]\n",
      "24 75 1906\n",
      "42 76 1930\n",
      "25 77 1972\n",
      "0 78 1997\n"
     ]
    }
   ],
   "source": [
    "#Slide 17\n",
    "#split catGroup into Lists depends on num of charts \n",
    "catGroupSplit = splitListpromo(dfCategory, catGroup, [i-14 for i in index[ind]])\n",
    "print(catGroupSplit)\n",
    "promoSalesPerRetailer(prs,dfCategory,len(index[ind]),catGroupSplit,position=sum(duplication[:ind]))\n",
    "posItr = sum(duplication[:ind]) + len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split secGroup into Lists depends on num of charts \n",
    "if len(sectors) != 0: \n",
    "    secGroupSplit = splitListpromo(dfSector, secGroup, [i-14 for i in index[ind]])\n",
    "    promoSalesPerRetailer(prs,dfSector,len(index[ind]),secGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split segGroup into Lists depends on num of charts \n",
    "if len(segments) != 0: \n",
    "    segGroupSplit = splitListpromo(dfSegment, segGroup, [i-14 for i in index[ind]])\n",
    "    promoSalesPerRetailer(prs,dfSegment,len(index[ind]),segGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split subsegGroup into Lists depends on num of charts \n",
    "if len(subsegments) != 0:\n",
    "    subsegGroupSplit = splitListpromo(dfSubSegment, subsegGroup, [i-14 for i in index[ind]])\n",
    "\n",
    "    promoSalesPerRetailer(prs,dfSubSegment,len(index[ind]),subsegGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split subcatGroup into Lists depends on num of charts \n",
    "if len(subcategories) != 0:\n",
    "    subcatGroupSplit = splitListpromo(dfSubCategory, subcatGroup, [i-14 for i in index[ind]])\n",
    "    print(subcatGroupSplit)\n",
    "    promoSalesPerRetailer(prs,dfSubCategory,len(index[ind]),subcatGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4bdf01c1-6ec8-405c-bb46-928310996eb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m filtered_dict \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m modified_valueUplift\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdict\u001b[39m[key1]}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_dict:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mvalueUplift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfiltered_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mduplication\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposItr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m posItr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_dict)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(ind,posItr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25800\\2139450290.py:41\u001b[0m, in \u001b[0;36mvalueUplift\u001b[1;34m(prs, modified_valueUplift, numOfDuplication, position)\u001b[0m\n\u001b[0;32m     39\u001b[0m tables, charts \u001b[38;5;241m=\u001b[39m createTableAndChart(slide\u001b[38;5;241m.\u001b[39mshapes)\n\u001b[0;32m     40\u001b[0m chart1 \u001b[38;5;241m=\u001b[39m charts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchart  \u001b[38;5;66;03m# First chart\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m chart2 \u001b[38;5;241m=\u001b[39m \u001b[43mcharts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mchart  \u001b[38;5;66;03m# Second chart\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Extract data for charts\u001b[39;00m\n\u001b[0;32m     44\u001b[0m category \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprodORitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# slide 21\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_valueUplift,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_valueUplift.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUplift(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        print(ind,posItr)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1d1031c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 1 2172\n"
     ]
    }
   ],
   "source": [
    "print(ind, duplication[ind], posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0e02f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danone Group\n",
      "Danone Group\n",
      "Danone Group\n",
      "Danone Group\n",
      "Danone Group\n",
      "2177 5\n"
     ]
    }
   ],
   "source": [
    "if len(categories)>0:\n",
    "    seasonality(prs,dfCategory0, len(index[ind]), categoryGroup0, position=posItr,slideby=\"Category\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fec4ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahorramas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrefour\n",
      "Carrefour Hyper\n",
      "Carrefour Super\n",
      "Mercadona\n",
      "2182 20\n"
     ]
    }
   ],
   "source": [
    "if len(sectors)>0:\n",
    "    seasonality(prs, dfSector0, len(index[ind]), secGroup0, position=posItr,slideby=\"Sector\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "95c95329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahorramas\n",
      "Ahorramas\n",
      "Ahorramas\n",
      "Ahorramas\n",
      "Carrefour\n",
      "Carrefour\n",
      "Carrefour\n",
      "Carrefour\n",
      "Carrefour Hyper\n",
      "Carrefour Hyper\n",
      "Carrefour Hyper\n",
      "Carrefour Hyper\n",
      "Carrefour Super\n",
      "Carrefour Super\n",
      "Carrefour Super\n",
      "Carrefour Super\n",
      "Mercadona\n",
      "Mercadona\n",
      "Mercadona\n",
      "Mercadona\n",
      "2202 20 87\n"
     ]
    }
   ],
   "source": [
    "if len(segments)>0:\n",
    "    seasonality(prs, dfSegment0, len(index[ind]), segGroup0, position=posItr,slideby=\"Segment\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind]),ind)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cfffb4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahorramas\n",
      "Ahorramas\n",
      "Ahorramas\n",
      "Ahorramas\n",
      "Carrefour\n",
      "Carrefour\n",
      "Carrefour\n",
      "Carrefour\n",
      "Carrefour Hyper\n",
      "Carrefour Hyper\n",
      "Carrefour Hyper\n",
      "Carrefour Hyper\n",
      "Carrefour Super\n",
      "Carrefour Super\n",
      "Carrefour Super\n",
      "Carrefour Super\n",
      "Mercadona\n",
      "Mercadona\n",
      "Mercadona\n",
      "Mercadona\n",
      "2222 0 88\n"
     ]
    }
   ],
   "source": [
    "if len(subsegments) != 0:\n",
    "    seasonality(prs,dfSubSegment0,len(index[ind]),subsegGroup0,position=posItr,slideby=\"SubSegment\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "\n",
    "print(posItr, len(index[ind]),ind)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8d74d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subcategories) != 0:\n",
    "    seasonality(prs,dfSubCategory0,len(index[ind]),subcatGroup0,position=posItr,slideby=\"SubCategory\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d43fdd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 89 1 2222\n"
     ]
    }
   ],
   "source": [
    "print(len(index[ind]), ind, duplication[ind], posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bdb5a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222 89\n"
     ]
    }
   ],
   "source": [
    "print(posItr,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e773cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date1 = pd.to_datetime(end_date)\n",
    "start_date1 = end_date1 - pd.DateOffset(months=12)\n",
    " \n",
    "# Generate all weekly periods (weekly ends, e.g., Sundays) between start and end\n",
    "week_ends = pd.date_range(start=start_date1, end=end_date1, freq='W-SUN')\n",
    " \n",
    "# Convert to list of dates (if needed)\n",
    "week_ends_list = week_ends.to_list()\n",
    "all_weeks_df = pd.DataFrame({'End of Week': week_ends_list})\n",
    "\n",
    "def add_all_weeks(data):\n",
    "    final_data ={}\n",
    "    for key,df in data.items():\n",
    "        df_full = all_weeks_df.merge(df, on='End of Week', how='left')\n",
    "        df_full.fillna(0, inplace=True)\n",
    "        final_data[key] = df_full\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bab704be",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dfCategory1 = add_all_weeks(modified_dfCategory1)\n",
    "modified_dfSector1 = add_all_weeks(modified_dfSector1)\n",
    "modified_dfSegment1 = add_all_weeks(modified_dfSegment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f6b6cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "34 2242\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "58 2276\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "34 2334\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0 2368\n"
     ]
    }
   ],
   "source": [
    "catGroup1Split = splitListpromo(modified_dfCategory1, catGroup1, [i-25 for i in index[ind]])\n",
    "\n",
    "Promotional_Frequency(prs,modified_dfCategory1,len(index[ind]),catGroup1Split,position=posItr)\n",
    "posItr +=len(catGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "#Sector Replace\n",
    "if len(sectors) != 0: \n",
    "    secGroup1Split = splitListpromo(modified_dfSector1, secGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSector1,len(index[ind]),secGroup1Split,position=posItr)\n",
    "    posItr += len(secGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "\n",
    "if len(segments) != 0: \n",
    "    segGroup1Split = splitListpromo(modified_dfSegment1, segGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSegment1,len(index[ind]),segGroup1Split,position=posItr)\n",
    "    posItr += len(segGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "if len(subsegments) != 0:\n",
    "    subsegGroup1Split = splitListpromo(modified_dfSubSegment1, subsegGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSubSegment1,len(index[ind]),subsegGroup1Split,position=posItr)\n",
    "    posItr += len(subsegGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "if len(subcategories) != 0:\n",
    "    subcatGroup1Split = splitListpromo(modified_dfSubCategory1, subcatGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSubCategory1,len(index[ind]),subcatGroup1Split,position=posItr)\n",
    "    posItr += len(subcatGroup1Split)\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c746a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slide 1 with no client brands\n",
    "# for key,value in Scope.items():\n",
    "#     dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "#     for key1,value1 in dict.items():\n",
    "#         filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "#         if filtered_dict:\n",
    "#             promoValueSales_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "#             posItr += len(filtered_dict)\n",
    "#         ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "09d3bb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b39417e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 2 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(promotionsBrandNOTSortedTotalFinal,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in promotionsBrandNOTSortedTotalFinal.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            promoEvolutionNew(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0fb051d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2475\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "42070784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 3 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newpromotionsNotBrandsWithMarket,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newpromotionsNotBrandsWithMarket.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VSOD1(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "61072cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2487\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7e53b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 4 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(concated,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in concated.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUpliftRetailer_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ead65353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "995b3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 11 with no client prio\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoShare_vs_ValueShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8d61cb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "26d694f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 2616\n"
     ]
    }
   ],
   "source": [
    "print(ind, posItr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8cd77382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "9\n",
      "55 6\n",
      "6\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "10\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "8\n",
      "6\n",
      "3\n",
      "6\n",
      "11\n",
      "6\n",
      "12\n",
      "6\n",
      "12\n",
      "66 7\n",
      "6\n",
      "10\n",
      "6\n",
      "6\n",
      "6\n",
      "8\n",
      "6\n",
      "3\n",
      "6\n",
      "12\n",
      "6\n",
      "11\n",
      "6\n",
      "10\n",
      "6\n",
      "6\n",
      "6\n",
      "5\n",
      "6\n",
      "11\n",
      "6\n",
      "5\n",
      "6\n",
      "7\n",
      "6\n",
      "8\n",
      "6\n",
      "8\n",
      "6\n",
      "10\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "6\n",
      "9\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "3\n",
      "6\n",
      "12\n",
      "6\n",
      "12\n",
      "6\n",
      "12\n",
      "90 8\n",
      "6\n",
      "12\n",
      "6\n",
      "7\n",
      "6\n",
      "5\n",
      "6\n",
      "11\n",
      "6\n",
      "8\n",
      "6\n",
      "9\n",
      "6\n",
      "8\n",
      "6\n",
      "3\n",
      "6\n",
      "4\n",
      "6\n",
      "11\n",
      "6\n",
      "3\n",
      "6\n",
      "10\n",
      "6\n",
      "3\n",
      "6\n",
      "12\n",
      "104 9\n",
      "104 10\n"
     ]
    }
   ],
   "source": [
    "# slide 12 with no client prio\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newModifiedBrands,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newModifiedBrands.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoSalesTotalSize_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "    ind +=1\n",
    "    print(posItr,ind)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e142e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 13 with no client prio\n",
    "# if promo_type:\n",
    "#     for key,value in Scope.items():\n",
    "#         dict = {key: count_df(PromoSalesTypes_data,value) }\n",
    "#         for key1,value1 in dict.items():\n",
    "#             filtered_dict = {key: value for key, value in PromoSalesTypes_data.items() if key in dict[key1]}\n",
    "#             if filtered_dict:\n",
    "#                 PromoSalesTypes_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "#             posItr += len(filtered_dict)\n",
    "#             ind +=1\n",
    "# else:\n",
    "#     ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5f3c723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 14 with no client prio\n",
    "if feature_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                featureShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "bfbef8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 15 with no client prio\n",
    "if display_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                displayShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217af932",
   "metadata": {},
   "source": [
    "## Output slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c56e0982-087b-439b-a549-736abdbb54b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 1: Opened Excel workbook: Book1\n",
      "Slide 2: Opened Excel workbook: Book1\n",
      "Slide 3: Opened Excel workbook: Book1\n",
      "Slide 4: Opened Excel workbook: Book1\n",
      "Slide 5: Opened Excel workbook: Book1\n",
      "Slide 6: Opened Excel workbook: Book1\n",
      "Slide 31: Opened Excel workbook: Book1\n",
      "Slide 32: Opened Excel workbook: Book1\n",
      "Slide 33: Opened Excel workbook: Book1\n",
      "Slide 34: Opened Excel workbook: Book1\n",
      "Slide 35: Opened Excel workbook: Book1\n",
      "Slide 36: Opened Excel workbook: Book1\n"
     ]
    }
   ],
   "source": [
    "outputPath=os.getcwd() + f\"\\\\Promotion {client_manuf[0]}.pptx\"\n",
    "prs.save(outputPath)\n",
    "# # app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "final=os.getcwd() +f\"\\\\Promotion {client_manuf[0]}.pptx\"\n",
    "open_chart_data_in_excel(final,outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413d6eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\aleaa\\\\Documents\\\\Slide-Automate\\\\Promotion Slide Duplicate/ValueUpliftvsDepth/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[232], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m datasets_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m path1\n\u001b[1;32m----> 7\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(datasets_path\u001b[38;5;241m+\u001b[39md, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\aleaa\\\\Documents\\\\Slide-Automate\\\\Promotion Slide Duplicate/ValueUpliftvsDepth/'"
     ]
    }
   ],
   "source": [
    "%run \"..\\general_functions\\generalFunctions.ipynb\"\n",
    "%run \"..\\Promotion Slide Duplicate\\Promotion Replacement Function.ipynb\"\n",
    "\n",
    "path1 = r\"/ValueUpliftvsDepth/\"\n",
    "loaded_data = {}\n",
    "datasets_path = os.getcwd()+ path1\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        loaded_data[d.split('.')[0]] = pd.read_csv(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valueUplift_dict = {} # value\n",
    "i=0\n",
    "for key, df in loaded_data.items():\n",
    "    data = DetectHeader(df)\n",
    "    columns_to_ffill = [col for col in data.columns if 'item' in col.lower() or 'product' in col.lower()]\n",
    "    data[columns_to_ffill] = data[columns_to_ffill].fillna(method='ffill')\n",
    "    data = data[~data['Item'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "    for item in data['Item'].unique():\n",
    "        df = data[data['Item'] == item]\n",
    "        df['Discount Depth (%)'] = df['Discount Depth (%)'].str.replace('%','').astype(float) /100\n",
    "        df['Promo Price/Unit'] = df['Promo Price/Unit'].str.replace('£','').astype(float)\n",
    "        if normalized:\n",
    "            df['Value Uplift (v. base) Normalized'] = df['Value Uplift (v. base) Normalized'].str.replace('%','').astype(float) /100\n",
    "        else:\n",
    "            df['Value Uplift (v. base)'] = df['Value Uplift (v. base)'].str.replace().str.replace('%','').astype(float) /100\n",
    "        df = df[df['End of Week'] != '0']\n",
    "        df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "        df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)].reset_index(drop=True)\n",
    "        if df.shape[0]>0 and not df['Discount Depth (%)'].isna().all():\n",
    "            df = df.fillna(0).reset_index(drop = True)\n",
    "            new_key = key+'_'+ item\n",
    "            valueUplift_dict[new_key] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = 'before'\n",
    "decimals = 2\n",
    "currency = '£'\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending July 2024\"\n",
    "\n",
    "\n",
    "index = [20]\n",
    "duplication = [len(valueUplift_dict.keys())]\n",
    "section_names = [\"Value Uplift by product\"]\n",
    "path = os.getcwd() + '//Promotion base Oct 2024.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated value.pptx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation(new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each key-slide_num pair in modified_valueUplift\n",
    "for key, slide_num in zip(valueUplift_dict, range(len(valueUplift_dict.keys()))):\n",
    "        # Access the slide to be modified\n",
    "        slide = prs.slides[slide_num]\n",
    "        \n",
    "        # Extract data for the current key\n",
    "        df = valueUplift_dict[key]\n",
    "        #df = df[df['Value Uplift (v. base) Normalized'] !=0 ]\n",
    "        # Get shapes in the slide\n",
    "        shapes = slide.shapes\n",
    "        \n",
    "        # Find and update title shape\n",
    "        titleNumber = get_shape_number(shapes, \"Value Uplift vs discount depth | By Event | Category/Sector | Brand | Coop Alleanza | P12M\")\n",
    "        datasourcenum = get_shape_number(shapes, \"Data Source | Trade Panel\")\n",
    "        headerNumber = get_shape_number(shapes, 'Value Uplift vs discount depth (Replace With SO WHAT)')\n",
    "        if titleNumber is not None:\n",
    "            shapes[datasourcenum].text = data_source\n",
    "            shapes[titleNumber].text = shapes[titleNumber].text.replace('Category/Sector', key.split('_')[2]) \\\n",
    "                .replace('Brand | Coop Alleanza ', df['Item'][0])\n",
    "            shapes[titleNumber].text_frame.paragraphs[0].font.size = Pt(12)\n",
    "            shapes[titleNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "            shapes[headerNumber].text_frame.paragraphs[0].font.size = Pt(16)\n",
    "            shapes[headerNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "\n",
    "        # Create table and chart objects\n",
    "        tables, charts = createTableAndChart(slide.shapes)\n",
    "        chart1 = charts[0].chart  # First chart\n",
    "        chart2 = charts[1].chart  # Second chart\n",
    "        \n",
    "        # Extract data for charts\n",
    "        category = df['Item'].tolist()\n",
    "        x_values_discount = df['Discount Depth (%)'].tolist()\n",
    "        x_values_price = df['Promo Price/Unit'].tolist()\n",
    "        if normalized:\n",
    "            y_values = df['Value Uplift (v. base) Normalized'].tolist()\n",
    "        else:\n",
    "            y_values = df['Value Uplift (v. base)'].tolist()\n",
    "\n",
    "        \n",
    "        x_values_discount = [mround_numpy(value, 0.05) for value in x_values_discount]\n",
    "        x_values_price = [mround_numpy(value, 0.5) for value in x_values_price]\n",
    "        #Update first chart with Discount Depth vs Value Uplift data\n",
    "        chart_data1 = XyChartData()\n",
    "        series1 = chart_data1.add_series('Scatter')\n",
    "        for i in range(len(category)):\n",
    "            series1.add_data_point(x_values_discount[i], y_values[i])\n",
    "        chart1.replace_data(chart_data1)\n",
    "        \n",
    "        # Access the X-axis\n",
    "        \n",
    "        xlsx_file = BytesIO()\n",
    "        with chart_data1._workbook_writer._open_worksheet(xlsx_file) as (workbook, worksheet):\n",
    "            chart_data1._workbook_writer._populate_worksheet(workbook, worksheet)\n",
    "            worksheet.write(0, 4, \"Item\")\n",
    "            worksheet.write_column(1, 4, df['Item'].to_list(), None)\n",
    "            worksheet.write(0, 5, \"End of Week\")\n",
    "            worksheet.write_column(1, 5, df['End of Week'].to_list(), None)\n",
    "\n",
    "        chart1._workbook.update_from_xlsx_blob(xlsx_file.getvalue())\n",
    "\n",
    "        # Update second chart with Promo Price/Unit vs Value Uplift data\n",
    "        chart_data2 = XyChartData()\n",
    "        series2 = chart_data2.add_series('Scatter')\n",
    "        for i in range(len(category)):\n",
    "            series2.add_data_point(x_values_price[i], y_values[i])\n",
    "        chart2.replace_data(chart_data2)\n",
    "        \n",
    "        x_axis = chart2.category_axis\n",
    "        \n",
    "        # Loop through each X-axis category label and format as currency\n",
    "        if sign.lower() == 'before':\n",
    "            x_axis.tick_labels.number_format = f'\"{currency}\"#,##0.00'  if decimals == 2 else f'\"{currency}\"#,##0'\n",
    "        else:\n",
    "            x_axis.tick_labels.number_format = f'#,##0.00\"{currency}\"'  if decimals == 2 else f'#,##0\"{currency}\"'\n",
    "       \n",
    "        #x_axis.has_major_gridlines = False  # Optional: remove gridlines\n",
    "\n",
    "        xlsx_file = BytesIO()\n",
    "        with chart_data2._workbook_writer._open_worksheet(xlsx_file) as (workbook, worksheet):\n",
    "            chart_data2._workbook_writer._populate_worksheet(workbook, worksheet)\n",
    "            worksheet.write(0, 4, \"Item\")\n",
    "            worksheet.write_column(1, 4, df['Item'].to_list(), None)\n",
    "            worksheet.write(0, 5, \"End of Week\")\n",
    "            worksheet.write_column(1, 5, df['End of Week'].to_list(), None)\n",
    "        chart2._workbook.update_from_xlsx_blob(xlsx_file.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath=os.getcwd() + \"\\\\Promotion EdgeWell ValueUplift.pptx\"\n",
    "prs.save(outputPath)\n",
    "app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "presentation = app.Presentations.Open(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623751f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
