{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a454f5-c4e1-460c-a27d-40aa00dfcac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"..\\general_functions\\generalFunctions.ipynb\"\n",
    "%run \"..\\Promotion Slide Duplicate\\Promotion Replacement Function.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999d128",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b6fbff-4bd3-4c15-8d5d-f3b65886bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_type = True\n",
    "\n",
    "normalized = True\n",
    "national = False \n",
    "\n",
    "display_share = False  # True if Available\n",
    "feature_share = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86ff0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ManufOrTopC =\"Top Companies\" \n",
    "BrandOrTopB = \"Top Brands\"\n",
    "\n",
    "client_manuf = [\"Pbg\"]\n",
    "client_brands = [\"Schick\", \"Equate\", \"Cremo\"]\n",
    " \n",
    "decimals = 2\n",
    "sign = \"Before\"\n",
    "currency = '$'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    " \n",
    " \n",
    "categories = [\"Manual Shave Men\"]\n",
    "sectors = [\"System\",\"Disposables\"]\n",
    "segments = [\"Razors\", \"Refills\", \"Disposables\"]\n",
    "subsegments= []\n",
    "subcategories= []\n",
    " \n",
    "customareas='REGIONS'\n",
    "national = False\n",
    "areas = [\"RETAILER\", f\"{customareas}\"]\n",
    "regions_RET  = [\"Walmart\"]\n",
    "channels_RET = []\n",
    "market_RET = [\"Walmart Div1 Corp\", \"Walmart Nm Corp\", \"Walmart Sc Corp\"]\n",
    "regions_CHAN = []\n",
    "channels_CHAN = []\n",
    "market_CHAN = []\n",
    "regions_CUST = []\n",
    "channels_CUST = []\n",
    "market_CUST = [\"Walmart East\", \"Walmart North\", \"Walmart Southeast\", \"Walmart Southwest\",\t\"Walmart West\"]\n",
    "\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending March  2025\"\n",
    "years = ['2023', '2024','2025']\n",
    "\n",
    "subcatg_parent = \"Segment\"\n",
    "subcatg_parent_list = segments\n",
    "percent = 1000000\n",
    "percentstr=\"'000 000\"\n",
    "\n",
    "\n",
    "\n",
    "# Add one month to the original ending date (YYYY-MM-01)\n",
    "start_date = \"2022-02-01\"\t\n",
    "end_date = \"2025-04-01\"\n",
    "prodORitem = \"SKU\"\n",
    "\n",
    "\n",
    "# Guidline Promo Columns ex :Volume Uplift >>> \"[Measures].[Volume Uplift IYA]\" using filter_dictionary_keys(fieldsNamePosition, 'Volume Upli')\n",
    "#promo_col = ['[Measures].[Straight Discount 10-20 Sales]','[Measures].[Straight Discount 20-30 Sales]', '[Measures].[Straight Discount 30-40 Sales]','[Measures].[Straight Discount 40+ Sales]']\n",
    "promo_col = []\n",
    "selectedBrands = client_brands \n",
    "marketList = regions_RET + channels_RET + market_RET + regions_CHAN + channels_CHAN + market_CHAN \n",
    "notInScope = []\n",
    "OpenEditData=True\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93407ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max_total_size = {\n",
    "#         'Rossmann | Male Shaving': 7.76779931984066,\n",
    "#         'Rossmann | Male Dispo': 9.39896018537874,\n",
    "#         'Rossmann | Male System': 7.07098834397453,\n",
    "#         'Rossmann | Male Blades': 7.66470415775729,\n",
    "#         'Rossmann | Male Razor': 4.18256105457904\n",
    "#         }\n",
    "\n",
    "# custom_colors = [\n",
    "#     RGBColor(91, 159, 153),    # Darker teal\n",
    "#     RGBColor(131, 199, 193),   # Brighter medium teal\n",
    "#     RGBColor(168, 216, 212),   # Original light teal\n",
    "#     RGBColor(198, 236, 232),   # Very light teal\n",
    "#     RGBColor(111, 179, 173),\n",
    "#     RGBColor(121, 189, 183)\n",
    "#]\n",
    "# custom_colors = [\n",
    "#     RGBColor(111, 179, 173),  \n",
    "#     RGBColor(121, 189, 183),  \n",
    "#     RGBColor(168, 216, 212),  \n",
    "#     RGBColor(178, 226, 222), \n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a8ab5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Scope = {\n",
    "    \"Category\": categories,\n",
    "    \"Sector\": sectors,\n",
    "    \"Segment\": segments,\n",
    "    \"Subsegment\": subsegments,\n",
    "    \"Subcategory\": subcategories\n",
    "}\n",
    "suffixes = [\"Category\", \"Sector\", \"Segment\",'SubSegment', 'SubCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0373cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectHeader(df):\n",
    "  # df = df.replace( np.nan, 0)\n",
    "  vals=df.values\n",
    "  vals=list(map(lambda x : all([type(i)==str for i in x ]),vals))\n",
    "  # print(vals)\n",
    "  break_point=0\n",
    "  for i,v in enumerate(vals):\n",
    "    if v:\n",
    "      break_point=i\n",
    "      # print(break_point)\n",
    "      break  \n",
    "  df.columns=df.iloc[break_point]\n",
    "  df=df.iloc[break_point+1:,:]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fd5a3",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461dc096-382d-4c3b-91c1-8610a7cd684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "datasets_path = os.getcwd()+\"/Pbg Datasets/\"\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3163ddce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Manual Shave Men | Walmart', 'Manual Shave Men | Walmart Div1 Corp', 'Manual Shave Men | Walmart Nm Corp', 'Manual Shave Men | Walmart Sc Corp', 'Manual Shave Men | Walmart East', 'Manual Shave Men | Walmart North', 'Manual Shave Men | Walmart Southeast', 'Manual Shave Men | Walmart Southwest', 'Manual Shave Men | Walmart West'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sector_client_VSOD.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b3e2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketList = regions_RET + channels_RET + market_RET + regions_CHAN + channels_CHAN + market_CHAN + regions_CUST + channels_CUST + market_CUST\n",
    "categoryList=categories +sectors+segments+subsegments+subcategories\n",
    "defaults = {\n",
    "    'Manual Shave Men':\t6.69,\n",
    "    'Disposables': 7.98,\n",
    "    'System': 5.03,\n",
    "    'Razors': 2.80,\n",
    "    'Refills': 6.58\n",
    "}\n",
    "diff_market_value = {\n",
    "}\n",
    " \n",
    "def totalsize (lis,defaultdic,diffmarketdic=[],cat1=False) :\n",
    "    if cat1:\n",
    "        max_total_size = {\n",
    "        f\"{category} | {market}\": diff_market_value.get(market.upper(), {}).get(category, defaults[category])\n",
    "        for market in marketList\n",
    "        for category in defaults\n",
    "    }\n",
    "    else:\n",
    "        max_total_size = {\n",
    "        f\"{market} | {category}\": diff_market_value.get(market.upper(), {}).get(category, defaults[category])\n",
    "        for market in marketList\n",
    "        for category in defaults\n",
    "    }    \n",
    "    return max_total_size  \n",
    " \n",
    "max_total_size=totalsize(categoryList,defaults,diff_market_value,cat1=True)\n",
    "       \n",
    " \n",
    "custom_colors = [\n",
    "    RGBColor(91, 159, 153),    # Darker teal\n",
    "    RGBColor(131, 199, 193),   # Brighter medium teal\n",
    "    RGBColor(168, 216, 212),   # Original light teal\n",
    "    RGBColor(198, 236, 232),   # Very light teal\n",
    "    RGBColor(111, 179, 173),\n",
    "    RGBColor(121, 189, 183)\n",
    "]\n",
    "# custom_colors = [\n",
    "#     RGBColor(111, 179, 173),  \n",
    "#     RGBColor(121, 189, 183),  \n",
    "#     RGBColor(168, 216, 212),  \n",
    "#     RGBColor(178, 226, 222),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb1f42",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94d9ab22-4ebf-4b74-8859-4cad1e38a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningData(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "      \n",
    "        df=DetectHeader(data[key])\n",
    "        # Set column names and skip the first row\n",
    "        if  df.columns.isna().any():\n",
    "            continue\n",
    "        # Perform specific cleaning operations based on the DataFrame columns and key\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if f'{BrandOrTopB}' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].fillna(method='ffill')\n",
    "                df[f'{prodORitem}'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df.reset_index(drop=True)\n",
    "            \n",
    "            elif f'{BrandOrTopB}' in df.columns:\n",
    "                df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].fillna(method='ffill')\n",
    "                #df.fillna(0, inplace=True)\n",
    "                if normalized:\n",
    "                    df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])] = df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift Normalized IYA'].isna(), None, df['Value Uplift Normalized IYA'] - 1)\n",
    "                else:\n",
    "                    df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])] = df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift IYA'].isna(), None, df['Value Uplift IYA'] - 1)\n",
    "               \n",
    "                df['VSOD Evaluation vs YA'] = np.where(df['VSOD IYA'].isna(), None, df['VSOD IYA'] - 1)\n",
    "\n",
    "                df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df[~df[f'{BrandOrTopB}'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "\n",
    "       \n",
    "                \n",
    "            elif 'End of Week' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df[f'{prodORitem}'] = df[f'{prodORitem}'].fillna(method='ffill')\n",
    "                if normalized:\n",
    "                    df = df[(df['Value Uplift (v. base) Normalized'] >= 0)]\n",
    "                else:\n",
    "                    df = df[(df['Value Uplift (v. base)'] >= 0)]\n",
    "                df = df[(df['End of Week'].str.contains('2024|2025')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                new_start=(datetime.strptime(end_date, \"%Y-%m-%d\") - relativedelta(months=12)).strftime(\"%Y-%m-%d\")\n",
    "                df = df[(df['End of Week'] >= new_start) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df[f'{prodORitem}'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df = df[df['Promo Sales'] > 1000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                    df =  df[df['Value Uplift (v. base) Normalized']<10]\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                    df = df[df['Value Uplift (v. base']<10]\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-22|-23|-24|Jan-25')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                if 'Sector' == df.columns[1]:\n",
    "                    df[direct_parent[\"Sector\"]].fillna(method='ffill', inplace= True)\n",
    "                    df['Sector'] = df['Sector'].replace(0, np.nan)\n",
    "                    df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                    df['Sector'] = df.apply(lambda row: row[direct_parent[\"Sector\"]] if 'Total' in row[direct_parent[\"Sector\"]] and row[direct_parent[\"Sector\"]] != categories[0] else row['Sector'], axis=1)\n",
    "\n",
    "                elif 'Segment' == df.columns[1]:\n",
    "                    df['Segment'] = df['Segment'].replace(0, np.nan)  \n",
    "                    df[direct_parent[\"Segment\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['Segment'] = df.apply(lambda row: row[direct_parent[\"Segment\"]] if 'Total' in row[direct_parent[\"Segment\"]] and row[direct_parent[\"Segment\"]] != categories[0] else row['Segment'], axis=1)\n",
    "                    df['Segment'].fillna(method='ffill', inplace=True)\n",
    "                elif 'SubSegment' == df.columns[1]:\n",
    "                    df['SubSegment'] = df['SubSegment'].replace(0, np.nan)\n",
    "                    df[direct_parent[\"SubSegment\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['SubSegment'] = df.apply(lambda row: row[direct_parent[\"SubSegment\"]] if 'Total' in row[direct_parent[\"SubSegment\"]] and row[direct_parent[\"SubSegment\"]] != categories[0] else row['SubSegment'], axis=1)\n",
    "                    df['SubSegment'].fillna(method='ffill', inplace=True)\n",
    "                elif 'SubCategory' == df.columns[1]:\n",
    "                    df['SubCategory'] = df['SubCategory'].replace(0, np.nan)\n",
    "                    df[direct_parent[\"SubCategory\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['SubCategory'] = df.apply(lambda row: row[direct_parent[\"SubCategory\"]] if 'Total' in row[direct_parent[\"SubCategory\"]] and row[direct_parent[\"SubCategory\"]] != categories[0] else row['SubCategory'], axis=1)\n",
    "                    df['SubCategory'].fillna(method='ffill', inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "        if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaned_data[modified_key] = df\n",
    "        else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaned_data[key] = df\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df05b63b-986d-473b-9a40-ef7798a932ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningdata_with_grand_total(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaningdata_with_grand_total = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "   \n",
    "        df=DetectHeader(data[key])\n",
    "\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if f'{BrandOrTopB}' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].fillna(method='ffill')\n",
    "                df[f'{prodORitem}'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "            \n",
    "            elif f'{BrandOrTopB}' in df.columns:\n",
    "                df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].fillna(method='ffill')\n",
    "                if normalized:\n",
    "                    df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])] = df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift Normalized IYA'].isna(), None, df['Value Uplift Normalized IYA'] - 1)\n",
    "                else:\n",
    "                    df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])] = df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift IYA'].isna(), None, df['Value Uplift IYA'] - 1)\n",
    "               \n",
    "                df['VSOD Evaluation vs YA'] = np.where(df['VSOD IYA'].isna(), None, df['VSOD IYA'] - 1)\n",
    "                df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                #df = df[~df[f'{BrandOrTopB}'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "\n",
    "            elif 'End of Week' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df[f'{prodORitem}'] = df[f'{prodORitem}'].fillna(method='ffill')\n",
    "                df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df[f'{prodORitem}'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                \n",
    "                df = df[df['Promo Sales'] > 1000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-21|-22|-23|Jan-24')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "            \n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[modified_key] = df\n",
    "            else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[key] = df\n",
    "    \n",
    "    return cleaningdata_with_grand_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc586ee-696d-43bb-b763-ecfe53252dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning13New(data):\n",
    "    \"\"\"\n",
    "    Clean and process data for specific brands and regions.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing raw data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned and processed data.\n",
    "    \"\"\"\n",
    "    data_cleaned = {}\n",
    "    \n",
    "    # Define maximum total size for each combination of product type and region\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        # Skip processing if the region is 'NATIONAL' or 'National'\n",
    "        if 'NATIONAL' in areas or 'National' in key:\n",
    "            continue\n",
    "        new_data = []\n",
    "        # Skip first 12 rows as they are headers and metadata\n",
    "        # df = df.iloc[12:]\n",
    "        df=DetectHeader(data[key])\n",
    "        \n",
    "        # # Set columns names based on the first row, and skip the first row\n",
    "        # df.columns = df.iloc[0]\n",
    "        # df = df.iloc[1:]\n",
    "        # Fill missing values in 'Top Brands' column with the previous non-null value\n",
    "        df[f'{BrandOrTopB}'].fillna(method='ffill', inplace=True)\n",
    "        # Filter out rows where 'Top Brands' is 'Grand Total' or 'Other'\n",
    "        df = df[(df[f'{BrandOrTopB}'] != 'Grand Total') & (df[f'{BrandOrTopB}'] != 'Other')]\n",
    "        # Remove 'GR' suffix from 'Total Size' and convert it to integer\n",
    "        df['Total Size'] = df['Total Size'].str.extract('(\\d+)', expand=False)\n",
    "        df.fillna('0',inplace=True)\n",
    "        df['Total Size'] = df['Total Size'].astype(int)\n",
    "        # Sort data by 'Value Share' in descending order\n",
    "        df = df.sort_values(by='Value Share', ascending=False).reset_index(drop=True)\n",
    "        for i, brand in enumerate(df[f'{BrandOrTopB}'].unique()):\n",
    "            # Determine the product key based on the first two elements of the key\n",
    "            product_key = key.split('|')[0] + '|' + key.split('|')[1]\n",
    "            # Get the maximum total size for the product key, if it exists\n",
    "            max_size = max_total_size.get(product_key, None)\n",
    "            # Filter rows for the current brand and check if total size is within the maximum allowed size\n",
    "            if max_size is not None:\n",
    "                brand_df = df[(df[f'{BrandOrTopB}'] == brand) & (df['Total Size'] <= max_size)]\n",
    "            else:\n",
    "                brand_df = pd.DataFrame()\n",
    "            # Calculate recruitment ratio if the brand has data and total size is greater than zero\n",
    "            #brand_total = df[(df['Top Brands'] == brand + ' Total')]['Promo Value'].values\n",
    "            brand_total = df[(df[f'{BrandOrTopB}'].str.strip() == (brand + ' Total').strip())]['Promo Value'].values\n",
    "\n",
    "            if not brand_df.empty and brand_total.size > 0 and brand_total[0] > 0:\n",
    "                brand_sum = brand_df['Promo Value'].sum() / brand_total[0]\n",
    "                new_data.append({f'{BrandOrTopB}': brand, 'Recruitment': brand_sum, 'Consumption': 1 - brand_sum, 'Value Share': df['Value Share'][i], 'SUM':brand_df['Promo Value'].sum()})\n",
    "        # Create a new DataFrame with cleaned data\n",
    "        new = pd.DataFrame(new_data)\n",
    "        new.fillna(0, inplace=True)\n",
    "        \n",
    "        # Add cleaned data to the dictionary if it contains non-zero rows\n",
    "        if new.shape[0] != 0:\n",
    "            data_cleaned[key] = new\n",
    "        \n",
    "    return data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07630d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def VSOD_Clean(VSOD_Data):\n",
    "#     \"\"\"\n",
    "#     Clean and preprocess VSOD data in a dictionary of DataFrames.\n",
    "\n",
    "#     Parameters:\n",
    "#     - VSOD_Data (dict): Dictionary containing VSOD DataFrames.\n",
    "\n",
    "#     Returns:\n",
    "#     - dict: Dictionary containing cleaned VSOD DataFrames.\n",
    "#     \"\"\"\n",
    "#     VSOD_cleaned = {}\n",
    "#     for key in VSOD_Data:\n",
    " \n",
    "#         df=DetectHeader(VSOD_Data[key])\n",
    "#         # Fill NaN values with 0\n",
    "#         # print(key)\n",
    "#         if 'Sector' == df.columns[1]:\n",
    "#             df[direct_parent[\"Sector\"]] = df[direct_parent[\"Sector\"]].replace(0, np.nan)\n",
    "#             df[direct_parent[\"Sector\"]].fillna(method='ffill', inplace = True)\n",
    "#             df['Sector'] = df['Sector'].replace(0, np.nan)\n",
    "#             df['Sector'] = df.apply(lambda row: row[direct_parent[\"Sector\"]] if 'Total' in row[direct_parent[\"Sector\"]] and row[direct_parent[\"Sector\"]] != categories[0] else row['Sector'], axis=1)\n",
    "#             df = df[~(df[direct_parent[\"Sector\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "#                     (df[direct_parent[\"Sector\"]] != categories[0])) | \n",
    "#                     df[direct_parent[\"Sector\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "#             df['Sector'].fillna(method='ffill', inplace=True)\n",
    "#             df.fillna(0, inplace=True)\n",
    "#         df = df.reset_index(drop=True)\n",
    "#         if df.shape[0] > 0:\n",
    "#             VSOD_cleaned[key] = df\n",
    "#     return VSOD_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd7a1182-ecdd-40a8-8d9e-a110ad873ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VSOD_Clean(VSOD_Data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess VSOD data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - VSOD_Data (dict): Dictionary containing VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned VSOD DataFrames.\n",
    "    \"\"\"\n",
    "    VSOD_cleaned = {}\n",
    "    for key in VSOD_Data:\n",
    " \n",
    "        df=DetectHeader(VSOD_Data[key])\n",
    "        # Fill NaN values with 0\n",
    "        # print(key)\n",
    "        if 'Sector' == df.columns[1]:\n",
    "            df[direct_parent[\"Sector\"]] = df[direct_parent[\"Sector\"]].replace(0, np.nan)\n",
    "            df[direct_parent[\"Sector\"]].fillna(method='ffill', inplace = True)\n",
    "            df['Sector'] = df['Sector'].replace(0, np.nan)\n",
    "            df['Sector'] = df.apply(lambda row: row[direct_parent[\"Sector\"]] if 'Total' in row[direct_parent[\"Sector\"]] and row[direct_parent[\"Sector\"]] != categories[0] else row['Sector'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"Sector\"]].str.contains(r'\\bGrand\\b', case=False))].reset_index(drop=True)\n",
    "            df['Sector'].fillna(method='ffill', inplace=True)\n",
    "            \n",
    "        elif 'Segment' == df.columns[1]:\n",
    "            df[direct_parent[\"Segment\"]].fillna(method='ffill', inplace=True)\n",
    "            df['Segment'] = df['Segment'].replace(0, np.nan)\n",
    "            # df=df[~df['Sector'].str.contains(r'\\btotal\\b', case=False) | df['Sector'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['Segment'] = df.apply(lambda row: row[direct_parent[\"Segment\"]] if 'Total' in row[direct_parent[\"Segment\"]] and row[direct_parent[\"Segment\"]] != categories[0] else row['Segment'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"Segment\"]].str.contains(r'\\bGrand\\b', case=False))].reset_index(drop=True)\n",
    "            df['Segment'].fillna(method='ffill', inplace=True)    \n",
    "        elif 'SubSegment' == df.columns[1]:\n",
    "            df['SubSegment'] = df['SubSegment'].replace(0, np.nan)\n",
    "            df[direct_parent[\"SubSegment\"]].fillna(method='ffill', inplace=True)\n",
    "            # df=df[~df['Segment'].str.contains(r'\\btotal\\b', case=False) | df['Segment'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubSegment'] = df.apply(lambda row: row[direct_parent[\"SubSegment\"]] if 'Total' in row[direct_parent[\"SubSegment\"]] and row[direct_parent[\"SubSegment\"]] != categories[0] else row['SubSegment'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"SubSegment\"]].str.contains(r'\\bGrand\\b', case=False))].reset_index(drop=True)\n",
    "            df['SubSegment'].fillna(method='ffill', inplace=True)\n",
    "        elif 'SubCategory' == df.columns[1]:\n",
    "            df['SubCategory'] = df['SubCategory'].replace(0, np.nan)\n",
    "            df[direct_parent[\"SubCategory\"]].fillna(method='ffill', inplace=True)\n",
    "            # df=df[~df['Segment'].str.contains(r'\\btotal\\b', case=False) | df['Segment'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubCategory'] = df.apply(lambda row: row[direct_parent[\"SubCategory\"]] if 'Total' in row[direct_parent[\"SubCategory\"]] and row[direct_parent[\"SubCategory\"]] != categories[0] else row['SubCategory'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"SubCategory\"]].str.contains(r'\\bGrand\\b', case=False))].reset_index(drop=True)\n",
    "            df['SubCategory'].fillna(method='ffill', inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] > 0:\n",
    "            VSOD_cleaned[key] = df\n",
    "    return VSOD_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d32522c1-88ce-4f84-b71c-4e777b408673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(VSODClient_Cleaned, VSODCleaned, col):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of DataFrames based on a common column.\n",
    "\n",
    "    Parameters:\n",
    "    - VSODClient_Cleaned (dict): Dictionary containing cleaned VSOD client DataFrames.\n",
    "    - VSODCleaned (dict): Dictionary containing cleaned VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing merged DataFrames.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for key in VSODClient_Cleaned:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        #merged_df = pd.merge(VSODCleaned[key], VSODClient_Cleaned[key], on=col, how='left')\n",
    "        merged_df = pd.merge(VSODClient_Cleaned[key],VSODCleaned[key], on=col, how='left')\n",
    "        #merged_df = merged_df.dropna(subset=['Grand Total'])\n",
    "        merged_df['Grand Total'] = merged_df['Grand Total'].fillna(0)\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            merged_dict[key] = merged_df     \n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7caef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified_promotionBrandsP12M_display = cleaningData(display_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1c2b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionBrandsP12M = cleaningData(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77d32fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionBrandsP12M_total = cleaningdata_with_grand_total(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ad3da6e-fb47-4873-b875-bd146d89d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionProductsP12M = cleaningData(promotions_products_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5159e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionProductsP12M_updated = {}\n",
    "for key, df in modified_promotionProductsP12M.items():\n",
    "    df = df.copy()\n",
    "    df = df[df[f'{prodORitem}'] != '']\n",
    "    df = df[df['Promo Sales'] >= 10000]\n",
    "    df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "    if not df.empty:\n",
    "        modified_promotionProductsP12M_updated[key] = df\n",
    "modified_promotionProductsP12M_volumeuplift = modified_promotionProductsP12M_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b40a1e66-80f4-4dd0-9af8-a1a4e1cdc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionEndOfWeek = cleaningData(promotions_EndOfWeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c86f6a2a-90f7-40b1-8f9f-723f5578455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_valueUplift = cleaningData(value_uplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3253f1fa-b573-493b-b3c6-d825cee538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning13New(data):\n",
    "    \"\"\"\n",
    "    Clean and process data for specific brands and regions.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing raw data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned and processed data.\n",
    "    \"\"\"\n",
    "    data_cleaned = {}\n",
    "    \n",
    "    # Define maximum total size for each combination of product type and region\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        # Skip processing if the region is 'NATIONAL' or 'National'\n",
    "        if 'NATIONAL' in areas or 'National' in key:\n",
    "            continue\n",
    "        \n",
    "        new_data = []\n",
    "        \n",
    "        # Skip first 12 rows as they are headers and metadata\n",
    "        # df = df.iloc[12:]\n",
    "        df=DetectHeader(data[key])\n",
    "        \n",
    "        # # Set columns names based on the first row, and skip the first row\n",
    "        # df.columns = df.iloc[0]\n",
    "        # df = df.iloc[1:]\n",
    "        \n",
    "        # Fill missing values in 'Top Brands' column with the previous non-null value\n",
    "        df[f'{BrandOrTopB}'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "        # Filter out rows where 'Top Brands' is 'Grand Total' or 'Other'\n",
    "        df = df[(df[f'{BrandOrTopB}'] != 'Grand Total') & (df[f'{BrandOrTopB}'] != 'Other')]\n",
    "        # Remove 'GR' suffix from 'Total Size' and convert it to integer\n",
    "        df['Total Size'] = df['Total Size'].str.extract('(\\d+)', expand=False)\n",
    "        df.fillna('0',inplace=True)\n",
    "        df['Total Size'] = df['Total Size'].astype(int)\n",
    "        # Sort data by 'Value Share' in descending order\n",
    "        df = df.sort_values(by='Value Share', ascending=False).reset_index(drop=True)\n",
    "        for i, brand in enumerate(df[f'{BrandOrTopB}'].unique()):\n",
    "            # Split and strip both parts of the key\n",
    "            part1 = key.split('|')[0].strip()\n",
    "            part2 = key.split('|')[1].strip()\n",
    "\n",
    "            # Determine the product key with cleaned parts\n",
    "            if part1 == categories[0]:\n",
    "                product_key = f\"{part2} | {part1}\"\n",
    "            else:\n",
    "                product_key = f\"{part1} | {part2}\"\n",
    "\n",
    "            # Get the maximum total size for the product key, if it exists\n",
    "            max_size = max_total_size.get(product_key, None)\n",
    "            # print(product_key, max_size)\n",
    "\n",
    "            # print(product_key)\n",
    "            # Get the maximum total size for the product key, if it exists\n",
    "            # max_size = max_total_size.get(product_key, None)\n",
    "            # print(product_key,max_size)\n",
    "            # Filter rows for the current brand and check if total size is within the maximum allowed size\n",
    "            if max_size is not None:\n",
    "                \n",
    "                brand_df = df[(df[f'{BrandOrTopB}'] == brand) & (df['Total Size'] <= max_size)]\n",
    "            else:\n",
    "                brand_df = pd.DataFrame()\n",
    "                \n",
    "            # Calculate recruitment ratio if the brand has data and total size is greater than zero\n",
    "            #brand_total = df[(df['Top Brands'] == brand + ' Total')]['Promo Value'].values\n",
    "            brand_total = df[(df[f'{BrandOrTopB}'].str.strip() == (brand + ' Total').strip())]['Promo Value'].values\n",
    "\n",
    "            if not brand_df.empty and brand_total.size > 0 and brand_total[0] > 0:\n",
    "                brand_sum = brand_df['Promo Value'].sum() / brand_total[0]\n",
    "                new_data.append({f'{BrandOrTopB}': brand, 'Recruitment': brand_sum, 'Consumption': 1 - brand_sum, 'Value Share': df['Value Share'][i], 'SUM':brand_df['Promo Value'].sum()})\n",
    "        \n",
    "        # Create a new DataFrame with cleaned data\n",
    "        new = pd.DataFrame(new_data)\n",
    "        new.fillna(0, inplace=True)\n",
    "        \n",
    "        # Add cleaned data to the dictionary if it contains non-zero rows\n",
    "        if new.shape[0] != 0:\n",
    "            data_cleaned[key] = new\n",
    "        \n",
    "    return data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1997b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newModifiedBrands = cleaning13New(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "586375bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(VSODClient_Cleaned, VSODCleaned, col):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of DataFrames based on a common column.\n",
    "\n",
    "    Parameters:\n",
    "    - VSODClient_Cleaned (dict): Dictionary containing cleaned VSOD client DataFrames.\n",
    "    - VSODCleaned (dict): Dictionary containing cleaned VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing merged DataFrames.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for key in VSODClient_Cleaned:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        #merged_df = pd.merge(VSODCleaned[key], VSODClient_Cleaned[key], on=col, how='left')\n",
    "        merged_df = pd.merge(VSODClient_Cleaned[key],VSODCleaned[key], on=col, how='left')\n",
    "        #merged_df = merged_df.dropna(subset=['Grand Total'])\n",
    "        merged_df['Grand Total'] = merged_df['Grand Total'].fillna(0)\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            merged_dict[key] = merged_df     \n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5376db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors) >0:\n",
    "    a = VSOD_Clean(Sector_VSOD)\n",
    "    if len(Sector_client_VSOD) >0:\n",
    "        b = cleaningData(Sector_client_VSOD)\n",
    "        sect_vsod_merged = merging(b,a, col=[direct_parent[\"Sector\"],'Sector'])\n",
    "    else:\n",
    "        sect_vsod_merged = a\n",
    "    c = cleaningData(Sector_manuf_VSOD)\n",
    "    for key in sect_vsod_merged:\n",
    "        merged_df = pd.merge(sect_vsod_merged[key], c[key], on=[direct_parent[\"Sector\"],'Sector'], how='left')\n",
    "        if merged_df.shape[0]>0:\n",
    "            sect_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(segments) >0:\n",
    "    a = VSOD_Clean(Segment_VSOD)\n",
    "    if len(Segment_client_VSOD) > 0:\n",
    "        b = cleaningData(Segment_client_VSOD)\n",
    "        seg_vsod_merged = merging(a,b, col=[direct_parent[\"Segment\"],'Segment'])\n",
    "    else:\n",
    "        seg_vsod_merged = a\n",
    "    \n",
    "    c = cleaningData(Segment_manuf_VSOD)\n",
    "    for key in seg_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(seg_vsod_merged[key], c[key], on=[direct_parent[\"Segment\"],'Segment'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            seg_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(subsegments) >0:\n",
    "    a = VSOD_Clean(SubSegment_VSOD)\n",
    "    if len(SubSegment_client_VSOD) > 0 :\n",
    "        b = cleaningData(SubSegment_client_VSOD)\n",
    "        subseg_vsod_merged = merging(a,b, col=[direct_parent[\"SubSegment\"],'SubSegment'])\n",
    "    else:\n",
    "        subseg_vsod_merged = a\n",
    "    c = cleaningData(SubSegment_manuf_VSOD)\n",
    "    for key in subseg_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(subseg_vsod_merged[key], c[key], on=[direct_parent[\"SubSegment\"],'SubSegment'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            subseg_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(subcategories) >0:\n",
    "    a = VSOD_Clean(SubCategory_VSOD)\n",
    "    # print(a)\n",
    "    if len(SubCategory_client_VSOD) > 0 :\n",
    "        b = cleaningData(SubCategory_client_VSOD)\n",
    "        subcat_vsod_merged = merging(a,b, col=[direct_parent[\"SubCategory\"],'SubCategory'])\n",
    "        print(subcat_vsod_merged)\n",
    "    else:\n",
    "        subcat_vsod_merged = a\n",
    "    c = cleaningData(SubCategory_manuf_VSOD)\n",
    "    for key in subcat_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(subcat_vsod_merged[key], c[key], on=[direct_parent[\"SubCategory\"],'SubCategory'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            subcat_vsod_merged[key] = merged_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f75fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitkeys(dic, lis, parent,clientlist):\n",
    "    \"\"\"\n",
    "    Splits the keys of a dictionary into new keys based on unique values in a specified column.\n",
    "    Parameters:\n",
    "    dic (dict): The input dictionary with DataFrames as values.\n",
    "    lis (list): A list of sector names to filter by (if needed).\n",
    "    parent (str): The column name used for splitting (e.g., 'Sector').\n",
    "    \n",
    "    Returns:\n",
    "    dict: A new dictionary with updated keys and filtered DataFrames.\n",
    "    \"\"\"\n",
    "    splitvsod = {}\n",
    "    for key in dic.keys():\n",
    "        for key in dic.keys():\n",
    "            s = dic[key].copy()        \n",
    "            for value in s[parent].unique():\n",
    "                if isinstance(value, str) and not value.endswith(\"Total\"):\n",
    "                    new_key = f\"{key} | {value}\" \n",
    "                    filtered_df = s[s[parent].isin([value, f\"{value} Total\"])] \n",
    "                    \n",
    "                    for cli in clientlist:\n",
    "                        needed_col = [filtered_df.columns[0],filtered_df.columns[1],\"VSOD\",cli] \n",
    "                       \n",
    "                        existing_cols = [col for col in needed_col if col in filtered_df.columns]\n",
    "                        if existing_cols:  # Only try to filter if there are valid columns\n",
    "                            filtered_dfnew = filtered_df[existing_cols]\n",
    "                            splitvsod[new_key + \" | \" + cli] = filtered_dfnew\n",
    "                        else:\n",
    "                            print(f\"Warning: None of the columns in {needed_col} found in DataFrame.\")\n",
    "\n",
    "    keys_to_remove = [\n",
    "        k for k in splitvsod.keys() \n",
    "        if k.split(\" | \")[-2] not in lis\n",
    "    ]\n",
    "    for k in keys_to_remove:\n",
    "        del splitvsod[k]\n",
    "    return splitvsod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e97a11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=client_brands+client_manuf\n",
    "if len(sectors)!=0:\n",
    "    sect_vsod_merged=splitkeys(sect_vsod_merged,categories,parent=direct_parent['Sector'],clientlist=client)\n",
    "if len(segments)!=0:\n",
    "    seg_vsod_merged=splitkeys(seg_vsod_merged,sectors,parent=direct_parent['Segment'],clientlist=client)\n",
    "if len(subsegments)!=0:\n",
    "    subseg_vsod_merged=splitkeys(subseg_vsod_merged,segments,parent=direct_parent['SubSegment'],clientlist=client)\n",
    "if len(subcategories)!=0:\n",
    "    subcat_vsod_merged=splitkeys(subcat_vsod_merged,segments,parent=direct_parent['SubCategory'],clientlist=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b271ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors):\n",
    "    sect_vsod_count =0\n",
    "    for key,df in sect_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                sect_vsod_count +=1\n",
    "    sect_vsod_count = sect_vsod_count *len(categories)\n",
    " \n",
    "if len(segments):\n",
    "    seg_vsod_count =0\n",
    "    for key,df in seg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                seg_vsod_count +=1\n",
    "    #seg_vsod_count = seg_vsod_count * len(sectors) \n",
    "    seg_vsod_count = seg_vsod_count           \n",
    " \n",
    "if len(subsegments) >0:\n",
    "    subseg_vsod_count =0\n",
    "    for key,df in subseg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subseg_vsod_count +=1\n",
    "    #subseg_vsod_count =subseg_vsod_count *len(segments)\n",
    "    subseg_vsod_count = subseg_vsod_count\n",
    " \n",
    "if len(subcategories) >0:\n",
    "    subcat_vsod_count =0\n",
    "    for key,df in subcat_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subcat_vsod_count +=1\n",
    "    #subcat_vsod_count = subcat_vsod_count * len(subsegments)\n",
    "    subcat_vsod_count = subcat_vsod_count\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dd04ba3-5570-4ad4-8d61-252872480a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandSortedTotalFinal = {}\n",
    "promotionsBrandSortedTotal = dfSort(modified_promotionBrandsP12M, client_brands, f\"{BrandOrTopB}\", num=8, salesCol='Promo Value')\n",
    "\n",
    "for key, df in promotionsBrandSortedTotal.items():\n",
    "    # Step 1: Get client brands first\n",
    "    df_client = selectClientBrands(df, f'{BrandOrTopB}', 'Promo Value')\n",
    "    df_remaining = df[~df[f'{BrandOrTopB}'].isin(client_brands)]\n",
    "\n",
    "    # Step 2: Combine client brands and top non-client brands\n",
    "    number_of_brands_needed = max(6 - len(df_client), 0)\n",
    "    df_top = df_remaining.sort_values(by='Promo Value', ascending=False).head(number_of_brands_needed)\n",
    "    df_combined = pd.concat([df_client, df_top], ignore_index=True)\n",
    "    \n",
    "    # Step 3: Apply filtering\n",
    "    df_combined = df_combined[~df_combined[f'{BrandOrTopB}'].str.contains('Others', case=False, na=False)]\n",
    "    df_combined = df_combined[~df_combined[f'{BrandOrTopB}'].str.contains('Grand Total', case=False, na=False)]\n",
    "    df_combined = df_combined[df_combined['Value Share'] > 0.01]\n",
    "\n",
    "    # Step 4: If fewer than 6 brands remain, add more from df_remaining\n",
    "    existing_brands = df_combined[f'{BrandOrTopB}'].tolist()\n",
    "    if df_combined.shape[0] < 6:\n",
    "        df_filler = df_remaining[~df_remaining[f'{BrandOrTopB}'].isin(existing_brands)]\n",
    "        df_filler = df_filler[~df_filler[f'{BrandOrTopB}'].str.contains('Others|Grand Total', case=False, na=False)]\n",
    "        df_filler = df_filler[df_filler['Value Share'] > 0.01]\n",
    "        df_filler = df_filler.sort_values(by='Promo Value', ascending=False).head(6 - df_combined.shape[0])\n",
    "        df_combined = pd.concat([df_combined, df_filler], ignore_index=True)\n",
    "\n",
    "    # Step 5: Sort final list\n",
    "    df_combined = df_combined.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Step 6: Cast types and store result if valid\n",
    "    if df_combined.shape[0] > 0:\n",
    "        df_combined['VSOD Evaluation vs YA'] = df_combined['VSOD Evaluation vs YA'].astype(float)\n",
    "        df_combined['Promo Value Uplift vs YA'] = df_combined['Promo Value Uplift vs YA'].astype(float)\n",
    "        promotionsBrandSortedTotalFinal[key] = df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e66555e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandNOTSortedTotalFinal={}\n",
    "promotionsBrandNOTSortedTotalFinal=dfSort(modified_promotionBrandsP12M, client_brands, f\"{BrandOrTopB}\", num=8,salesCol='Promo Value')\n",
    "for key,df in modified_promotionBrandsP12M.items():\n",
    "     df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "     df = df[~df[f'{BrandOrTopB}'].str.contains('Others', case=False)]\n",
    "     df = df[~df[f'{BrandOrTopB}'].str.contains('Grand Total', case=False)]\n",
    "     df = df[df['Value Share'] > 0.01]\n",
    "     df['VSOD Evaluation vs YA'] = df['VSOD Evaluation vs YA'].astype(float)\n",
    "     df['Promo Value Uplift vs YA'] = df['Promo Value Uplift vs YA'].astype(float)\n",
    "     if df.shape[0] >0:\n",
    "          promotionsBrandNOTSortedTotalFinal[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75e0e42f-2500-4d43-9ed2-2b5ac567a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands_og = selectedBrands\n",
    "selectedBrands= selectedBrands + [\"Grand Total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab470824-4116-42ea-b87c-150650a1d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsSelected={key:modified_promotionBrandsP12M_total[key][modified_promotionBrandsP12M_total[key][f'{BrandOrTopB}'].isin(selectedBrands)].sort_values(by='Promo Value',ascending=False) for key in modified_promotionBrandsP12M_total.keys()   if all(cat != key.split(' | ')[0] for cat in categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5deb8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in promotionsBrandsSelected:\n",
    "    # Identify the Grand Total row (adjust this if necessary to match your data)\n",
    "    grand_total_row = promotionsBrandsSelected[key].loc[promotionsBrandsSelected[key][f'{BrandOrTopB}'] == 'Grand Total']\n",
    "    # Remove the Grand Total row from the dataframe\n",
    "    sorted_df = promotionsBrandsSelected[key].loc[promotionsBrandsSelected[key][f'{BrandOrTopB}'] != 'Grand Total']\n",
    "    # Concatenate the Grand Total row to the top of the dataframe\n",
    "    promotionsBrandsSelected[key] = pd.concat([grand_total_row, sorted_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea88f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands = selectedBrands_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d4a6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not including client brands\n",
    "promotionsNotBrandsSelected = {\n",
    "    key: modified_promotionBrandsP12M_total[key][\n",
    "        ~modified_promotionBrandsP12M_total[key][f'{BrandOrTopB}'].isin(selectedBrands)\n",
    "    ].sort_values(by='Value Share', ascending=False)\n",
    "    for key in modified_promotionBrandsP12M_total.keys()\n",
    "    if all(cat != key.split(' | ')[0] for cat in categories)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eabcdd35-a8e6-4667-8efe-17c5dea262ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAttribute(dic, marketList):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames and a list of markets, and concatenates\n",
    "    the DataFrames by adding a 'SOURCE' column to each DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are strings in the format 'market | source', and\n",
    "                values are DataFrames containing market data.\n",
    "    marketList (list): A list of market names (strings).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are market names and values are concatenated DataFrames\n",
    "          with an added 'SOURCE' column.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict to store the resulting DataFrames\n",
    "    marketDic = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the list of markets\n",
    "    for market in marketList:\n",
    "        # Iterate through the items in the dictionary\n",
    "        for key, value in dic.items():\n",
    "            # Check if the market name matches the key's market part\n",
    "            if market == key.split(' | ')[0]:\n",
    "                # Extract the source part from the key and assign it to the 'SOURCE' column\n",
    "                value['SOURCE'] = list(set(key.split(' | ')) - set([market]))[0]\n",
    "                value = value[value['Value Share'] >0.01]\n",
    "                value = value[~value[f'{BrandOrTopB}'].str.contains('Other')].reset_index(drop=True)\n",
    "            \n",
    "                # Only include rows where 'SOURCE' is not 'National'\n",
    "                if (value['SOURCE'] != 'National').all():\n",
    "                    marketDic[market].append(value)\n",
    "\n",
    "        # Concatenate all DataFrames in the list for each market\n",
    "        if len(marketDic[market]) != 0:\n",
    "            marketDic[market] = pd.concat(marketDic[market])\n",
    "    \n",
    "    return marketDic\n",
    "\n",
    "def fillingMissingBrands(dic):\n",
    "    \"\"\"\n",
    "    This function fills in missing brands for each market and source combination in the\n",
    "    provided dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are market names and values are DataFrames\n",
    "                containing market data with 'Top Brands' and 'SOURCE' columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: The input dictionary with missing brands filled in each DataFrame.\n",
    "    \"\"\"\n",
    "    # Iterate through the dictionary items\n",
    "    for key, value in dic.items():\n",
    "        # Get the unique list of top brands in the DataFrame\n",
    "        brandList = value[f'{BrandOrTopB}'].unique().tolist()\n",
    "        # Iterate through the unique sources in the DataFrame\n",
    "        for source in value['SOURCE'].unique():\n",
    "            # Check if the number of unique brands for the source is less than the total unique brands\n",
    "            if value[value['SOURCE'] == source][f'{BrandOrTopB}'].nunique() != len(brandList):\n",
    "                # Find the missing brands for the source\n",
    "                missingBrand = list(set(brandList) - set(value[value['SOURCE'] == source][f'{BrandOrTopB}'].unique()))\n",
    "                # Create a DataFrame for the missing brands with the current source\n",
    "                missingBrand = pd.DataFrame({f'{BrandOrTopB}': missingBrand, 'SOURCE': source}).explode(f'{BrandOrTopB}')\n",
    "                # Concatenate the missing brands DataFrame with the original DataFrame\n",
    "                value = pd.concat([value, missingBrand]).replace(np.nan, 0).reset_index(drop=True)\n",
    "        # Update the dictionary with the filled DataFrame\n",
    "        dic[key] = value\n",
    "    \n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5825f852-d5a8-4717-863c-60bce7831474",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsWithMarket=concatAttribute(promotionsBrandsSelected,marketList)\n",
    "promotionsBrandsWithMarket = fillingMissingBrands(promotionsBrandsWithMarket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "993b4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsNotBrandsWithMarket=concatAttribute(promotionsNotBrandsSelected,marketList)\n",
    "promotionsNotBrandsWithMarket = fillingMissingBrands(promotionsNotBrandsWithMarket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "762fb5e2-0b4d-442f-9531-040980af3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_market(data, Scope):\n",
    "    final = {}\n",
    "    for k, df in data.items():\n",
    "        for key, value in Scope.items():\n",
    "            for v in value:\n",
    "                new_key = k + ' | ' + v\n",
    "                if new_key not in final:\n",
    "                    df_market = df[df['SOURCE'].isin(value)]\n",
    "                    if df_market.shape[0] > 0:\n",
    "                        final[new_key] = df_market\n",
    "                    break  # stop checking once we use one valid value\n",
    "    return final    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2a003a6-6da6-495c-a2d9-a9b0af5a7a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newpromotionsBrandsWithMarket = split_market(promotionsBrandsWithMarket,Scope)\n",
    "newpromotionsNotBrandsWithMarket = split_market(promotionsNotBrandsWithMarket,Scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e045f79-110e-4cb8-b237-b514151b9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def concatAttributeNew(sorted):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames sorted by keys and concatenates the DataFrames\n",
    "    based on specified categories, sectors, and segments. It adds a 'SOURCE' column to each DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    sorted (dict): A dictionary where keys are strings in the format 'category | sector | segment | brand',\n",
    "                   and values are DataFrames containing market data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are the categories, sectors, and segments, and values are concatenated DataFrames\n",
    "          with an added 'SOURCE' column.\n",
    "    \"\"\"\n",
    "    # Create a unique list of all category/sector/segment/subsegment/subcategory values\n",
    "    all_keys = list(dict.fromkeys(categories + sectors + segments + subsegments + subcategories))\n",
    "    # all_keys = all_keys[:-1]  # Exclude last if needed (e.g., due to Edgewell issue)\n",
    "\n",
    "    marketDic = defaultdict(list)\n",
    "    concatenatedDic = {}\n",
    "\n",
    "    for i in all_keys:\n",
    "        if i in concatenatedDic:\n",
    "            continue  # already handled this key\n",
    "\n",
    "        for key, value in sorted.items():\n",
    "            if i in key:\n",
    "                parts = key.split(' | ')\n",
    "                # Decide which part of the key to use as SOURCE\n",
    "                markets = parts[1] if i in categories else parts[0]\n",
    "\n",
    "                # Copy and modify to avoid changing original data\n",
    "                df = value.copy()\n",
    "                df['SOURCE'] = markets\n",
    "                marketDic[i].append(df)\n",
    "\n",
    "        if marketDic[i]:\n",
    "            concatenatedDic[i] = pd.concat(marketDic[i], ignore_index=True)\n",
    "\n",
    "    return concatenatedDic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28e29be2-2f65-4603-8858-1087e008b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = concatAttributeNew(modified_promotionBrandsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3ee231f-9b67-4eef-ad0a-ae6e7d75eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        df = modified_promotionProductsP12M[key]\n",
    "        df = df[df[f'{prodORitem}'] != '']\n",
    "        df=df.sort_values(by=['Promo Share'], ascending=False)\n",
    "        # Filter and sort the DataFrame\n",
    "        df['cumulative promo share'] = df['Promo Share'].cumsum()\n",
    "        df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "        df = df[df['VSOD'] >= 0.05]\n",
    "        df = df[df['cumulative promo share'] <= 0.8]\n",
    "        df = df.sort_values(by='Incr Value', ascending=False).reset_index(drop=True)\n",
    "        df = df.head(50)\n",
    "        df['index'] = str(df.index + 1)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] >0:\n",
    "            cleaned_data[key] = df\n",
    "        #else:\n",
    "            #print(key)\n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b8c6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_modified_promotionProductsP12M = filter_data(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3df8ceaa-7083-4015-8ccb-6202d9ac4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_Top(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        combined_df = pd.DataFrame() \n",
    "        for client in client_brands:\n",
    "            \n",
    "            df = modified_promotionProductsP12M[key]\n",
    "            # Filter the DataFrame for the current client brand\n",
    "            df = df[df[f'{prodORitem}'] != '']\n",
    "            df = df[df[f'{BrandOrTopB}'] == client]\n",
    "            df = df.sort_values(by=f'{BrandOrTopB}')\n",
    "            df['Promo Share'] = pd.to_numeric(df['Promo Share'], errors='coerce')\n",
    "            df['cumulative promo share'] = df.groupby(f'{BrandOrTopB}')['Promo Share'].cumsum()\n",
    "            df = df[df['cumulative promo share'] <= 0.8]\n",
    "\n",
    "            df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "            df = df[df['VSOD'] >= 0.05]\n",
    "            if df.shape[0] >0:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "                combined_df = combined_df.sort_values(by='Incr Value', ascending=False).head(20).reset_index(drop=True)\n",
    "        if combined_df.shape[0] > 0:\n",
    "            cleaned_data[key] = combined_df.reset_index(drop=True)  # Store the combined DataFrame for the current key\n",
    "                \n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec81127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_promotionProductsP12M_updated = {}\n",
    "# for key, df in modified_promotionProductsP12M.items():\n",
    "#     df = df.copy()\n",
    "#     df = df[df['Product'] != '']\n",
    "#     df = df[df['Promo Sales'] >= 10000]\n",
    "#     df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "#     if not df.empty:\n",
    "#         modified_promotionProductsP12M_updated[key] = df\n",
    "# modified_promotionProductsP12M = modified_promotionProductsP12M_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0241c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_Bot(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        combined_df = pd.DataFrame() \n",
    "        for client in client_brands:\n",
    "            df = modified_promotionProductsP12M[key]\n",
    "            # Filter the DataFrame for the current client brand\n",
    "            df = df[df[f'{prodORitem}'] != '']\n",
    "            df = df[df[f'{BrandOrTopB}'] == client]\n",
    "            df = df.sort_values(by=f'{BrandOrTopB}')\n",
    "            df['Promo Share'] = pd.to_numeric(df['Promo Share'], errors='coerce')\n",
    "            df['cumulative promo share'] = df.groupby(f'{BrandOrTopB}')['Promo Share'].cumsum()\n",
    "            df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "            df = df[df['VSOD'] >= 0.05]\n",
    "            if df.shape[0] >0:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "                combined_df = combined_df.sort_values(by='Incr Value', ascending=False).tail(20).reset_index(drop=True)\n",
    "                combined_df = combined_df.sort_values(by ='Incr Value', ascending= True).reset_index(drop=True)\n",
    "        if combined_df.shape[0] > 0:\n",
    "            cleaned_data[key] = combined_df.reset_index(drop=True)  # Store the combined DataFrame for the current key\n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5316486-6b68-430c-963f-11df7ef98c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20clientonly = filter_data_Top(modified_promotionProductsP12M)\n",
    "\n",
    "bottom20clientonly = filter_data_Bot(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f3e9198-c36a-4544-8a00-92a1e2ab89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotionsEndOfWeekCleaning(promotions_EndOfWeek, notInScope, col=f'{BrandOrTopB}'):\n",
    "    \"\"\"\n",
    "    Clean promotions end of week data.\n",
    "\n",
    "    Parameters:\n",
    "    promotions_EndOfWeek (dict): Dictionary containing promotions end of week data.\n",
    "    notInScope (list): List of items not in scope to be excluded.\n",
    "    col (str): Column name to check for filtering. Default is 'Top Brands'.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing cleaned promotions end of week data.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store cleaned promotions end of week data\n",
    "    promotionsEndOfWeek = {}\n",
    "    # Iterate over items in promotions_EndOfWeek dictionary\n",
    "    for key, value in promotions_EndOfWeek.items():\n",
    "        # Make a copy of the dataframe to avoid modifying the original data\n",
    "        df = value.copy()\n",
    "        # Check if the dataframe is not empty\n",
    "        if df.shape[0] != 0:\n",
    "            # Modify the key to match the desired format if applicable\n",
    "            modified_key = key\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "            # Check if the key contains any item from the notInScope list\n",
    "            # If not, add the dataframe to the cleaned dictionary after filtering out 'Grand Total' rows\n",
    "            flag = False if any(element in modified_key for element in notInScope) else True\n",
    "            if flag:\n",
    "                promotionsEndOfWeek[modified_key] = df[df[col] != 'Grand Total'].reset_index(drop=True).replace(np.nan, 0)\n",
    "        else:\n",
    "            print(key, ' Is empty')\n",
    "    \n",
    "    return promotionsEndOfWeek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3bdd031-7086-48bc-b9b6-d9e09be82bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=cleaningData(promotions_EndOfWeek)\n",
    "promotionsEndOfWeekCleaned=promotionsEndOfWeekCleaning(mod,notInScope,col='End of Week')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e7408ac-109a-48e1-83ee-1f3acda454ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarket = list(set([key.split(' | ')[0]+' | '+key.split(' | ')[1] for key in promotionsEndOfWeekCleaned]))\n",
    "brandMarketCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat in key.split(' | ')[-1] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[-1] for cat in subcategories )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33070cde-4b18-410c-bf87-00d132014a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeDates(dfList, promotionsEndOfWeekCleaned):\n",
    "    \"\"\"\n",
    "    Complete dates for each dataframe in dfList based on promotionsEndOfWeekCleaned.\n",
    "\n",
    "    Parameters:\n",
    "    dfList (list): List of dataframe keys.\n",
    "    promotionsEndOfWeekCleaned (dict): Dictionary containing cleaned promotions end of week data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing EndOfWeekcompletDate (dictionary), dfGroup (list), and dic (dictionary).\n",
    "    \"\"\"\n",
    "    # Create a list of unique brand-category combinations\n",
    "    brandCatList = sorted(set(key.split(' | ')[0] + ' | ' + key.split(' | ')[2] for key in dfList))\n",
    "    \n",
    "    # Initialize dictionaries and lists\n",
    "    EndOfWeekcompletDate = {}\n",
    "    dfGroup = []\n",
    "    dic = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each brand-category combination\n",
    "    for key in brandCatList:\n",
    "        for name in dfList:\n",
    "            if (key.split(' | ')[0] == name.split(' | ')[0]) and (key.split(' | ')[1] == name.split(' | ')[2]):\n",
    "                dic[key] += 1\n",
    "                \n",
    "    # Iterate over unique brand-category combinations\n",
    "    for name in dic.keys():\n",
    "        # Get dataframe keys associated with the current brand-category combination\n",
    "        dfName = [key for key in dfList if name == (key.split(' | ')[0] + ' | ' + key.split(' | ')[2])]\n",
    "        \n",
    "        # Extract unique dates from all associated dataframes\n",
    "        uniqueDates = pd.concat([promotionsEndOfWeekCleaned[key] for key in dfName])[['End of Week']].drop_duplicates()\n",
    "        if uniqueDates.shape[0] > 0:\n",
    "            # Initialize dictionary for complete dates for each dataframe\n",
    "            dfCompleteDates = {}\n",
    "            \n",
    "            # Add dataframe keys to the group list\n",
    "            dfGroup.append(dfName)\n",
    "            \n",
    "            # Populate EndOfWeekcompletDate dictionary with dataframes merged on unique dates\n",
    "            for key in dfName:\n",
    "                EndOfWeekcompletDate[key] = pd.merge(uniqueDates, promotionsEndOfWeekCleaned[key], how='left').replace(np.nan, 0).sort_values(by='End of Week').reset_index(drop = True)\n",
    "    return EndOfWeekcompletDate, dfGroup, dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26bde262-eab5-4afc-ab44-9c3658cb974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory,catGroup,catDuplication=completeDates(brandMarketCategory,promotionsEndOfWeekCleaned)\n",
    "if len(sectors) != 0:\n",
    "    dfSector,secGroup,secDuplication=completeDates(brandMarketSector,promotionsEndOfWeekCleaned)\n",
    "if len(segments) != 0:\n",
    "    dfSegment,segGroup,segDuplication=completeDates(brandMarketSegment,promotionsEndOfWeekCleaned)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment,subsegGroup,subsegDuplication=completeDates(brandMarketSubSegment,promotionsEndOfWeekCleaned)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory,subcatGroup,subcatDuplication=completeDates(brandMarketSubCategory,promotionsEndOfWeekCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4108f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors):\n",
    "    sect_vsod_count =0\n",
    "    for key,df in sect_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                sect_vsod_count +=1\n",
    "    sect_vsod_count = sect_vsod_count *len(categories)\n",
    " \n",
    "if len(segments):\n",
    "    seg_vsod_count =0\n",
    "    for key,df in seg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                seg_vsod_count +=1\n",
    "    #seg_vsod_count = seg_vsod_count * len(sectors) \n",
    "    seg_vsod_count = seg_vsod_count           \n",
    " \n",
    "if len(subsegments) >0:\n",
    "    subseg_vsod_count =0\n",
    "    for key,df in subseg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subseg_vsod_count +=1\n",
    "    #subseg_vsod_count =subseg_vsod_count *len(segments)\n",
    "    subseg_vsod_count = subseg_vsod_count\n",
    " \n",
    "if len(subcategories) >0:\n",
    "    subcat_vsod_count =0\n",
    "    for key,df in subcat_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subcat_vsod_count +=1\n",
    "    #subcat_vsod_count = subcat_vsod_count * len(subsegments)\n",
    "    subcat_vsod_count = subcat_vsod_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18bd063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(subsegments) != 0:\n",
    "#     fourth_key, fourth_value = next(iter(subsegDuplication.items()))\n",
    "    \n",
    "#     # Check if the key is not in the other segment list\n",
    "#     if fourth_key not in segments:\n",
    "#         PromoRet.update({fourth_key: fourth_value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43c6f94a-d00f-46ad-9011-678073e6bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoRet ={}\n",
    "if len(categories)!=0:\n",
    "    first_key, first_value = next(iter(catDuplication.items()))\n",
    "    PromoRet.update({first_key: first_value})\n",
    "if len(sectors)!=0:\n",
    "    sec_key, sec_value = next(iter(secDuplication.items()))\n",
    "    PromoRet.update({sec_key:sec_value})\n",
    "if len(segments)!=0:\n",
    "    third_key, third_value = next(iter(segDuplication.items()))\n",
    "    PromoRet.update({third_key: third_value})\n",
    "if len(subsegments)!=0:\n",
    "    fourth_key, fourth_value = next(iter(subsegDuplication.items()))\n",
    "    PromoRet.update({fourth_key:fourth_value})\n",
    "if len(subcategories)!=0:\n",
    "    fifth_key, fifth_value = next(iter(subcatDuplication.items()))\n",
    "    PromoRet.update({fifth_key:fifth_value })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c094ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromoSalesTypes_data = {}\n",
    "# for key, df in modified_promotionBrandsP12M.items():\n",
    "#     df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "#     df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    "#     df = df[df['Value Share'] > 0.01]\n",
    "#     df = df[df['Promo Value'] > 0]\n",
    "#     # Select client brands and additional brands needed to make 10 brands\n",
    "#     df_client = selectClientBrands(modified_promotionBrandsP12M[key],'Top Brands', 'Value Share')\n",
    "#     number_of_brands_needed = max(10 - len(df_client),0)\n",
    "#     df = df[~df['Top Brands'].isin(client_brands)]\n",
    "#     df = df.sort_values(by='Value Share', ascending=False).head(number_of_brands_needed)\n",
    "#     # Concatenate client brands and additional brands\n",
    "#     df = pd.concat([df_client, df], ignore_index=True)\n",
    "#     df = df[df['Promo Value'] > 0]\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     if df.shape[0]:\n",
    "#         modified_promotionBrandsP12M[key] =df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a728a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoSalesTypes_data = {}\n",
    "for key, df in brands_promo_type.items():\n",
    "    df = DetectHeader(df)\n",
    "    df[f'{BrandOrTopB}'] = df[f'{BrandOrTopB}'].ffill()\n",
    "    df[\"Promo Sales\"] = pd.to_numeric(df[\"Promo Sales\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"Value Share\"] = pd.to_numeric(df[\"Value Share\"], errors=\"coerce\").fillna(0)\n",
    "    df = df[df['Promo Type'].notna()]\n",
    "    brand_totals = df.groupby(f\"{BrandOrTopB}\")['Promo Sales'].sum()\n",
    "\n",
    "    # df[\"Base Brand\"] = df[\"Top Brands\"].str.replace(\" Total\", \"\", regex=False)\n",
    "    # brand_totals = df[df[\"Top Brands\"].str.endswith(\"Total\")].set_index(\"Base Brand\")[\"Promo Sales\"].to_dict()\n",
    "    df[\"Brand Total Sales\"] = df[f\"{BrandOrTopB}\"].map(brand_totals)\n",
    "    df[\"% Promo Sales\"] = df[\"Promo Sales\"] / df[\"Brand Total Sales\"]\n",
    "\n",
    "    df = df[~df[f'{BrandOrTopB}'].str.contains('Others|Grand Total', case=False)]\n",
    "    df = df[df['Value Share'] > 0.01]\n",
    "    df = df[df['Promo Sales'] > 0]\n",
    "    # Select client brands and additional brands needed to make 10 brands\n",
    "    df_client = selectClientBrands(brands_promo_type[key],f'{BrandOrTopB}', 'Value Share')\n",
    "    comp_brand = df[~df[f'{BrandOrTopB}'].isin(cb for cb in client_brands)].drop_duplicates(f\"{BrandOrTopB}\")\n",
    "    if not df_client.empty:\n",
    "        comp_brand = comp_brand.nlargest(10-df_client[f\"{BrandOrTopB}\"].nunique(), \"Value Share\")[f\"{BrandOrTopB}\"].to_list()\n",
    "        # Concatenate client brands and additional brands\n",
    "        df = df[df[f\"{BrandOrTopB}\"].isin(comp_brand + client_brands)]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.sort_values(\"Value Share\", ascending=False).reset_index(drop=True)\n",
    "        # print(comp_brand)\n",
    "        if df.shape[0]:\n",
    "            PromoSalesTypes_data[key] =df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b23654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.getcwd() + '//Promotion base Oct 2024.pptx'\n",
    "# prs = Presentation(path)\n",
    "# slide = prs.slides[12]\n",
    "# brands = list(dataa['Top Brands'].unique())\n",
    "# promotype = dataa['Promo Type'].unique().tolist()\n",
    "# tables, charts = createTableAndChart(slide.shapes)\n",
    "# # Update table with category data\n",
    "# table = tables[0].table\n",
    "# num_rows_to_remove = len(table.rows) - len(brands)\n",
    "# #table_height = 3.88\n",
    "# table = removeRowFromTable(table, num_rows_to_remove, rowToExclude=0)\n",
    "# for row_number, row in enumerate(table.rows, start=0):\n",
    "#     for column_num, cell in enumerate(row.cells):\n",
    "#         if column_num == 0:\n",
    "#             cell.text = str(brands[row_number])\n",
    "#             set_cell_font(cell, 'Nexa Bold', 9)\n",
    "#             cell.text_frame.paragraphs[0].alignment = PP_ALIGN.LEFT\n",
    "# chart = charts[0].chart\n",
    "# chart_data = CategoryChartData()\n",
    "# chart_data.categories = brands\n",
    "# for promo_type in promotype:\n",
    "#     brand_values = {brand: 0 for brand in brands}\n",
    "    \n",
    "#     promo_data = dataa[dataa['Promo Type'] == promo_type]\n",
    "#     for _, row in promo_data.iterrows():\n",
    "#         brand_values[row['Top Brands']] = row['% Promo Sales']\n",
    "#     series_values = [value if value != 0 else None for value in [brand_values[brand] for brand in brands]]\n",
    "\n",
    "#     if any(value is not None for value in series_values):\n",
    "#         chart_data.add_series(promo_type, series_values)\n",
    "\n",
    "\n",
    "# chart.replace_data(chart_data)\n",
    "# chart.chart_style = 3\n",
    "# for i, series in enumerate(chart.series):\n",
    "#     fill = series.format.fill\n",
    "#     fill.solid()\n",
    "#     fill.fore_color.rgb = custom_colors[i]\n",
    "\n",
    "# outputPath=os.getcwd() + \"\\\\Promotion EdgeWell test.pptx\"\n",
    "# prs.save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88c1ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lis = []\n",
    "cat_lis = []\n",
    "if categories:\n",
    "    for i in range(len(catGroup)):\n",
    "        cat_lis += genrateIndexList(catGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(cat_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup)):\n",
    "        sec_lis += genrateIndexList(secGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(sec_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "seg_lis=[]\n",
    "if segments:\n",
    "    for i in range(len(segGroup)):\n",
    "        seg_lis += genrateIndexList(segGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(seg_lis)\n",
    "\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "subseg_lis =[]\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup)):\n",
    "        subseg_lis +=  genrateIndexList(subsegGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(subseg_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "subcat_lis =[]\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup)):\n",
    "        subcat_lis +=  genrateIndexList(subcatGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(subcat_lis)\n",
    "else:\n",
    "    final_lis.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd4fa9",
   "metadata": {},
   "source": [
    "### New Slide 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15c05226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = Sector_MonthYear['Boots']\n",
    "# d = DetectHeader(d).reset_index(drop=True)\n",
    "# d['Sector'] = d['Sector'].fillna(method = 'ffill')\n",
    "# d = d[~d['Sector'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "# d['year'] = pd.to_datetime(d['MonthYear'], format='%b-%y').dt.year\n",
    "# #yearly_avg_sales = d.groupby(['year', 'Sector'])['Value Sales'].mean().reset_index(drop=True) \n",
    "# yearly_avg_sales = d.groupby(['year', 'Sector'])['Value Sales'].transform('mean').reset_index(drop=True) \n",
    "\n",
    "# print(yearly_avg_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b59236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0c9e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonthYear_clean(data,column):\n",
    "    month_year_data={}\n",
    "    for key,df in data.items():\n",
    "        df=DetectHeader(data[key]).reset_index(drop=True)\n",
    "        df[column] = df[column].fillna(method = 'ffill')\n",
    "        df = df[~((df[column].str.contains('Total', case=False)) & (df[column] != categories[0]))].reset_index(drop=True)\n",
    "\n",
    "        # df = df[~df[column].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "        df['year'] = pd.to_datetime(df['MonthYear'], format='%b-%y').dt.year\n",
    "        df['year'] = df['year'].astype(int)\n",
    "\n",
    "        # Convert 'Month' column to datetime\n",
    "        df['Month_dt'] = pd.to_datetime(df['MonthYear'], format=\"%b-%y\")\n",
    "\n",
    "        # Convert start date string to datetime\n",
    "        start_date_dt = pd.to_datetime(start_date)\n",
    "\n",
    "        # Filter the DataFrame\n",
    "        df = df[df['Month_dt'] >= start_date_dt]\n",
    "\n",
    "        # Drop the helper column if needed\n",
    "        df = df.drop(columns=['Month_dt'])\n",
    "        yearly_avg_sales = df.groupby(['year', column])['Value Sales'].transform('mean').reset_index(drop=True) \n",
    "        yearly_avg_sales = yearly_avg_sales.replace(0, float('nan'))\n",
    "        \n",
    "        # Calculate 'Sales index' and handle NaN values gracefully\n",
    "        df['Sales index'] = (df['Value Sales'] / yearly_avg_sales * 100).fillna(0).astype(int)\n",
    "        # df['Sales index'] = (df['Value Sales'] / yearly_avg_sales * 100).astype(int)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if df.shape[0]>0:\n",
    "            # if column =='Sector':\n",
    "            #     newkey =key\n",
    "            #     month_year_data[newkey] = df\n",
    "            # elif column == 'Segment':\n",
    "            #     newkey= key.split(' | ')[0] + ' | ' + segments[-1]\n",
    "            #     month_year_data[newkey] = df\n",
    "            # elif column == 'SubSegment':\n",
    "            #     newkey= key.split(' | ')[0] + ' | ' + subsegments[-1]\n",
    "            #     month_year_data[newkey] = df\n",
    "            # else:\n",
    "            #     newkey= key.split(' | ')[0] + ' | ' + subcategories[-1]\n",
    "            month_year_data[key] = df\n",
    "    return month_year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "800863e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sector_MonthYear = {\n",
    "    f\"{key} | {categories[0]}\": value for key, value in Sector_MonthYear.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db1737be",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_month_year=MonthYear_clean(Category_MonthYear,column='Category')\n",
    "sector_month_year = MonthYear_clean(Sector_MonthYear,column='Sector')\n",
    "segment_month_year = MonthYear_clean(Segment_MonthYear,column='Segment')\n",
    "subcat_month_year = MonthYear_clean(SubCategory_MonthYear,column='SubCategory')\n",
    "subseg_month_year = MonthYear_clean(SubSegment_MonthYear,column='SubSegment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6ac48172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_month_year(data, column):\n",
    "    final_month_year ={}\n",
    "    for key,df in data.items():\n",
    "        for sec in df[column].unique():\n",
    "            newkey = key + ' | ' + sec\n",
    "            new_df = df[df[column] == sec].reset_index(drop=True)\n",
    "            if new_df.shape[0] > 0:\n",
    "                final_month_year[newkey] = new_df\n",
    "    return final_month_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "265d60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_month_year1=split_month_year(category_month_year,'Category')\n",
    "sector_month_year1 = split_month_year(sector_month_year,'Sector')\n",
    "segment_month_year1 = split_month_year(segment_month_year,'Segment')\n",
    "subseg_month_year1 = split_month_year(subseg_month_year,'SubSegment')\n",
    "subcat_month_year1 = split_month_year(subcat_month_year,'SubCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b92a3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_year1 = {}\n",
    "month_year1.update(sector_month_year1)\n",
    "month_year1.update(segment_month_year1)\n",
    "month_year1.update(subcat_month_year1)\n",
    "month_year1.update(subseg_month_year1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a94833af",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarketCategory = [key for key in category_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in sector_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in segment_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in subseg_month_year1.keys() if any(cat == key.split(' | ')[2] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in subcat_month_year1.keys() if any(cat == key.split(' | ')[2] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "17cb72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def completeDates1(dfList, promotionsEndOfWeekCleaned,column=\"Sector\"):\n",
    "    \"\"\"\n",
    "    Complete dates for each dataframe in dfList based on promotionsEndOfWeekCleaned.\n",
    "\n",
    "    Parameters:\n",
    "    dfList (list): List of dataframe keys.\n",
    "    promotionsEndOfWeekCleaned (dict): Dictionary containing cleaned promotions end of week data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing EndOfWeekcompletDate (dictionary), dfGroup (list), and dic (dictionary).\n",
    "    \"\"\"\n",
    "    # Create a list of unique brand-category combinations\n",
    "    brandCatList = sorted(set(key.split(' | ')[0]  for key in dfList))\n",
    "    # Initialize dictionaries and lists\n",
    "    EndOfWeekcompletDate = {}\n",
    "    dfGroup = []\n",
    "    dic = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each brand-category combination\n",
    "    for key in brandCatList:\n",
    "        for name in dfList:\n",
    "            if (key.split(' | ')[0] == name.split(' | ')[0]):\n",
    "                \n",
    "                dic[key] += 1\n",
    "                \n",
    "    # Iterate over unique brand-category combinations\n",
    "    for name in dic.keys():\n",
    "\n",
    "        if column == \"Sector\" :\n",
    "            \n",
    "            # Filter dataframe keys associated with the current brand-category combination\n",
    "            dfName = [key for key in dfList if name == key.split(' | ')[0] and len(name.split(' | ')) == 1]\n",
    "        else:\n",
    "            \n",
    "            dfName = [key for key in dfList if name == key.split(' | ')[0]  ]\n",
    "        \n",
    "        # Extract unique dates from all associated dataframes\n",
    "        uniqueDates = pd.concat([promotionsEndOfWeekCleaned[key] for key in dfName])[['MonthYear']].drop_duplicates()\n",
    "        # Initialize dictionary for complete dates for each dataframe\n",
    "        dfCompleteDates = {}\n",
    "        # Add dataframe keys to the group list\n",
    "        dfGroup.append(dfName)\n",
    "        # Populate EndOfWeekcompletDate dictionary with dataframes merged on unique dates\n",
    "        for key in dfName:\n",
    "            EndOfWeekcompletDate[key] = pd.merge(uniqueDates, promotionsEndOfWeekCleaned[key], how='left')#.replace(np.nan, 0)\n",
    "            column = EndOfWeekcompletDate[key].columns[1]\n",
    "            year = EndOfWeekcompletDate[key].columns[3]\n",
    "            monthyear = EndOfWeekcompletDate[key].columns[0]\n",
    "            EndOfWeekcompletDate[key][column] = EndOfWeekcompletDate[key][column].fillna(method='ffill')      \n",
    "            EndOfWeekcompletDate[key][year] = pd.to_datetime(EndOfWeekcompletDate[key][monthyear], format='%b-%y').dt.year\n",
    "            EndOfWeekcompletDate[key] = EndOfWeekcompletDate[key].fillna(0)\n",
    "    return EndOfWeekcompletDate, dfGroup, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3b1814ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCategory0,categoryGroup0,categoryDuplication0=completeDates1(brandMarketCategory,category_month_year1,column=\"Category\")\n",
    "if len(sectors) != 0:\n",
    "    dfSector0,secGroup0,secDuplication0=completeDates1(brandMarketSector,sector_month_year1,column=\"Sector\")\n",
    "if len(segments) != 0:\n",
    "    dfSegment0,segGroup0,segDuplication0=completeDates1(brandMarketSegment,segment_month_year1,column=\"Segment\")\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment0,subsegGroup0,subsegDuplication0=completeDates1(brandMarketSubSegment,subseg_month_year1,column=\"Subsegment\")\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory0,subcatGroup0,subcatDuplication0=completeDates1(brandMarketSubCategory,subcat_month_year1,column=\"Subcategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99912278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupingkeys(data):\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    for sublist in data:\n",
    "        for entry in sublist:\n",
    "            prefix = \" | \".join(entry.split(\" | \")[:2])  # Extract first two parts\n",
    "            grouped[prefix].append(entry)\n",
    "    result = list(grouped.values())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "55074d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryGroup0=groupingkeys(categoryGroup0)\n",
    "if len(sectors) != 0:\n",
    "    secGroup0=groupingkeys(secGroup0)\n",
    "if len(segments) != 0:\n",
    "    segGroup0=groupingkeys(segGroup0)\n",
    "if len(subsegments) != 0:\n",
    "    subsegGroup0=groupingkeys(subsegGroup0)\n",
    "if len(subcategories) != 0:\n",
    "    subcatGroup0=groupingkeys(subcatGroup0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e026eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 20, 20, 20, 20, 20, 20, 20, 20], [21, 21, 21, 21, 21, 21, 21, 21, 21]]\n",
      "[[20, 20, 20, 20, 20, 20, 20, 20, 20], [21, 21, 21, 21, 21, 21, 21, 21, 21], [21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20]]\n",
      "[[20, 20, 20, 20, 20, 20, 20, 20, 20], [21, 21, 21, 21, 21, 21, 21, 21, 21], [21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20], []]\n"
     ]
    }
   ],
   "source": [
    "final_lis0 = []\n",
    "category_lis = []\n",
    "if categories:\n",
    "    for i in range(len(categoryGroup0)):\n",
    "        category_lis += genrateIndexList(categoryGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(category_lis)  # Append empty list if sectors is False\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup0)):\n",
    "        sec_lis += genrateIndexList(secGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(sec_lis)  # Append empty list if sectors is False\n",
    "print(final_lis0)\n",
    "\n",
    "seg_lis = []\n",
    "if segments:\n",
    "    for i in range(len(segGroup0)):\n",
    "        seg_lis += genrateIndexList(segGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(seg_lis)  # Append empty list if segments is False\n",
    "print(final_lis0)\n",
    "\n",
    "subseg_lis = []\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup0)):\n",
    "        subseg_lis += genrateIndexList(subsegGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(subseg_lis)  # Append empty list if subsegments is False\n",
    "print(final_lis0)\n",
    "\n",
    "subcat_lis = []\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup0)):\n",
    "        subcat_lis += genrateIndexList(subcatGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(subcat_lis)  # Append empty list if subcategories is False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc50143",
   "metadata": {},
   "source": [
    "### New slide 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3653fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_endofweek_P12M = {}\n",
    "past_12_months = pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n",
    "for key in modified_promotionEndOfWeek.keys():\n",
    "    df=modified_promotionEndOfWeek[key].copy()\n",
    "    df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "    filtered_df = df[df['End of Week'].dt.strftime('%b-%y').isin(past_12_months)]\n",
    "    if filtered_df.shape[0] >0:\n",
    "        modified_endofweek_P12M[key] = filtered_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06d18d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarket = list(set([key.split(' | ')[0]+' | '+key.split(' | ')[1] for key in modified_endofweek_P12M]))\n",
    "brandMarketCategory= [key for key in modified_endofweek_P12M.keys() if any(cat in key.split(' | ')[-1] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[-1] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[-1] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[-1] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[-1] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3ace0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory1,catGroup1,catDuplication1=completeDates(brandMarketCategory,modified_endofweek_P12M)\n",
    "if len(sectors) != 0:\n",
    "    dfSector1,secGroup1,secDuplication1=completeDates(brandMarketSector,modified_endofweek_P12M)\n",
    "if len(segments) != 0:\n",
    "    dfSegment1,segGroup1,segDuplication1=completeDates(brandMarketSegment,modified_endofweek_P12M)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment1,subsegGroup1,subsegDuplication1=completeDates(brandMarketSubSegment,modified_endofweek_P12M)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory1,subcatGroup1,subcatDuplication1=completeDates(brandMarketSubCategory,modified_endofweek_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8479ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promofrequencyclean(data):        \n",
    "        modified_dfCategory1 = {}\n",
    "        for k in data.keys():\n",
    "                chart_df=data[k].copy()\n",
    "                chart_df['Weekly VSOD'] = np.where((chart_df['VSOD']>.2)&(chart_df['Value Uplift (v. base) Normalized'] != ''),1,None)\n",
    "                chart_df['try'] = 0\n",
    "                chart_df['New Uplift'] = 0\n",
    "                chart_df['try'] = np.where((chart_df['Value Uplift (v. base) Normalized']>=2),1.8,chart_df['Value Uplift (v. base) Normalized'])\n",
    "                chart_df['New Uplift'] = np.where((chart_df['Weekly VSOD']==1)&(chart_df['Value Uplift (v. base) Normalized']>0.05),chart_df['try'],None)\n",
    "                if not chart_df['Weekly VSOD'].isnull().all():\n",
    "                        modified_dfCategory1[k]= chart_df \n",
    "        return modified_dfCategory1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ad97599",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories)!=0: \n",
    "    modified_dfCategory1=promofrequencyclean(dfCategory1)\n",
    "if len(sectors)!=0: \n",
    "    modified_dfSector1=promofrequencyclean(dfSector1)\n",
    "if len(segments)!=0: \n",
    "    modified_dfSegment1=promofrequencyclean(dfSegment1)\n",
    "if len(subsegments)!=0: \n",
    "    modified_dfSubSegment1=promofrequencyclean(dfSubSegment1)\n",
    "if len(subcategories)!=0: \n",
    "    modified_dfSubCategory1=promofrequencyclean(dfSubCategory1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f0aae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarket = list(set([key.split(' | ')[0]+' | '+key.split(' | ')[1] for key in modified_endofweek_P12M]))\n",
    "brandMarketCategory= [key for key in modified_dfCategory1.keys() if any(cat in key.split(' | ')[-1] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in modified_dfSector1.keys() if any(cat == key.split(' | ')[-1] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in modified_dfSegment1.keys() if any(cat == key.split(' | ')[-1] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in modified_dfSubSegment1.keys() if any(cat == key.split(' | ')[-1] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in modified_dfSubCategory1.keys() if any(cat == key.split(' | ')[-1] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31f3763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory1,catGroup1,catDuplication1=completeDates(brandMarketCategory,modified_endofweek_P12M)\n",
    "if len(sectors) != 0:\n",
    "    dfSector1,secGroup1,secDuplication1=completeDates(brandMarketSector,modified_endofweek_P12M)\n",
    "if len(segments) != 0:\n",
    "    dfSegment1,segGroup1,segDuplication1=completeDates(brandMarketSegment,modified_endofweek_P12M)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment1,subsegGroup1,subsegDuplication1=completeDates(brandMarketSubSegment,modified_endofweek_P12M)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory1,subcatGroup1,subcatDuplication1=completeDates(brandMarketSubCategory,modified_endofweek_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c2bbe9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_region(data):\n",
    "    # Define categories for grouping\n",
    "    market_groups = {\n",
    "        \"RETAILER_REGIONS\": regions_RET,\n",
    "        \"RETAILER_CHANNELS\": channels_RET,\n",
    "        \"RETAILER_MARKET\": market_RET,\n",
    "        \"CHANNEL_REGIONS\": regions_CHAN,\n",
    "        \"CHANNEL_CHANNELS\": channels_CHAN,\n",
    "        \"CHANNEL_MARKET\": market_CHAN,\n",
    "        f\"{customareas}_REGIONS\": regions_CUST,\n",
    "        f\"{customareas}_CHANNELS\": channels_CUST,\n",
    "        f\"{customareas}_MARKET\": market_CUST,\n",
    "    }\n",
    "    result = []\n",
    "    for sublist in data:\n",
    "        for category, keywords in market_groups.items():\n",
    "            # Filter items matching the current category\n",
    "            base_category = category.split(\"_\")[0]\n",
    "\n",
    "            group = [\n",
    "                f\"{item} | {base_category}\" for item in sublist if item.split(\" | \")[1] in keywords\n",
    "            ]\n",
    "            if group:  # Append only non-empty groups\n",
    "                result.append(group)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "016f5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories)>0:\n",
    "    catGroup1 = group_by_region(catGroup1)\n",
    "if len(sectors)>0:\n",
    "    secGroup1 = group_by_region(secGroup1)\n",
    "if len(segments)>0:\n",
    "    segGroup1 = group_by_region(segGroup1)\n",
    "if len(subsegments)>0:\n",
    "    subsegGroup1 = group_by_region(subsegGroup1)\n",
    "if len(subcategories)>0:\n",
    "    subcatGroup1 = group_by_region(subcatGroup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95b943e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lis1 = []\n",
    "cat_lis = []\n",
    "if categories:\n",
    "    for i in range(len(catGroup1)):\n",
    "        cat_lis += genrateIndexList(catGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(cat_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup1)):\n",
    "        sec_lis += genrateIndexList(secGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(sec_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "seg_lis=[]\n",
    "if segments:\n",
    "    for i in range(len(segGroup1)):\n",
    "        seg_lis += genrateIndexList(segGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(seg_lis)\n",
    "\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "subseg_lis =[]\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup1)):\n",
    "        subseg_lis +=  genrateIndexList(subsegGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(subseg_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "subcat_lis =[]\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup1)):\n",
    "        subcat_lis +=  genrateIndexList(subcatGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(subcat_lis)\n",
    "else:\n",
    "    final_lis1.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "21faef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "retailer=regions_RET+channels_RET+market_RET\n",
    "channels=regions_CHAN+channels_CHAN+channels_CHAN\n",
    "customarea=regions_CUST+channels_CUST+market_CUST\n",
    "def addarea(modified_dfCategory1,retailer,market=\"RETAILER\"):\n",
    "    keys_to_modify = [k for k in modified_dfCategory1.keys() if k.split(\" | \")[1] in retailer]\n",
    "    for k in keys_to_modify:\n",
    "        new_key = k + \" | \"+ market\n",
    "        modified_dfCategory1[new_key] = modified_dfCategory1[k]  \n",
    "        del modified_dfCategory1[k]       \n",
    "    return modified_dfCategory1      \n",
    "if len(categories)>0:            \n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,retailer,market=\"RETAILER\")\n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,channels,market=\"CHANNELS\")\n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,customarea,market=f\"{customareas}\")\n",
    "\n",
    "if len(sectors)>0:            \n",
    "    modified_dfSector1=addarea(modified_dfSector1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSector1=addarea(modified_dfSector1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSector1=addarea(modified_dfSector1,customarea,market=f\"{customareas}\")\n",
    "if len(segments)>0:            \n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,customarea,market=f\"{customareas}\")\n",
    "if len(subsegments)>0:            \n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,customarea,market=f\"{customareas}\")\n",
    "if len(subcategories)>0:            \n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,customarea,market=f\"{customareas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec5211df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(modified_promotionBrandsP12M.keys()):  # Convert to list to avoid runtime errors\n",
    "    df = modified_promotionBrandsP12M[k].copy()\n",
    "    # Filter rows based on 'Top Brands'\n",
    "    df = df[~df[f'{BrandOrTopB}'].str.contains('Others', case=False, na=False)]\n",
    "    df = df[~df[f'{BrandOrTopB}'].str.contains('Grand Total', case=False, na=False)]\n",
    "    df = df[df['Value Share'] > 0.01]\n",
    "    if not df.empty:\n",
    "        modified_promotionBrandsP12M[k] = df\n",
    "    else:\n",
    "        del modified_promotionBrandsP12M[k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee2a74",
   "metadata": {},
   "source": [
    "\n",
    "## Slide duplication: index, duplication and section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ceca67f7-038d-44e1-98aa-cbc9a32d8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [*[0]*5,\n",
    "        #  *[1]*5,\n",
    "         *[1]*5,\n",
    "         *[2]*5,\n",
    "         *[3]*5,\n",
    "         *[4]*5,\n",
    "         *[5]*5,\n",
    "         *[6]*5,\n",
    "         *[7]*5,\n",
    "         *[8]*4,\n",
    "         *[9]*5,\n",
    "         *[10]*5,\n",
    "         *[11]*5,\n",
    "         *[12]*5,\n",
    "         *[13]*5,\n",
    "         *[14]*5,\n",
    "         *final_lis,\n",
    "         *[19]*5,\n",
    "         *final_lis0,\n",
    "         *final_lis1,\n",
    "         *[0]*5,\n",
    "         *[1]*5,\n",
    "         *[2]*5,\n",
    "         *[9]*5,\n",
    "         *[10]*5,\n",
    "         #*[11]*5,\n",
    "         *[12]*5,\n",
    "         *[13]*5\n",
    "        ]\n",
    "duplication = combine_duplications(Scope,count_df,[promotionsBrandSortedTotalFinal, #0\n",
    "                                                   #promotionsBrandSortedTotalFinal, #1\n",
    "                                                   newpromotionsNotBrandsWithMarket, #2\n",
    "                                                   concated, #3\n",
    "                                                   modified_promotionProductsP12M_volumeuplift, #4\n",
    "                                                   new_modified_promotionProductsP12M, #5\n",
    "                                                   new_modified_promotionProductsP12M, #6\n",
    "                                                   top20clientonly, #7\n",
    "                                                   bottom20clientonly,#8\n",
    "                                                   modified_promotionBrandsP12M, #10\n",
    "                                                   newModifiedBrands, #11\n",
    "                                                   PromoSalesTypes_data if promo_type else None,#12\n",
    "                                                   modified_promotionBrandsP12M if feature_share else None, #13\n",
    "                                                   modified_promotionBrandsP12M if display_share else None, #14\n",
    "                                                   modified_promotionEndOfWeek,#15\n",
    "                                                   PromoRet, #16-19\n",
    "                                                   modified_valueUplift, #20\n",
    "                                                   #month_year1,#21\n",
    "                                                   #modified_endofweek_P12M, #22\n",
    "                                                   modified_promotionBrandsP12M, #0 with no client\n",
    "                                                   #promotionsBrandNOTSortedTotalFinal, #1 with no client\n",
    "                                                   newpromotionsNotBrandsWithMarket, #2 with no client\n",
    "                                                   concated, #3 with no client\n",
    "                                                   modified_promotionBrandsP12M, # 10 with no client\n",
    "                                                   newModifiedBrands, #11 with no client\n",
    "                                                   #PromoSalesTypes_data if promo_type else None,#12  with no client\n",
    "                                                   modified_promotionBrandsP12M if feature_share else None, #13  with no client\n",
    "                                                   modified_promotionBrandsP12M if display_share else None #14 with no client\n",
    "                                                  ])\n",
    "section_names = [#\"Promo Value Sale\",#0\n",
    "                 \"Promo Evolution\", #1\n",
    "                 \"VSOD Summary by Sector\" , #2\n",
    "                 \"Value uplift by retailer by brand\", #3 \n",
    "                 \"Volume Uplift vs discount depth\",#4\n",
    "                 \"Value Uplift vs Promo Efficiency Quadrant\", #5\n",
    "                 \"Top 20 promotions\", #6\n",
    "                 \"Top 20 promotions CLIENT ONLY\", #7\n",
    "                 \"Bottom 20 promotions CLIENT ONLY\", #8\n",
    "                 \"Promo share vs Value Share\", #10\n",
    "                 \"Promo Sales by total size\",#11\n",
    "                 \"Promo Sales by promo type\", #12\n",
    "                 \"Feature Share vs Fair Share\", #13\n",
    "                 \"Display Share vs Fair Share\", #14\n",
    "                 \"Promo Frequency learnings\", #15\n",
    "                 \"Promo sales per retailer\", #16-19\n",
    "                 \"Value Uplift vs discount depth\" ,#20\n",
    "                 #\"Seasonality Index\",#21\n",
    "                 #\"Promotional Frequency Analysis\", #22\n",
    "                 #\"Promo Value Sale no client prio\",\n",
    "                 \"Promo Evolution no client prio\",\n",
    "                 \"VSOD Summary by Sector no client prio\",\n",
    "                 \"Value uplift by retailer by brand no client prio\",\n",
    "                \"Promo share vs Value Share no client prio\", #10\n",
    "                 \"Promo Sales by total size no client prio\",#11\n",
    "                 #\"Promo Sales by promo type no client prio\", #12\n",
    "                 \"Feature Share vs Fair Share no client prio\", #13\n",
    "                 \"Display Share vs Fair Share no client prio\" #14\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "#duplication.insert(89, 0)\n",
    "\n",
    "if len(sectors) > 0:\n",
    "       #duplication.insert(45,(len(client_manuf)+len(client_brands))*len(categories)* len(marketList))\n",
    "       duplication.insert(40, sect_vsod_count)\n",
    "if len(segments) > 0:\n",
    "        #duplication.insert(46,(len(client_manuf)+len(client_brands))*len(sectors)* len(marketList)) \n",
    "         duplication.insert(41, seg_vsod_count)\n",
    " \n",
    "else:\n",
    "    duplication.insert(41,1)  \n",
    "  \n",
    "if len(subsegments) > 0:\n",
    "        #duplication.insert(47,(len(client_manuf)+len(client_brands))*len(segments)* len(marketList))\n",
    "        duplication.insert(42, subseg_vsod_count)\n",
    "\n",
    "else:\n",
    "    duplication.insert(42,1)\n",
    "\n",
    "if len(subcategories) > 0:\n",
    "        #duplication.insert(48,(len(client_manuf)+len(client_brands))*len(segments)* len(marketList))\n",
    "        duplication.insert(43, subcat_vsod_count)\n",
    "\n",
    "else:\n",
    "    duplication.insert(43,1)\n",
    "\n",
    "\n",
    "duplication.insert(84,1)\n",
    "duplication.insert(85, 1)\n",
    "duplication.insert(86, 1)\n",
    "duplication.insert(87, 1)\n",
    "duplication.insert(88, 1)\n",
    "duplication.insert(89, 1)\n",
    "duplication.insert(90, 1)\n",
    "duplication.insert(91, 1)\n",
    "duplication.insert(92, 1)\n",
    "duplication.insert(93, 1)\n",
    "\n",
    "section_names = [f\"{name} {suffix}\" for name in section_names for suffix in suffixes]\n",
    "\n",
    "section_names.insert(40,'Volume Sold on Deal Sector')\n",
    "section_names.insert(41,'Volume Sold on Deal Segment')\n",
    "section_names.insert(42,'Volume Sold on Deal SubSegment')\n",
    "section_names.insert(43,'Volume Sold on Deal SubCategory')\n",
    "\n",
    "section_names.insert(84,'Seasonality Index Category')\n",
    "section_names.insert(85,'Seasonality Index Sector')\n",
    "section_names.insert(86,'Seasonality Index Segment')\n",
    "section_names.insert(87,'Seasonality Index Subsegment')\n",
    "section_names.insert(88,'Seasonality Index Subcategory')\n",
    "\n",
    "section_names.insert(89,'Promotional Frequency Analysis Category')\n",
    "section_names.insert(90,'Promotional Frequency Analysis Sector')\n",
    "section_names.insert(91,'Promotional Frequency Analysis Segment')\n",
    "section_names.insert(92,'Promotional Frequency Analysis Subsegment')\n",
    "section_names.insert(93,'Promotional Frequency Analysis Subcategory')\n",
    "# section_names.insert(94,'Promotional Frequency Analysis Subcategory')\n",
    "\n",
    "duplication[77]=1\n",
    "#index = [i for i in index if i != []]\n",
    "# duplication = [i for i in duplication if i != []]\n",
    "\n",
    "path = os.getcwd() + '//Promotion base.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated.pptx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bbcf3",
   "metadata": {},
   "source": [
    "### Deck 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "78c45915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slides_name = ['Promo sales per retailer']\n",
    "# indices = [i for i, s in enumerate(section_names) if any(sub.lower() in s.lower() for sub in slides_name)]\n",
    "# indices\n",
    "# for i in range(indices[0], indices[-1]+1):\n",
    "#     duplication[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "44e60db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 36,\n",
       " 72,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 35,\n",
       " 62,\n",
       " 97,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 18,\n",
       " 34,\n",
       " 50,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 18,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1a93e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, [18, 18, 18, 18, 15, 18, 18, 15, 18, 18, 15], [18, 18, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15], [18, 18, 18, 18, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15], [], [], 19, 19, 19, 19, 19, [20, 20, 20, 20, 20, 20, 20, 20, 20], [21, 21, 21, 21, 21, 21, 21, 21, 21], [21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20, 21, 20], [], [], [26, 27, 26, 27, 26, 28, 29, 26], [26, 28, 29, 26, 28, 29, 26, 26, 28, 29, 26, 28, 29, 26, 26, 27, 29, 26, 26, 28, 29, 26], [26, 28, 29, 26, 28, 29, 26, 26, 28, 29, 26, 28, 29, 26, 26, 27, 29, 26, 26, 27, 29, 26, 26, 28, 29, 26], [], [], 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13]\n",
      "[9, 18, 27, 0, 0, 0, 9, 9, 0, 0, 1, 2, 3, 0, 0, 9, 18, 27, 0, 0, 9, 18, 27, 0, 0, 9, 18, 27, 0, 0, 9, 18, 27, 0, 0, 9, 18, 27, 0, 0, 36, 72, 1, 1, 9, 18, 27, 0, 0, 0, 0, 0, 0, 0, 4, 8, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 62, 97, 0, 0, 1, 1, 1, 1, 0, 18, 34, 50, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 18, 27, 0, 0, 0, 9, 9, 0, 0, 1, 2, 3, 0, 0, 9, 18, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['Promo Evolution Category', 'Promo Evolution Sector', 'Promo Evolution Segment', 'Promo Evolution SubSegment', 'Promo Evolution SubCategory', 'VSOD Summary by Sector Category', 'VSOD Summary by Sector Sector', 'VSOD Summary by Sector Segment', 'VSOD Summary by Sector SubSegment', 'VSOD Summary by Sector SubCategory', 'Value uplift by retailer by brand Category', 'Value uplift by retailer by brand Sector', 'Value uplift by retailer by brand Segment', 'Value uplift by retailer by brand SubSegment', 'Value uplift by retailer by brand SubCategory', 'Volume Uplift vs discount depth Category', 'Volume Uplift vs discount depth Sector', 'Volume Uplift vs discount depth Segment', 'Volume Uplift vs discount depth SubSegment', 'Volume Uplift vs discount depth SubCategory', 'Value Uplift vs Promo Efficiency Quadrant Category', 'Value Uplift vs Promo Efficiency Quadrant Sector', 'Value Uplift vs Promo Efficiency Quadrant Segment', 'Value Uplift vs Promo Efficiency Quadrant SubSegment', 'Value Uplift vs Promo Efficiency Quadrant SubCategory', 'Top 20 promotions Category', 'Top 20 promotions Sector', 'Top 20 promotions Segment', 'Top 20 promotions SubSegment', 'Top 20 promotions SubCategory', 'Top 20 promotions CLIENT ONLY Category', 'Top 20 promotions CLIENT ONLY Sector', 'Top 20 promotions CLIENT ONLY Segment', 'Top 20 promotions CLIENT ONLY SubSegment', 'Top 20 promotions CLIENT ONLY SubCategory', 'Bottom 20 promotions CLIENT ONLY Category', 'Bottom 20 promotions CLIENT ONLY Sector', 'Bottom 20 promotions CLIENT ONLY Segment', 'Bottom 20 promotions CLIENT ONLY SubSegment', 'Bottom 20 promotions CLIENT ONLY SubCategory', 'Volume Sold on Deal Sector', 'Volume Sold on Deal Segment', 'Volume Sold on Deal SubSegment', 'Volume Sold on Deal SubCategory', 'Promo share vs Value Share Category', 'Promo share vs Value Share Sector', 'Promo share vs Value Share Segment', 'Promo share vs Value Share SubSegment', 'Promo share vs Value Share SubCategory', 'Promo Sales by total size Category', 'Promo Sales by total size Sector', 'Promo Sales by total size Segment', 'Promo Sales by total size SubSegment', 'Promo Sales by total size SubCategory', 'Promo Sales by promo type Category', 'Promo Sales by promo type Sector', 'Promo Sales by promo type Segment', 'Promo Sales by promo type SubSegment', 'Promo Sales by promo type SubCategory', 'Feature Share vs Fair Share Category', 'Feature Share vs Fair Share Sector', 'Feature Share vs Fair Share Segment', 'Feature Share vs Fair Share SubSegment', 'Feature Share vs Fair Share SubCategory', 'Display Share vs Fair Share Category', 'Display Share vs Fair Share Sector', 'Display Share vs Fair Share Segment', 'Display Share vs Fair Share SubSegment', 'Display Share vs Fair Share SubCategory', 'Promo Frequency learnings Category', 'Promo Frequency learnings Sector', 'Promo Frequency learnings Segment', 'Promo Frequency learnings SubSegment', 'Promo Frequency learnings SubCategory', 'Promo sales per retailer Category', 'Promo sales per retailer Sector', 'Promo sales per retailer Segment', 'Promo sales per retailer SubSegment', 'Promo sales per retailer SubCategory', 'Value Uplift vs discount depth Category', 'Value Uplift vs discount depth Sector', 'Value Uplift vs discount depth Segment', 'Value Uplift vs discount depth SubSegment', 'Value Uplift vs discount depth SubCategory', 'Seasonality Index Category', 'Seasonality Index Sector', 'Seasonality Index Segment', 'Seasonality Index Subsegment', 'Seasonality Index Subcategory', 'Promotional Frequency Analysis Category', 'Promotional Frequency Analysis Sector', 'Promotional Frequency Analysis Segment', 'Promotional Frequency Analysis Subsegment', 'Promotional Frequency Analysis Subcategory', 'Promo Evolution no client prio Category', 'Promo Evolution no client prio Sector', 'Promo Evolution no client prio Segment', 'Promo Evolution no client prio SubSegment', 'Promo Evolution no client prio SubCategory', 'VSOD Summary by Sector no client prio Category', 'VSOD Summary by Sector no client prio Sector', 'VSOD Summary by Sector no client prio Segment', 'VSOD Summary by Sector no client prio SubSegment', 'VSOD Summary by Sector no client prio SubCategory', 'Value uplift by retailer by brand no client prio Category', 'Value uplift by retailer by brand no client prio Sector', 'Value uplift by retailer by brand no client prio Segment', 'Value uplift by retailer by brand no client prio SubSegment', 'Value uplift by retailer by brand no client prio SubCategory', 'Promo share vs Value Share no client prio Category', 'Promo share vs Value Share no client prio Sector', 'Promo share vs Value Share no client prio Segment', 'Promo share vs Value Share no client prio SubSegment', 'Promo share vs Value Share no client prio SubCategory', 'Promo Sales by total size no client prio Category', 'Promo Sales by total size no client prio Sector', 'Promo Sales by total size no client prio Segment', 'Promo Sales by total size no client prio SubSegment', 'Promo Sales by total size no client prio SubCategory', 'Feature Share vs Fair Share no client prio Category', 'Feature Share vs Fair Share no client prio Sector', 'Feature Share vs Fair Share no client prio Segment', 'Feature Share vs Fair Share no client prio SubSegment', 'Feature Share vs Fair Share no client prio SubCategory', 'Display Share vs Fair Share no client prio Category', 'Display Share vs Fair Share no client prio Sector', 'Display Share vs Fair Share no client prio Segment', 'Display Share vs Fair Share no client prio SubSegment', 'Display Share vs Fair Share no client prio SubCategory']\n",
      "129\n",
      "129\n",
      "129\n",
      "[[18, 18, 18, 18, 15, 18, 18, 15, 18, 18, 15], [18, 18, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15], [18, 18, 18, 18, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15, 18, 18, 15], [], []]\n",
      "[[26, 27, 26, 27, 26, 28, 29, 26], [26, 28, 29, 26, 28, 29, 26, 26, 28, 29, 26, 28, 29, 26, 26, 27, 29, 26, 26, 28, 29, 26], [26, 28, 29, 26, 28, 29, 26, 26, 28, 29, 26, 28, 29, 26, 26, 27, 29, 26, 26, 27, 29, 26, 26, 28, 29, 26], [], []]\n",
      "978\n"
     ]
    }
   ],
   "source": [
    "print(index)\n",
    "print(duplication)\n",
    "print(section_names)\n",
    "print(len(index))\n",
    "print(len(duplication))\n",
    "print(len(section_names))\n",
    "print(final_lis)\n",
    "print(final_lis1)\n",
    "print(sum(duplication))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "474f4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb24e2",
   "metadata": {},
   "source": [
    "## If we want specific slides duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e14224cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slides_name = ['display share', 'feature share']\n",
    "# indices = [i for i, s in enumerate(section_names) if any(sub.lower() in s.lower() for sub in slides_name)]\n",
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "016df9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_section_names = [section_names[i] for i in indices]\n",
    "# filtered_duplication = [duplication[i] for i in indices]\n",
    "# filtered_index = [index[i] for i in indices]\n",
    "\n",
    "# print(filtered_section_names)\n",
    "# print(filtered_duplication)\n",
    "# print(filtered_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c8ba8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slideDuplication(filtered_index,filtered_duplication,filtered_section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830c75f",
   "metadata": {},
   "source": [
    "## Replace duplicated slides with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "78376167-8225-4f4f-af41-cd366f3e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation(new_pre)\n",
    "posItr = 0\n",
    "ind =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2e21be34-0498-4c06-b4e4-bd68b94e38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slide 1\n",
    "# for key,value in Scope.items():\n",
    "#     dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "#     for key1,value1 in dict.items():\n",
    "#         filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "#         if filtered_dict:\n",
    "#             promoValueSales(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "#             posItr += len(filtered_dict)\n",
    "#         ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ccad2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modified_promotionBrandsP12M.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a49aea0-21ff-4bf4-9427-d2f7a6b8a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 2\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(promotionsBrandSortedTotalFinal,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in promotionsBrandSortedTotalFinal.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            promoEvolutionNew(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "08c9a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ec6850ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 54\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1cd4fcae-6d85-4244-82e5-9a19f05b6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 3\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newpromotionsBrandsWithMarket,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newpromotionsBrandsWithMarket.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VSOD1(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6c5c8934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "92914c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 72\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b8dc777-7e02-4573-b8f9-000bb04fcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 4\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(concated,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in concated.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUpliftRetailer(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "97c4cfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "27312e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 78\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9c8ee2a2-5cdf-41f3-ad8a-95fb9baacc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 5\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionProductsP12M_volumeuplift,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionProductsP12M_volumeuplift.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VolumeUplift(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "79067da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cd0b667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 132\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "30f24f2e-b093-4fe6-8605-1ef06f69e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 6\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(new_modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in new_modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            ValueUpliftvsPromoEfficiencyQuadrant(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "64e1fc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "569c3cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 186\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ba714fd4-a9da-45d9-9eb0-1a1889324bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 7\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(new_modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in new_modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            top20(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e462a55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "477805d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 240\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "df90b497-4ca3-4467-9965-3eeba68fef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 8\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(top20clientonly,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in top20clientonly.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            top20Client(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e578ca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 294\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4108f604-685c-4fd8-aab4-5c66967a67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 9\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(bottom20clientonly,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in bottom20clientonly.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            bot20Client(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c4b098f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "16765a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 348\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "762a2079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n"
     ]
    }
   ],
   "source": [
    "if len(sectors)>0:\n",
    "    newVolumeSold(prs, sect_vsod_merged, position=posItr, parent=direct_parent['Sector'], child = 'Sector')\n",
    "    print(posItr)\n",
    "    posItr += sect_vsod_count\n",
    "    ind +=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "244a0620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 384\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d41024fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(segments)>0:\n",
    "    newVolumeSold(prs, seg_vsod_merged, position=posItr, parent=direct_parent['Segment'], child = 'Segment')\n",
    "    posItr += seg_vsod_count\n",
    "    ind +=1\n",
    "    \n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "66bc536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 456\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "92a5db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subsegments)>0:\n",
    "    newVolumeSold(prs, subseg_vsod_merged, position=posItr, parent=direct_parent['SubSegment'], child = 'SubSegment')\n",
    "    posItr += subseg_vsod_count\n",
    "    ind+=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "780b7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 456\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2298d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subcategories)>0:\n",
    "    newVolumeSold(prs, subcat_vsod_merged, position=posItr, parent=direct_parent['SubCategory'], child = 'SubCategory')\n",
    "    posItr += subcat_vsod_count\n",
    "    ind+=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5677352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0ee42400-58ae-4311-8ac7-9914c0ae666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 11\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoShare_vs_ValueShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7f7b6a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 510\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "569341f7-9b94-4bf5-9114-fe10a6d0cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 12\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newModifiedBrands,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newModifiedBrands.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoSalesTotalSize(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0b4e2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 510\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2e6331b0-3674-471a-8da0-d10719174ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Series: UNDEFINED - [0.9967600705048523, 0.12777053455019557, 0.9981306494667818, 1.0, 0, 0.9992492541291023, 0.7866666666666666, 0.9993214475784992]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.0010408307686043752, 0, 0, 0, 0.008143007579656253, 1.624991062549156e-06, 0, 7.317722192655667e-05]\n",
      "Adding Series: COUPON - [0.002199098726543327, 0.8722294654498044, 0.0018693505332182893, 0, 0.4331548719988804, 0.0007491208798351609, 0.21333333333333335, 0.0006053751995742416]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0, 0.5587021204214633, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.013289472056986681, 0.9464882943143813, 0.012998327782696065, 0, 0.8512142845088012, 0.004455171624277347, 0.0034314536133619976, 0.11538461538461539]\n",
      "Adding Series: UNDEFINED - [0.9843212426262351, 0.05351170568561873, 0.987001672217304, 1.0, 0, 0.9955448283757227, 0.996568546386638, 0.8846153846153846]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.002389285316778175, 0, 0, 0, 0.002230548101643236, 0, 0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0, 0.1465551673895555, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.015997437969896146, 0.9035639412997903, 0.024757492399015493, 0, 0.8506449130163617, 0.0042779469534577775]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.013767099263416346, 0, 0, 0, 3.317453787868735e-05, 0]\n",
      "Adding Series: UNDEFINED - [0.9702354627666875, 0.09643605870020965, 0.9752425076009845, 1.0, 0, 0.9957220530465423]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0, 0.14932191244575962, 0]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.00038440440560581307, 0, 0, 0, 0.011001932640240608, 1.8375090727010465e-06, 0, 0]\n",
      "Adding Series: COUPON - [0.0008293263033870356, 0.779236276849642, 0.0006777338087510496, 0, 0.2605634179215833, 0.00027623886392939064, 0.3333333333333333, 0.00021476961290111727]\n",
      "Adding Series: UNDEFINED - [0.9987862692910071, 0.220763723150358, 0.999322266191249, 1.0, 0, 0.9997219236269979, 0.6666666666666666, 0.9997852303870989]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0, 0.7284346494381762, 0, 0, 0]\n",
      "Adding Series: UNDEFINED - [0.9973706711097492, 0.12166301969365427, 0.9983766557296262, 0, 0.9992492541291023, 1.0, 0.7866666666666666, 0.9993214475784992]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.0010738320254609947, 0, 0, 0.011735248133531533, 1.624991062549156e-06, 0, 0, 7.317722192655667e-05]\n",
      "Adding Series: COUPON - [0.0015554968647897988, 0.8783369803063458, 0.001623344270373786, 0.2764518437668739, 0.0007491208798351609, 0, 0.21333333333333335, 0.0006053751995742416]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.7118129080995946, 0, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.022497738361832446, 0.0020233501870237924, 0, 0.7883743868144296]\n",
      "Adding Series: UNDEFINED - [0.9775022616381676, 0.9979766498129762, 1.0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.21162561318557035]\n",
      "Adding Series: COUPON - [0.009632893261119012, 0.9464882943143813, 0.0462494580141639, 0.7099135888194513, 0.004455171624277347, 0, 0.0034314536133619976, 0.11538461538461539]\n",
      "Adding Series: UNDEFINED - [0.9878649892636885, 0.05351170568561873, 0.9537505419858361, 0, 0.9955448283757227, 1.0, 0.996568546386638, 0.8846153846153846]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.0025021174751925154, 0, 0, 0.0043701572815747994, 0, 0, 0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.28571625389897387, 0, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.09071961386573059, 0.00941722184172841, 0, 0.9985206250718143]\n",
      "Adding Series: UNDEFINED - [0.9092803861342694, 0.9905827781582716, 1.0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.0014793749281856831]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.014541251892658097, 0, 0, 0, 6.703738339685126e-05, 0, 0.0009064688916357644]\n",
      "Adding Series: UNDEFINED - [0.9739658838310622, 0.0958041958041958, 0.9665741383298635, 0.9957220530465423, 0, 1.0, 0.9966048619695097]\n",
      "Adding Series: COUPON - [0.01149286427627976, 0.9041958041958041, 0.03342586167013648, 0.0042779469534577775, 0.6982345705082439, 0, 0.002488669138854553]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0, 0.30169839210835925, 0, 0]\n",
      "Adding Series: COUPON - [0.09610426811801775, 0.021862567113445867, 0, 0.9999573114067205]\n",
      "Adding Series: UNDEFINED - [0.9038957318819822, 0.9781374328865541, 1.0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 4.2688593279501923e-05]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.0003957842881643789, 0, 0, 0.014239458759771749, 1.8375090727010465e-06, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.000583233575039065, 0.7939209726443769, 0.0005350664265392947, 0.16123349774381507, 0.00027623886392939064, 0, 0.3333333333333333, 0.00021476961290111727]\n",
      "Adding Series: UNDEFINED - [0.9990209821367966, 0.2060790273556231, 0.9994649335734607, 0, 0.9997219236269979, 1.0, 0.6666666666666666, 0.9997852303870989]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.8245270434964131, 0, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.009142165076341488, 0.0007744074140982575, 0, 0.5981116107727793]\n",
      "Adding Series: UNDEFINED - [0.9908578349236585, 0.9992255925859017, 1.0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.4018883892272207]\n",
      "Adding Series: COUPON - [0.022497738361832446, 0.0020233501870237924, 0, 0.7883743868144296]\n",
      "Adding Series: UNDEFINED - [0.9775022616381676, 0.9979766498129762, 1.0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.21162561318557035]\n",
      "Adding Series: COUPON - [0.09071961386573059, 0.00941722184172841, 0, 0.9985206250718143]\n",
      "Adding Series: UNDEFINED - [0.9092803861342694, 0.9905827781582716, 1.0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.0014793749281856831]\n",
      "Adding Series: COUPON - [0.09610426811801775, 0.021862567113445867, 0, 0.9999573114067205]\n",
      "Adding Series: UNDEFINED - [0.9038957318819822, 0.9781374328865541, 1.0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 4.2688593279501923e-05]\n",
      "Adding Series: COUPON - [0.009142165076341488, 0.0007744074140982575, 0, 0.5981116107727793]\n",
      "Adding Series: UNDEFINED - [0.9908578349236585, 0.9992255925859017, 1.0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.4018883892272207]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.0021428111204734638, 0, 0, 3.423989152802364e-06, 0, 0]\n",
      "Adding Series: UNDEFINED - [0.9970230220048655, 0.48055315471045806, 0, 0.9994612923732924, 1.0, 1.0]\n",
      "Adding Series: COUPON - [0.0008341668746610318, 0.5194468452895419, 0.3818224140515888, 0.0005352836375547695, 0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0.6181775859484112, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.0022801072658978337, 1.0, 0.24248571605458644, 0.0009422748411586475, 0, 0.21818181818181817]\n",
      "Adding Series: UNDEFINED - [0.9977198927341022, 0, 0, 0.9990577251588414, 1.0, 0.7818181818181819]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0, 0, 0.01551961164291027, 0, 0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0.7419946723025033, 0, 0, 0]\n",
      "Adding Series: UNDEFINED - [0.9873745026856586, 0.25236593059936907, 0.9969242673372667, 0, 1.0, 1.0]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.0062289225769426694, 0, 0, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.006396574737398683, 0.7476340694006309, 0.00307573266273334, 0.9855218080792185, 0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0.014478191920781532, 0, 0]\n",
      "Adding Series: COUPON - [0.01180570587471319, 1.0, 0.6286919078588272, 0, 0.0057974288026803695, 0.11920529801324503]\n",
      "Adding Series: UNDEFINED - [0.9881942941252868, 0, 0, 1.0, 0.9942025711973196, 0.8807947019867549]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0, 0, 0.005658041661166784, 0, 0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0.365650050480006, 0, 0, 0]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.02991566285023114, 0, 0, 0, 0]\n",
      "Adding Series: COUPON - [0.006271436382615611, 0.5418060200668896, 0.003071926117241919, 0.9999754868916153, 0]\n",
      "Adding Series: UNDEFINED - [0.9638129007671532, 0.45819397993311034, 0.9969280738827581, 0, 1.0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 2.4513108384708723e-05, 0]\n",
      "Adding Series: UNDEFINED - [0.9835686637100186, 0, 1.0, 0.9946517564118148, 0]\n",
      "Adding Series: COUPON - [0.016431336289981353, 1.0, 0, 0.0053482435881852435, 0.5847622651260659]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0, 0, 0, 0, 9.228838273838089e-05]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0, 0, 0.41514544649119567]\n",
      "Adding Series: UNDEFINED - [0.9989073780393255, 0.6266173752310537, 0, 0.9997982849044698, 1.0]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0.0007783904569466898, 0, 0, 3.879136452503854e-06, 0]\n",
      "Adding Series: COUPON - [0.0003142315037278194, 0.3733826247689464, 0.21358365117862738, 0.0001978359590776965, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0.7864163488213726, 0, 0]\n",
      "Adding Series: COUPON - [0.000861503335714005, 1.0, 0.14446806693025707, 0.00034680318453769856, 0, 0.3333333333333333]\n",
      "Adding Series: UNDEFINED - [0.999138496664286, 0, 0, 0.9996531968154623, 1.0, 0.6666666666666666]\n",
      "Adding Series: COUPON + SPECIAL PRICE - [0, 0, 0.018801098848551568, 0, 0, 0]\n",
      "Adding Series: SPECIAL PRICE - [0, 0, 0.8367308342211913, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# slide 13\n",
    "if promo_type:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(PromoSalesTypes_data,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in PromoSalesTypes_data.items() if key in dict[key1]}\n",
    "            if filtered_dict:\n",
    "                PromoSalesTypes(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5192201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 534\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "555338fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 14\n",
    "if feature_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                featureShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b2ecb5b3-8308-43ec-b4aa-16c2e7b0797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 15\n",
    "if display_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                displayShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f16eefd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 534\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b5ffadfa-5f94-4967-84f6-54c1d2ab7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 16\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionEndOfWeek,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionEndOfWeek.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoFrequency(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b84ad318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 728\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "43caf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if categories:\n",
    "    catFinal = sorted(splitDfsPromo(dfCategory,(client_manuf) ,genrateIndexList(catGroup[0])[0]))\n",
    "    catFinal = catFinal+sorted(splitDfsPromo(dfCategory,(client_brands) ,genrateIndexList(catGroup[0])[0]))\n",
    "    catFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "56f6fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 728\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "05117abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sectors:\n",
    "    secFinal = sorted(splitDfsPromo(dfSector,(client_manuf)  ,genrateIndexList(secGroup[0])[0]))\n",
    "    secFinal = secFinal + sorted(splitDfsPromo(dfSector,(client_brands)  ,genrateIndexList(secGroup[0])[0]))\n",
    "    secFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9f5855f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 728\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c788ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if segments:\n",
    "    segFinal = sorted(splitDfsPromo(dfSegment,(client_manuf)  ,genrateIndexList(segGroup[0])[0]))\n",
    "    segFinal = segFinal+sorted(splitDfsPromo(dfSegment,(client_brands)  ,genrateIndexList(segGroup[0])[0]))\n",
    "    segFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "889d3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if subsegments:\n",
    "    subsegFinal = sorted(splitDfsPromo(dfSubSegment,(client_manuf)  ,genrateIndexList(subsegGroup[0])[0]))\n",
    "    subsegFinal = subsegFinal + sorted(splitDfsPromo(dfSubSegment,(client_brands)  ,genrateIndexList(subsegGroup[0])[0]))\n",
    "    subsegFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f3153b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if subcategories:\n",
    "    subcatFinal = sorted(splitDfsPromo(dfSubCategory,(client_manuf) ,genrateIndexList(subcatGroup[0])[0]))\n",
    "    subcatFinal = subcatFinal+sorted(splitDfsPromo(dfSubCategory,(client_brands) ,genrateIndexList(subcatGroup[0])[0]))\n",
    "    subcatFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "42f04b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 728\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5febe962-91f9-4b04-80a0-986f63399c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 75 741\n",
      "31 76 761\n",
      "0 77 792\n",
      "0 78 792\n"
     ]
    }
   ],
   "source": [
    "#Slide 17\n",
    "#split catGroup into Lists depends on num of charts \n",
    "catGroupSplit = splitListpromo(dfCategory, catGroup, [i-14 for i in index[ind]])\n",
    "promoSalesPerRetailer(prs,dfCategory,len(index[ind]),catGroupSplit,position=sum(duplication[:ind]))\n",
    "posItr = sum(duplication[:ind]) + len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split secGroup into Lists depends on num of charts \n",
    "if len(sectors) != 0: \n",
    "    secGroupSplit = splitListpromo(dfSector, secGroup, [i-14 for i in index[ind]])\n",
    "    promoSalesPerRetailer(prs,dfSector,len(index[ind]),secGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split segGroup into Lists depends on num of charts \n",
    "if len(segments) != 0: \n",
    "    segGroupSplit = splitListpromo(dfSegment, segGroup, [i-14 for i in index[ind]])\n",
    "    promoSalesPerRetailer(prs,dfSegment,len(index[ind]),segGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split subsegGroup into Lists depends on num of charts \n",
    "if len(subsegments) != 0:\n",
    "    subsegGroupSplit = splitListpromo(dfSubSegment, subsegGroup, [i-14 for i in index[ind]])\n",
    "\n",
    "    promoSalesPerRetailer(prs,dfSubSegment,len(index[ind]),subsegGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split subcatGroup into Lists depends on num of charts \n",
    "if len(subcategories) != 0:\n",
    "    subcatGroupSplit = splitListpromo(dfSubCategory, subcatGroup, [i-14 for i in index[ind]])\n",
    "    promoSalesPerRetailer(prs,dfSubCategory,len(index[ind]),subcatGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4bdf01c1-6ec8-405c-bb46-928310996eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 810\n",
      "80 844\n",
      "81 894\n",
      "82 894\n",
      "83 894\n"
     ]
    }
   ],
   "source": [
    "# slide 21\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_valueUplift,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_valueUplift.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUplift(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        print(ind,posItr)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1d1031c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 1 894\n"
     ]
    }
   ],
   "source": [
    "print(ind, duplication[ind], posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0e02f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pbg\n",
      "Pbg\n",
      "Pbg\n",
      "Pbg\n",
      "Pbg\n",
      "Pbg\n",
      "Pbg\n",
      "Pbg\n",
      "Pbg\n",
      "903 9\n"
     ]
    }
   ],
   "source": [
    "if len(categories)>0:\n",
    "    seasonality(prs,dfCategory0, len(index[ind]), categoryGroup0, position=posItr,slideby=\"Category\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fec4ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart\n",
      "Walmart Div1 Corp\n",
      "Walmart East\n",
      "Walmart Nm Corp\n",
      "Walmart North\n",
      "Walmart Sc Corp\n",
      "Walmart Southeast\n",
      "Walmart Southwest\n",
      "Walmart West\n",
      "912 18\n"
     ]
    }
   ],
   "source": [
    "if len(sectors)>0:\n",
    "    seasonality(prs, dfSector0, len(index[ind]), secGroup0, position=posItr,slideby=\"Sector\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "95c95329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart\n",
      "Walmart\n",
      "Walmart Div1 Corp\n",
      "Walmart Div1 Corp\n",
      "Walmart East\n",
      "Walmart East\n",
      "Walmart Nm Corp\n",
      "Walmart Nm Corp\n",
      "Walmart North\n",
      "Walmart North\n",
      "Walmart Sc Corp\n",
      "Walmart Sc Corp\n",
      "Walmart Southeast\n",
      "Walmart Southeast\n",
      "Walmart Southwest\n",
      "Walmart Southwest\n",
      "Walmart West\n",
      "Walmart West\n",
      "930 0 87\n"
     ]
    }
   ],
   "source": [
    "if len(segments)>0:\n",
    "    seasonality(prs, dfSegment0, len(index[ind]), segGroup0, position=posItr,slideby=\"Segment\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind]),ind)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cfffb4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930 0 88\n"
     ]
    }
   ],
   "source": [
    "if len(subsegments) != 0:\n",
    "    seasonality(prs,dfSubSegment0,len(index[ind]),subsegGroup0,position=posItr,slideby=\"SubSegment\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "\n",
    "print(posItr, len(index[ind]),ind)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8d74d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subcategories) != 0:\n",
    "    seasonality(prs,dfSubCategory0,len(index[ind]),subcatGroup0,position=posItr,slideby=\"SubCategory\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d43fdd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 89 1 930\n"
     ]
    }
   ],
   "source": [
    "print(len(index[ind]), ind, duplication[ind], posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bdb5a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930 89\n"
     ]
    }
   ],
   "source": [
    "print(posItr,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1d8b8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Promotional_Frequency(prs, data, numOfDuplicates, dfGroup, position=0):\n",
    "    \"\"\"\n",
    "    Update PowerPoint presentation with Promo Sales per Retailer data.\n",
    "\n",
    "    Parameters:\n",
    "    prs (Presentation): PowerPoint presentation object to modify.\n",
    "    endOfWeek (dict): Dictionary containing end of week data.\n",
    "    numOfDuplicates (int): Number of slides to duplicate and update.\n",
    "    dfGroup (list): List containing dataframes grouped for each slide.\n",
    "    position (int, optional): Starting slide position in the presentation. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "    Replace the slides with new data\n",
    "    \"\"\"\n",
    "    # Define dictionaries to map chart numbers to chart order\n",
    "        \n",
    "    ch1 = {0: 0}\n",
    "    ch2 = {0: 0, 1: 1}\n",
    "    ch3 = {0: 0, 1: 1, 2: 2}\n",
    "    ch4 = {0: 0, 1: 1, 2: 2, 3: 3}\n",
    "    # Iterate through each slide to update\n",
    "    i =0\n",
    "\n",
    "    for slide_num in range(numOfDuplicates):\n",
    "            # Extract dataframes for the current slide\n",
    "            dfs = dfGroup[slide_num]\n",
    "            # Extract brand and category information\n",
    "            for i in range(len(dfs)):\n",
    "                client = dfs[0].split(' | ')[0]\n",
    "                retailer_ = dfs[0].split(' | ')[1]\n",
    "                col_ = dfs[0].split(' | ')[2]\n",
    "                market= dfs[0].split(' | ')[-1]\n",
    "                # # Get shapes in the slide\n",
    "                shapes = prs.slides[slide_num + position].shapes\n",
    "                # # Find and update title shape\n",
    "                titleNumber = get_shape_number(shapes, 'Promotional Frequency Analysis | Economy | Bucegi | P12M')\n",
    "                datasourcenum = get_shape_number(shapes, \"DATA SOURCE: Trade Panel/Retailer Data | Ending Apr 2024\")\n",
    "                headerNumber = get_shape_number(shapes, 'Promotional Frequency Analysis (Replace with So What)')\n",
    "                if titleNumber is not None:\n",
    "                    shapes[datasourcenum].text = data_source\n",
    "                    shapes[titleNumber].text = shapes[titleNumber].text.replace('Economy', col_).replace('Bucegi', client + \" | \" + market)\n",
    "                    shapes[titleNumber].text_frame.paragraphs[0].font.size = Pt(12)\n",
    "                    shapes[titleNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "                    shapes[headerNumber].text_frame.paragraphs[0].font.size = Pt(16)\n",
    "                    shapes[headerNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "\n",
    "            # Create table and chart objects\n",
    "            tables, charts = createTableAndChart(shapes)\n",
    "            # Determine the appropriate chart order dictionary based on the number of charts\n",
    "            chDic = ch2 if len(charts) == 2 else ch3 if len(charts) == 3 else ch4 if len(charts) == 4 else ch1 \n",
    "\n",
    "            for chartNum in range(len(charts)):\n",
    "                print(chartNum)\n",
    "                chart = charts[chDic[chartNum]].chart\n",
    "                chart_data = CategoryChartData()\n",
    "                chart_df = data.get(dfs[chartNum])\n",
    "                if chart_df is None:\n",
    "                    print(f\"Key '{dfs[chartNum]}' not found in data! Skipping...\")\n",
    "                    continue\n",
    "\n",
    "                # Apply conditions to Value Uplift\n",
    "                \n",
    "                chart_df['Weekly VSOD'] = np.where((chart_df['VSOD']>.2)&(chart_df['Value Uplift (v. base) Normalized'] != ''),1,None)\n",
    "                chart_df['try'] = 0\n",
    "                chart_df['New Uplift'] = 0\n",
    "                chart_df['try'] = np.where((chart_df['Value Uplift (v. base) Normalized']>=2),1.8,chart_df['Value Uplift (v. base) Normalized'])\n",
    "                chart_df['New Uplift'] = np.where((chart_df['Weekly VSOD']==1)&(chart_df['Value Uplift (v. base) Normalized']>0.05),chart_df['try'],None)\n",
    "                \n",
    "                # Add series to chart\n",
    "                chart_data.categories = chart_df['End of Week'].astype(str)\n",
    "                chart_data.add_series('Weekly VSOD', chart_df['Weekly VSOD'])\n",
    "                chart_data.add_series('New Uplift', chart_df['New Uplift'])\n",
    "                # Replace chart data\n",
    "                chart.replace_data(chart_data)\n",
    "\n",
    "                # Apply formatting and data labels\n",
    "                for series_idx, series in enumerate(chart.series):\n",
    "                    if series_idx == 1:\n",
    "                        # Show data label if >200%\n",
    "                        for point_idx, point in enumerate(series.points):\n",
    "                            value = (chart_df['Value Uplift (v. base) Normalized'].astype(float).replace(np.nan,0).iloc[point_idx] * 100)\n",
    "                            data_label = point.data_label\n",
    "                            data_label.has_text_frame = False\n",
    "                                \n",
    "                            if value >=200:\n",
    "                                point.marker.format.fill.solid()\n",
    "                                point.marker.format.fill.fore_color.rgb = RGBColor(230,229,229)\n",
    "                                point.marker.format.line.color.rgb = RGBColor(230,229,229)\n",
    "                                data_label = point.data_label\n",
    "                                data_label.has_text_frame = True\n",
    "                                data_label.text_frame.text = (str(round(value) ) +\"%\") \n",
    "                                data_label.position = XL_LABEL_POSITION.CENTER\n",
    "                                paragraph = point.data_label.text_frame.paragraphs[0]\n",
    "                                run = paragraph.runs[0] if paragraph.runs else paragraph.add_run()\n",
    "                                run.font.size = Pt(8)\n",
    "                                run.font.color.rgb = RGBColor(0, 160, 151)\n",
    "    \n",
    "                    chart.replace_data(chart_data)\n",
    "\n",
    "            # Update table with retailer information\n",
    "            table = tables[0].table\n",
    "            for rowNum, row in enumerate(table.rows):\n",
    "                cell = row.cells[0]\n",
    "                cell.text = dfs[rowNum].split(' | ')[1]\n",
    "                cell.text_frame.paragraphs[0].font.name = 'Nexa Bold'\n",
    "                cell.text_frame.paragraphs[0].font.size = Pt(8)\n",
    "                cell.text_frame.paragraphs[0].alignment = PP_ALIGN.CENTER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f6b6cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "22 938\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "26 960\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0 986\n",
      "0 986\n"
     ]
    }
   ],
   "source": [
    "catGroup1Split = splitListpromo(modified_dfCategory1, catGroup1, [i-25 for i in index[ind]])\n",
    "\n",
    "Promotional_Frequency(prs,modified_dfCategory1,len(index[ind]),catGroup1Split,position=posItr)\n",
    "posItr +=len(catGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "#Sector Replace\n",
    "if len(sectors) != 0: \n",
    "    secGroup1Split = splitListpromo(modified_dfSector1, secGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSector1,len(index[ind]),secGroup1Split,position=posItr)\n",
    "    posItr += len(secGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "\n",
    "if len(segments) != 0: \n",
    "    segGroup1Split = splitListpromo(modified_dfSegment1, segGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSegment1,len(index[ind]),segGroup1Split,position=posItr)\n",
    "    posItr += len(segGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "if len(subsegments) != 0:\n",
    "    subsegGroup1Split = splitListpromo(modified_dfSubSegment1, subsegGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSubSegment1,len(index[ind]),subsegGroup1Split,position=posItr)\n",
    "    posItr += len(subsegGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "if len(subcategories) != 0:\n",
    "    subcatGroup1Split = splitListpromo(modified_dfSubCategory1, subcatGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSubCategory1,len(index[ind]),subcatGroup1Split,position=posItr)\n",
    "    posItr += len(subcatGroup1Split)\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c746a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slide 1 with no client brands\n",
    "# for key,value in Scope.items():\n",
    "#     dict = {key: count_df(promotionsBrandSortedTotalFinal,value) }\n",
    "#     for key1,value1 in dict.items():\n",
    "#         filtered_dict = {key: value for key, value in promotionsBrandSortedTotalFinal.items() if key in dict[key1]}\n",
    "#         if filtered_dict:\n",
    "#             promoEvolutionNew(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "#             posItr += len(filtered_dict)\n",
    "#         ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b39417e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 2 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(promotionsBrandNOTSortedTotalFinal,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in promotionsBrandNOTSortedTotalFinal.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            promoEvolutionNew(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "42070784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 3 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newpromotionsNotBrandsWithMarket,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newpromotionsNotBrandsWithMarket.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VSOD1(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c634a371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1058"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posItr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7e53b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 4 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(concated,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in concated.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUpliftRetailer_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9c2b0cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posItr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a734c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "995b3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 11 with no client prio\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoShare_vs_ValueShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "33d8a842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posItr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f4018019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Category': []}\n",
      "1118\n",
      "{'Sector': []}\n",
      "1118\n",
      "{'Segment': []}\n",
      "1118\n",
      "{'Subsegment': []}\n",
      "1118\n",
      "{'Subcategory': []}\n",
      "1118\n"
     ]
    }
   ],
   "source": [
    "# slide 12 with no client prio\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newModifiedBrands,value) }\n",
    "    print(dict)\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newModifiedBrands.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoSalesTotalSize_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1\n",
    "        print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7e142e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 13 with no client prio\n",
    "# if promo_type:\n",
    "#     for key,value in Scope.items():\n",
    "#         dict = {key: count_df(PromoSalesTypes_data,value) }\n",
    "#         for key1,value1 in dict.items():\n",
    "#             filtered_dict = {key: value for key, value in PromoSalesTypes_data.items() if key in dict[key1]}\n",
    "#             if filtered_dict:\n",
    "#                 PromoSalesTypes_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "#             posItr += len(filtered_dict)\n",
    "#             ind +=1\n",
    "# else:\n",
    "#     ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5f3c723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 14 with no client prio\n",
    "if feature_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                featureShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bfbef8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 15 with no client prio\n",
    "if display_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                displayShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217af932",
   "metadata": {},
   "source": [
    "## Output slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c56e0982-087b-439b-a549-736abdbb54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath=os.getcwd() + f\"\\\\Promotion {client_manuf[0]}.pptx\"\n",
    "prs.save(outputPath)\n",
    "# app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "# presentation = app.Presentations.Open(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b982c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 73: Opened Excel workbook: Book1\n",
      "Slide 74: Opened Excel workbook: Book1\n",
      "Slide 75: Opened Excel workbook: Book1\n",
      "Slide 76: Opened Excel workbook: Book1\n",
      "Slide 77: Opened Excel workbook: Book1\n",
      "Slide 78: Opened Excel workbook: Book1\n",
      "Slide 79: Opened Excel workbook: Book1\n",
      "Slide 80: Opened Excel workbook: Book1\n",
      "Slide 81: Opened Excel workbook: Book1\n",
      "Slide 82: Opened Excel workbook: Book1\n",
      "Slide 83: Opened Excel workbook: Book1\n",
      "Slide 84: Opened Excel workbook: Book1\n",
      "Slide 85: Opened Excel workbook: Book1\n",
      "Slide 86: Opened Excel workbook: Book1\n",
      "Slide 87: Opened Excel workbook: Book1\n",
      "Slide 88: Opened Excel workbook: Book1\n",
      "Slide 89: Opened Excel workbook: Book1\n",
      "Slide 90: Opened Excel workbook: Book1\n",
      "Slide 91: Opened Excel workbook: Book1\n",
      "Slide 92: Opened Excel workbook: Book1\n",
      "Slide 93: Opened Excel workbook: Book1\n",
      "Slide 94: Opened Excel workbook: Book1\n",
      "Slide 95: Opened Excel workbook: Book1\n",
      "Slide 96: Opened Excel workbook: Book1\n",
      "Slide 97: Opened Excel workbook: Book1\n",
      "Slide 98: Opened Excel workbook: Book1\n",
      "Slide 99: Opened Excel workbook: Book1\n",
      "Slide 100: Opened Excel workbook: Book1\n",
      "Slide 101: Opened Excel workbook: Book1\n",
      "Slide 102: Opened Excel workbook: Book1\n",
      "Slide 103: Opened Excel workbook: Book1\n",
      "Slide 104: Opened Excel workbook: Book1\n",
      "Slide 105: Opened Excel workbook: Book1\n",
      "Slide 106: Opened Excel workbook: Book1\n",
      "Slide 107: Opened Excel workbook: Book1\n",
      "Slide 108: Opened Excel workbook: Book1\n",
      "Slide 109: Opened Excel workbook: Book1\n",
      "Slide 110: Opened Excel workbook: Book1\n",
      "Slide 111: Opened Excel workbook: Book1\n",
      "Slide 112: Opened Excel workbook: Book1\n",
      "Slide 113: Opened Excel workbook: Book1\n",
      "Slide 114: Opened Excel workbook: Book1\n",
      "Slide 115: Opened Excel workbook: Book1\n",
      "Slide 116: Opened Excel workbook: Book1\n",
      "Slide 117: Opened Excel workbook: Book1\n",
      "Slide 118: Opened Excel workbook: Book1\n",
      "Slide 119: Opened Excel workbook: Book1\n",
      "Slide 120: Opened Excel workbook: Book1\n",
      "Slide 121: Opened Excel workbook: Book1\n",
      "Slide 122: Opened Excel workbook: Book1\n",
      "Slide 123: Opened Excel workbook: Book1\n",
      "Slide 124: Opened Excel workbook: Book1\n",
      "Slide 125: Opened Excel workbook: Book1\n",
      "Slide 126: Opened Excel workbook: Book1\n",
      "Slide 127: Opened Excel workbook: Book1\n",
      "Slide 128: Opened Excel workbook: Book1\n",
      "Slide 129: Opened Excel workbook: Book1\n",
      "Slide 130: Opened Excel workbook: Book1\n",
      "Slide 131: Opened Excel workbook: Book1\n",
      "Slide 132: Opened Excel workbook: Book1\n",
      "Slide 133: Opened Excel workbook: Book1\n",
      "Slide 134: Opened Excel workbook: Book1\n",
      "Slide 135: Opened Excel workbook: Book1\n",
      "Slide 136: Opened Excel workbook: Book1\n",
      "Slide 137: Opened Excel workbook: Book1\n",
      "Slide 138: Opened Excel workbook: Book1\n",
      "Slide 139: Opened Excel workbook: Book1\n",
      "Slide 140: Opened Excel workbook: Book1\n",
      "Slide 141: Opened Excel workbook: Book1\n",
      "Slide 142: Opened Excel workbook: Book1\n",
      "Slide 143: Opened Excel workbook: Book1\n",
      "Slide 144: Opened Excel workbook: Book1\n",
      "Slide 145: Opened Excel workbook: Book1\n",
      "Slide 146: Opened Excel workbook: Book1\n",
      "Slide 147: Opened Excel workbook: Book1\n",
      "Slide 148: Opened Excel workbook: Book1\n",
      "Slide 149: Opened Excel workbook: Book1\n",
      "Slide 150: Opened Excel workbook: Book1\n",
      "Slide 151: Opened Excel workbook: Book1\n",
      "Slide 152: Opened Excel workbook: Book1\n",
      "Slide 153: Opened Excel workbook: Book1\n",
      "Slide 154: Opened Excel workbook: Book1\n",
      "Slide 155: Opened Excel workbook: Book1\n",
      "Slide 156: Opened Excel workbook: Book1\n",
      "Slide 157: Opened Excel workbook: Book1\n",
      "Slide 158: Opened Excel workbook: Book1\n",
      "Slide 159: Opened Excel workbook: Book1\n",
      "Slide 160: Opened Excel workbook: Book1\n",
      "Slide 161: Opened Excel workbook: Book1\n",
      "Slide 162: Opened Excel workbook: Book1\n",
      "Slide 163: Opened Excel workbook: Book1\n",
      "Slide 164: Opened Excel workbook: Book1\n",
      "Slide 165: Opened Excel workbook: Book1\n",
      "Slide 166: Opened Excel workbook: Book1\n",
      "Slide 167: Opened Excel workbook: Book1\n",
      "Slide 168: Opened Excel workbook: Book1\n",
      "Slide 169: Opened Excel workbook: Book1\n",
      "Slide 170: Opened Excel workbook: Book1\n",
      "Slide 171: Opened Excel workbook: Book1\n",
      "Slide 172: Opened Excel workbook: Book1\n",
      "Slide 173: Opened Excel workbook: Book1\n",
      "Slide 174: Opened Excel workbook: Book1\n",
      "Slide 175: Opened Excel workbook: Book1\n",
      "Slide 176: Opened Excel workbook: Book1\n",
      "Slide 177: Opened Excel workbook: Book1\n",
      "Slide 178: Opened Excel workbook: Book1\n",
      "Slide 179: Opened Excel workbook: Book1\n",
      "Slide 180: Opened Excel workbook: Book1\n",
      "Slide 181: Opened Excel workbook: Book1\n",
      "Slide 182: Opened Excel workbook: Book1\n",
      "Slide 183: Opened Excel workbook: Book1\n",
      "Slide 184: Opened Excel workbook: Book1\n",
      "Slide 185: Opened Excel workbook: Book1\n",
      "Slide 186: Opened Excel workbook: Book1\n",
      "Slide 793: Opened Excel workbook: Book1\n",
      "Slide 793: Opened Excel workbook: Book1\n",
      "Slide 794: Opened Excel workbook: Book1\n",
      "Slide 794: Opened Excel workbook: Book1\n",
      "Slide 795: Opened Excel workbook: Book1\n",
      "Slide 795: Opened Excel workbook: Book1\n",
      "Slide 796: Opened Excel workbook: Book1\n",
      "Slide 796: Opened Excel workbook: Book1\n",
      "Slide 797: Opened Excel workbook: Book1\n",
      "Slide 797: Opened Excel workbook: Book1\n",
      "Slide 798: Opened Excel workbook: Book1\n",
      "Slide 798: Opened Excel workbook: Book1\n",
      "Slide 799: Opened Excel workbook: Book1\n",
      "Slide 799: Opened Excel workbook: Book1\n",
      "Slide 800: Opened Excel workbook: Book1\n",
      "Slide 800: Opened Excel workbook: Book1\n",
      "Slide 801: Opened Excel workbook: Book1\n",
      "Slide 801: Opened Excel workbook: Book1\n",
      "Slide 802: Opened Excel workbook: Book1\n",
      "Slide 802: Opened Excel workbook: Book1\n",
      "Slide 803: Opened Excel workbook: Book1\n",
      "Slide 803: Opened Excel workbook: Book1\n",
      "Slide 804: Opened Excel workbook: Book1\n",
      "Slide 804: Opened Excel workbook: Book1\n",
      "Slide 805: Opened Excel workbook: Book1\n",
      "Slide 805: Opened Excel workbook: Book1\n",
      "Slide 806: Opened Excel workbook: Book1\n",
      "Slide 806: Opened Excel workbook: Book1\n",
      "Slide 807: Opened Excel workbook: Book1\n",
      "Slide 807: Opened Excel workbook: Book1\n",
      "Slide 808: Opened Excel workbook: Book1\n",
      "Slide 808: Opened Excel workbook: Book1\n",
      "Slide 809: Opened Excel workbook: Book1\n",
      "Slide 809: Opened Excel workbook: Book1\n",
      "Slide 810: Opened Excel workbook: Book1\n",
      "Slide 810: Opened Excel workbook: Book1\n",
      "Slide 811: Opened Excel workbook: Book1\n",
      "Slide 811: Opened Excel workbook: Book1\n",
      "Slide 812: Opened Excel workbook: Book1\n",
      "Slide 812: Opened Excel workbook: Book1\n",
      "Slide 813: Opened Excel workbook: Book1\n",
      "Slide 813: Opened Excel workbook: Book1\n",
      "Slide 814: Opened Excel workbook: Book1\n",
      "Slide 814: Opened Excel workbook: Book1\n",
      "Slide 815: Opened Excel workbook: Book1\n",
      "Slide 815: Opened Excel workbook: Book1\n",
      "Slide 816: Opened Excel workbook: Book1\n",
      "Slide 816: Opened Excel workbook: Book1\n",
      "Slide 817: Opened Excel workbook: Book1\n",
      "Slide 817: Opened Excel workbook: Book1\n",
      "Slide 818: Opened Excel workbook: Book1\n",
      "Slide 818: Opened Excel workbook: Book1\n",
      "Slide 819: Opened Excel workbook: Book1\n",
      "Slide 819: Opened Excel workbook: Book1\n",
      "Slide 820: Opened Excel workbook: Book1\n",
      "Slide 820: Opened Excel workbook: Book1\n",
      "Slide 821: Opened Excel workbook: Book1\n",
      "Slide 821: Opened Excel workbook: Book1\n",
      "Slide 822: Opened Excel workbook: Book1\n",
      "Slide 822: Opened Excel workbook: Book1\n",
      "Slide 823: Opened Excel workbook: Book1\n",
      "Slide 823: Opened Excel workbook: Book1\n",
      "Slide 824: Opened Excel workbook: Book1\n",
      "Slide 824: Opened Excel workbook: Book1\n",
      "Slide 825: Opened Excel workbook: Book1\n",
      "Slide 825: Opened Excel workbook: Book1\n",
      "Slide 826: Opened Excel workbook: Book1\n",
      "Slide 826: Opened Excel workbook: Book1\n",
      "Slide 827: Opened Excel workbook: Book1\n",
      "Slide 827: Opened Excel workbook: Book1\n",
      "Slide 828: Opened Excel workbook: Book1\n",
      "Slide 828: Opened Excel workbook: Book1\n",
      "Slide 829: Opened Excel workbook: Book1\n",
      "Slide 829: Opened Excel workbook: Book1\n",
      "Slide 830: Opened Excel workbook: Book1\n",
      "Slide 830: Opened Excel workbook: Book1\n",
      "Slide 831: Opened Excel workbook: Book1\n",
      "Slide 831: Opened Excel workbook: Book1\n",
      "Slide 832: Opened Excel workbook: Book1\n",
      "Slide 832: Opened Excel workbook: Book1\n",
      "Slide 833: Opened Excel workbook: Book1\n",
      "Slide 833: Opened Excel workbook: Book1\n",
      "Slide 834: Opened Excel workbook: Book1\n",
      "Slide 834: Opened Excel workbook: Book1\n",
      "Slide 835: Opened Excel workbook: Book1\n",
      "Slide 835: Opened Excel workbook: Book1\n",
      "Slide 836: Opened Excel workbook: Book1\n",
      "Slide 836: Opened Excel workbook: Book1\n",
      "Slide 837: Opened Excel workbook: Book1\n",
      "Slide 837: Opened Excel workbook: Book1\n",
      "Slide 838: Opened Excel workbook: Book1\n",
      "Slide 838: Opened Excel workbook: Book1\n",
      "Slide 839: Opened Excel workbook: Book1\n",
      "Slide 839: Opened Excel workbook: Book1\n",
      "Slide 840: Opened Excel workbook: Book1\n",
      "Slide 840: Opened Excel workbook: Book1\n",
      "Slide 841: Opened Excel workbook: Book1\n",
      "Slide 841: Opened Excel workbook: Book1\n",
      "Slide 842: Opened Excel workbook: Book1\n",
      "Slide 842: Opened Excel workbook: Book1\n",
      "Slide 843: Opened Excel workbook: Book1\n",
      "Slide 843: Opened Excel workbook: Book1\n",
      "Slide 844: Opened Excel workbook: Book1\n",
      "Slide 844: Opened Excel workbook: Book1\n",
      "Slide 845: Opened Excel workbook: Book1\n",
      "Slide 845: Opened Excel workbook: Book1\n",
      "Slide 846: Opened Excel workbook: Book1\n",
      "Slide 846: Opened Excel workbook: Book1\n",
      "Slide 847: Opened Excel workbook: Book1\n",
      "Slide 847: Opened Excel workbook: Book1\n",
      "Slide 848: Opened Excel workbook: Book1\n",
      "Slide 848: Opened Excel workbook: Book1\n",
      "Slide 849: Opened Excel workbook: Book1\n",
      "Slide 849: Opened Excel workbook: Book1\n",
      "Slide 850: Opened Excel workbook: Book1\n",
      "Slide 850: Opened Excel workbook: Book1\n",
      "Slide 851: Opened Excel workbook: Book1\n",
      "Slide 851: Opened Excel workbook: Book1\n",
      "Slide 852: Opened Excel workbook: Book1\n",
      "Slide 852: Opened Excel workbook: Book1\n",
      "Slide 853: Opened Excel workbook: Book1\n",
      "Slide 853: Opened Excel workbook: Book1\n",
      "Slide 854: Opened Excel workbook: Book1\n",
      "Slide 854: Opened Excel workbook: Book1\n",
      "Slide 855: Opened Excel workbook: Book1\n",
      "Slide 855: Opened Excel workbook: Book1\n",
      "Slide 856: Opened Excel workbook: Book1\n",
      "Slide 856: Opened Excel workbook: Book1\n",
      "Slide 857: Opened Excel workbook: Book1\n",
      "Slide 857: Opened Excel workbook: Book1\n",
      "Slide 858: Opened Excel workbook: Book1\n",
      "Slide 858: Opened Excel workbook: Book1\n",
      "Slide 859: Opened Excel workbook: Book1\n",
      "Slide 859: Opened Excel workbook: Book1\n",
      "Slide 860: Opened Excel workbook: Book1\n",
      "Slide 860: Opened Excel workbook: Book1\n",
      "Slide 861: Opened Excel workbook: Book1\n",
      "Slide 861: Opened Excel workbook: Book1\n",
      "Slide 862: Opened Excel workbook: Book1\n",
      "Slide 862: Opened Excel workbook: Book1\n",
      "Slide 863: Opened Excel workbook: Book1\n",
      "Slide 863: Opened Excel workbook: Book1\n",
      "Slide 864: Opened Excel workbook: Book1\n",
      "Slide 864: Opened Excel workbook: Book1\n",
      "Slide 865: Opened Excel workbook: Book1\n",
      "Slide 865: Opened Excel workbook: Book1\n",
      "Slide 866: Opened Excel workbook: Book1\n",
      "Slide 866: Opened Excel workbook: Book1\n",
      "Slide 867: Opened Excel workbook: Book1\n",
      "Slide 867: Opened Excel workbook: Book1\n",
      "Slide 868: Opened Excel workbook: Book1\n",
      "Slide 868: Opened Excel workbook: Book1\n",
      "Slide 869: Opened Excel workbook: Book1\n",
      "Slide 869: Opened Excel workbook: Book1\n",
      "Slide 870: Opened Excel workbook: Book1\n",
      "Slide 870: Opened Excel workbook: Book1\n",
      "Slide 871: Opened Excel workbook: Book1\n",
      "Slide 871: Opened Excel workbook: Book1\n",
      "Slide 872: Opened Excel workbook: Book1\n",
      "Slide 872: Opened Excel workbook: Book1\n",
      "Slide 873: Opened Excel workbook: Book1\n",
      "Slide 873: Opened Excel workbook: Book1\n",
      "Slide 874: Opened Excel workbook: Book1\n",
      "Slide 874: Opened Excel workbook: Book1\n",
      "Slide 875: Opened Excel workbook: Book1\n",
      "Slide 875: Opened Excel workbook: Book1\n",
      "Slide 876: Opened Excel workbook: Book1\n",
      "Slide 876: Opened Excel workbook: Book1\n",
      "Slide 877: Opened Excel workbook: Book1\n",
      "Slide 877: Opened Excel workbook: Book1\n",
      "Slide 878: Opened Excel workbook: Book1\n",
      "Slide 878: Opened Excel workbook: Book1\n",
      "Slide 879: Opened Excel workbook: Book1\n",
      "Slide 879: Opened Excel workbook: Book1\n",
      "Slide 880: Opened Excel workbook: Book1\n",
      "Slide 880: Opened Excel workbook: Book1\n",
      "Slide 881: Opened Excel workbook: Book1\n",
      "Slide 881: Opened Excel workbook: Book1\n",
      "Slide 882: Opened Excel workbook: Book1\n",
      "Slide 882: Opened Excel workbook: Book1\n",
      "Slide 883: Opened Excel workbook: Book1\n",
      "Slide 883: Opened Excel workbook: Book1\n",
      "Slide 884: Opened Excel workbook: Book1\n",
      "Slide 884: Opened Excel workbook: Book1\n",
      "Slide 885: Opened Excel workbook: Book1\n",
      "Slide 885: Opened Excel workbook: Book1\n",
      "Slide 886: Opened Excel workbook: Book1\n",
      "Slide 886: Opened Excel workbook: Book1\n",
      "Slide 887: Opened Excel workbook: Book1\n",
      "Slide 887: Opened Excel workbook: Book1\n",
      "Slide 888: Opened Excel workbook: Book1\n",
      "Slide 888: Opened Excel workbook: Book1\n",
      "Slide 889: Opened Excel workbook: Book1\n",
      "Slide 889: Opened Excel workbook: Book1\n",
      "Slide 890: Opened Excel workbook: Book1\n",
      "Slide 890: Opened Excel workbook: Book1\n",
      "Slide 891: Opened Excel workbook: Book1\n",
      "Slide 891: Opened Excel workbook: Book1\n",
      "Slide 892: Opened Excel workbook: Book1\n",
      "Slide 892: Opened Excel workbook: Book1\n",
      "Slide 893: Opened Excel workbook: Book1\n",
      "Slide 893: Opened Excel workbook: Book1\n",
      "Slide 894: Opened Excel workbook: Book1\n",
      "Slide 894: Opened Excel workbook: Book1\n",
      "Slide 1059: Opened Excel workbook: Book1\n",
      "Slide 1060: Opened Excel workbook: Book1\n",
      "Slide 1061: Opened Excel workbook: Book1\n",
      "Slide 1062: Opened Excel workbook: Book1\n",
      "Slide 1063: Opened Excel workbook: Book1\n",
      "Slide 1064: Opened Excel workbook: Book1\n"
     ]
    }
   ],
   "source": [
    "outputPath=os.getcwd() + f\"\\\\Promotion {client_manuf[0]}.pptx\"\n",
    "# prs.save(outputPath)\n",
    "# app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "# presentation = app.Presentations.Open(outputPath)\n",
    "final=os.getcwd() +f\"\\\\Promotion {client_manuf[0]}.pptx\"\n",
    "open_chart_data_in_excel(final,outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027352d5",
   "metadata": {},
   "source": [
    "## Value Uplift by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f413d6eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\BW4SA\\\\Documents\\\\Slide-Automate\\\\Promotion Slide Duplicate/ValueUpliftvsDepth/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m datasets_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m path1\n\u001b[1;32m----> 7\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(datasets_path\u001b[38;5;241m+\u001b[39md, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\BW4SA\\\\Documents\\\\Slide-Automate\\\\Promotion Slide Duplicate/ValueUpliftvsDepth/'"
     ]
    }
   ],
   "source": [
    "%run \"..\\general_functions\\generalFunctions.ipynb\"\n",
    "%run \"..\\Promotion Slide Duplicate\\Promotion Replacement Function.ipynb\"\n",
    "\n",
    "path1 = r\"/ValueUpliftvsDepth/\"\n",
    "loaded_data = {}\n",
    "datasets_path = os.getcwd()+ path1\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        loaded_data[d.split('.')[0]] = pd.read_csv(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valueUplift_dict = {} # value\n",
    "i=0\n",
    "for key, df in loaded_data.items():\n",
    "    data = DetectHeader(df)\n",
    "    columns_to_ffill = [col for col in data.columns if 'item' in col.lower() or 'product' in col.lower()]\n",
    "    data[columns_to_ffill] = data[columns_to_ffill].fillna(method='ffill')\n",
    "    data = data[~data['Item'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "    for item in data['Item'].unique():\n",
    "        df = data[data['Item'] == item]\n",
    "        df['Discount Depth (%)'] = df['Discount Depth (%)'].str.replace('%','').astype(float) /100\n",
    "        df['Promo Price/Unit'] = df['Promo Price/Unit'].str.replace('','').astype(float)\n",
    "        if normalized:\n",
    "            df['Value Uplift (v. base) Normalized'] = df['Value Uplift (v. base) Normalized'].str.replace('%','').astype(float) /100\n",
    "        else:\n",
    "            df['Value Uplift (v. base)'] = df['Value Uplift (v. base)'].str.replace().str.replace('%','').astype(float) /100\n",
    "        df = df[df['End of Week'] != '0']\n",
    "        df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "        df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)].reset_index(drop=True)\n",
    "        if df.shape[0]>0 and not df['Discount Depth (%)'].isna().all():\n",
    "            df = df.fillna(0).reset_index(drop = True)\n",
    "            new_key = key+'_'+ item\n",
    "            valueUplift_dict[new_key] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = 'before'\n",
    "decimals = 2\n",
    "currency = ''\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending July 2024\"\n",
    "\n",
    "\n",
    "index = [20]\n",
    "duplication = [len(valueUplift_dict.keys())]\n",
    "section_names = [\"Value Uplift by product\"]\n",
    "path = os.getcwd() + '//Promotion base Oct 2024.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated value.pptx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfe1fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "com_error",
     "evalue": "(-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147024894), None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mslideDuplication\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43mduplication\u001b[49m\u001b[43m,\u001b[49m\u001b[43msection_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnew_pre\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10524\\612993193.py:23\u001b[0m, in \u001b[0;36mslideDuplication\u001b[1;34m(index, duplication, section_names, path, new_pre)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Index list not equal the Duplication number list in length\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     22\u001b[0m app \u001b[38;5;241m=\u001b[39m win32\u001b[38;5;241m.\u001b[39mDispatch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPowerPoint.Application\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m presentation \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPresentations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Iterate through the slides in the original presentation and copy them to the new presentation\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(index)):\n",
      "File \u001b[1;32m<COMObject <unknown>>:2\u001b[0m, in \u001b[0;36mOpen\u001b[1;34m(self, FileName, ReadOnly, Untitled, WithWindow)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352567, 'Exception occurred.', (0, None, None, None, 0, -2147024894), None)"
     ]
    }
   ],
   "source": [
    "# slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddec53",
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at 'c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate//slide duplicated value.pptx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prs \u001b[38;5;241m=\u001b[39m \u001b[43mPresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pre\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\api.py:31\u001b[0m, in \u001b[0;36mPresentation\u001b[1;34m(pptx)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pptx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     pptx \u001b[38;5;241m=\u001b[39m _default_pptx_path()\n\u001b[1;32m---> 31\u001b[0m presentation_part \u001b[38;5;241m=\u001b[39m \u001b[43mPackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpptx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmain_document_part\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pptx_package(presentation_part):\n\u001b[0;32m     34\u001b[0m     tmpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a PowerPoint file, content type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\package.py:82\u001b[0m, in \u001b[0;36mOpcPackage.open\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pkg_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m IO[\u001b[38;5;28mbytes\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\package.py:160\u001b[0m, in \u001b[0;36mOpcPackage._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the package after loading all parts and relationships.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m     pkg_xml_rels, parts \u001b[38;5;241m=\u001b[39m \u001b[43m_PackageLoader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pkg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPackage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rels\u001b[38;5;241m.\u001b[39mload_from_xml(PACKAGE_URI, pkg_xml_rels, parts)\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\package.py:190\u001b[0m, in \u001b[0;36m_PackageLoader.load\u001b[1;34m(cls, pkg_file, package)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mcls\u001b[39m, pkg_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m IO[\u001b[38;5;28mbytes\u001b[39m], package: Package\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[CT_Relationships, \u001b[38;5;28mdict\u001b[39m[PackURI, Part]]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return (pkg_xml_rels, parts) pair resulting from loading `pkg_file`.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    The returned `parts` value is a {partname: part} mapping with each part in the package\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    those relationships into its |_Relationships| object.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\package.py:194\u001b[0m, in \u001b[0;36m_PackageLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[CT_Relationships, \u001b[38;5;28mdict\u001b[39m[PackURI, Part]]:\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return (pkg_xml_rels, parts) pair resulting from loading pkg_file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m     parts, xml_rels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parts\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xml_rels\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m partname, part \u001b[38;5;129;01min\u001b[39;00m parts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    197\u001b[0m         part\u001b[38;5;241m.\u001b[39mload_rels_from_xml(xml_rels[partname], parts)\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\util.py:191\u001b[0m, in \u001b[0;36mlazyproperty.__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    186\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# --- on first access, the __dict__ item will be absent. Evaluate fget()\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# --- and store that value in the (otherwise unused) host-object\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# --- __dict__ value of same name ('fget' nominally)\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(_T, value)\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\package.py:222\u001b[0m, in \u001b[0;36m_PackageLoader._parts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;129m@lazyproperty\u001b[39m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_parts\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[PackURI, Part]:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"dict {partname: Part} populated with parts loading from package.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Among other duties, this collection is passed to each relationships collection so each\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    relationship can resolve a reference to its target part when required. This reference can\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    only be reliably carried out once the all parts have been loaded.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     content_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_content_types\u001b[49m\n\u001b[0;32m    223\u001b[0m     package \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_package\n\u001b[0;32m    224\u001b[0m     package_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_package_reader\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\util.py:191\u001b[0m, in \u001b[0;36mlazyproperty.__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    186\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# --- on first access, the __dict__ item will be absent. Evaluate fget()\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# --- and store that value in the (otherwise unused) host-object\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# --- __dict__ value of same name ('fget' nominally)\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(_T, value)\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\package.py:207\u001b[0m, in \u001b[0;36m_PackageLoader._content_types\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;129m@lazyproperty\u001b[39m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_content_types\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ContentTypeMap:\n\u001b[0;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"|_ContentTypeMap| object providing content-types for items of this package.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    Provides a content-type (MIME-type) for any given partname.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ContentTypeMap\u001b[38;5;241m.\u001b[39mfrom_xml(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_package_reader\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCONTENT_TYPES_URI\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\serialized.py:38\u001b[0m, in \u001b[0;36mPackageReader.__getitem__\u001b[1;34m(self, pack_uri)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pack_uri: PackURI) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return bytes for part corresponding to `pack_uri`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blob_reader\u001b[49m[pack_uri]\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\util.py:191\u001b[0m, in \u001b[0;36mlazyproperty.__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    186\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# --- on first access, the __dict__ item will be absent. Evaluate fget()\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# --- and store that value in the (otherwise unused) host-object\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# --- __dict__ value of same name ('fget' nominally)\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(_T, value)\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\serialized.py:52\u001b[0m, in \u001b[0;36mPackageReader._blob_reader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;129m@lazyproperty\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_blob_reader\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _PhysPkgReader:\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"|_PhysPkgReader| subtype providing read access to the package file.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_PhysPkgReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pkg_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BW4SA\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pptx\\opc\\serialized.py:144\u001b[0m, in \u001b[0;36m_PhysPkgReader.factory\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mis_zipfile(pkg_file):\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ZipPkgReader(pkg_file)\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage not found at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m pkg_file)\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: Package not found at 'c:\\Users\\BW4SA\\Documents\\Slide-Automate\\Promotion Slide Duplicate//slide duplicated value.pptx'"
     ]
    }
   ],
   "source": [
    "prs = Presentation(new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each key-slide_num pair in modified_valueUplift\n",
    "for key, slide_num in zip(valueUplift_dict, range(len(valueUplift_dict.keys()))):\n",
    "        # Access the slide to be modified\n",
    "        slide = prs.slides[slide_num]\n",
    "        \n",
    "        # Extract data for the current key\n",
    "        df = valueUplift_dict[key]\n",
    "        #df = df[df['Value Uplift (v. base) Normalized'] !=0 ]\n",
    "        # Get shapes in the slide\n",
    "        shapes = slide.shapes\n",
    "        \n",
    "        # Find and update title shape\n",
    "        titleNumber = get_shape_number(shapes, \"Value Uplift vs discount depth | By Event | Category/Sector | Brand | Coop Alleanza | P12M\")\n",
    "        datasourcenum = get_shape_number(shapes, \"Data Source | Trade Panel\")\n",
    "        headerNumber = get_shape_number(shapes, 'Value Uplift vs discount depth (Replace With SO WHAT)')\n",
    "        if titleNumber is not None:\n",
    "            shapes[datasourcenum].text = data_source\n",
    "            shapes[titleNumber].text = shapes[titleNumber].text.replace('Category/Sector', key.split('_')[2]) \\\n",
    "                .replace('Brand | Coop Alleanza ', df['Item'][0])\n",
    "            shapes[titleNumber].text_frame.paragraphs[0].font.size = Pt(12)\n",
    "            shapes[titleNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "            shapes[headerNumber].text_frame.paragraphs[0].font.size = Pt(16)\n",
    "            shapes[headerNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "\n",
    "        # Create table and chart objects\n",
    "        tables, charts = createTableAndChart(slide.shapes)\n",
    "        chart1 = charts[0].chart  # First chart\n",
    "        chart2 = charts[1].chart  # Second chart\n",
    "        \n",
    "        # Extract data for charts\n",
    "        category = df['Item'].tolist()\n",
    "        x_values_discount = df['Discount Depth (%)'].tolist()\n",
    "        x_values_price = df['Promo Price/Unit'].tolist()\n",
    "        if normalized:\n",
    "            y_values = df['Value Uplift (v. base) Normalized'].tolist()\n",
    "        else:\n",
    "            y_values = df['Value Uplift (v. base)'].tolist()\n",
    "\n",
    "        \n",
    "        x_values_discount = [mround_numpy(value, 0.05) for value in x_values_discount]\n",
    "        x_values_price = [mround_numpy(value, 0.5) for value in x_values_price]\n",
    "        #Update first chart with Discount Depth vs Value Uplift data\n",
    "        chart_data1 = XyChartData()\n",
    "        series1 = chart_data1.add_series('Scatter')\n",
    "        for i in range(len(category)):\n",
    "            series1.add_data_point(x_values_discount[i], y_values[i])\n",
    "        chart1.replace_data(chart_data1)\n",
    "        \n",
    "        # Access the X-axis\n",
    "        \n",
    "        xlsx_file = BytesIO()\n",
    "        with chart_data1._workbook_writer._open_worksheet(xlsx_file) as (workbook, worksheet):\n",
    "            chart_data1._workbook_writer._populate_worksheet(workbook, worksheet)\n",
    "            worksheet.write(0, 4, \"Item\")\n",
    "            worksheet.write_column(1, 4, df['Item'].to_list(), None)\n",
    "            worksheet.write(0, 5, \"End of Week\")\n",
    "            worksheet.write_column(1, 5, df['End of Week'].to_list(), None)\n",
    "\n",
    "        chart1._workbook.update_from_xlsx_blob(xlsx_file.getvalue())\n",
    "\n",
    "        # Update second chart with Promo Price/Unit vs Value Uplift data\n",
    "        chart_data2 = XyChartData()\n",
    "        series2 = chart_data2.add_series('Scatter')\n",
    "        for i in range(len(category)):\n",
    "            series2.add_data_point(x_values_price[i], y_values[i])\n",
    "        chart2.replace_data(chart_data2)\n",
    "        \n",
    "        x_axis = chart2.category_axis\n",
    "        \n",
    "        # Loop through each X-axis category label and format as currency\n",
    "        if sign.lower() == 'before':\n",
    "            x_axis.tick_labels.number_format = f'\"{currency}\"#,##0.00'  if decimals == 2 else f'\"{currency}\"#,##0'\n",
    "        else:\n",
    "            x_axis.tick_labels.number_format = f'#,##0.00\"{currency}\"'  if decimals == 2 else f'#,##0\"{currency}\"'\n",
    "       \n",
    "        #x_axis.has_major_gridlines = False  # Optional: remove gridlines\n",
    "\n",
    "        xlsx_file = BytesIO()\n",
    "        with chart_data2._workbook_writer._open_worksheet(xlsx_file) as (workbook, worksheet):\n",
    "            chart_data2._workbook_writer._populate_worksheet(workbook, worksheet)\n",
    "            worksheet.write(0, 4, \"Item\")\n",
    "            worksheet.write_column(1, 4, df['Item'].to_list(), None)\n",
    "            worksheet.write(0, 5, \"End of Week\")\n",
    "            worksheet.write_column(1, 5, df['End of Week'].to_list(), None)\n",
    "        chart2._workbook.update_from_xlsx_blob(xlsx_file.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath=os.getcwd() + \"\\\\Promotion EdgeWell ValueUplift.pptx\"\n",
    "prs.save(outputPath)\n",
    "app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "presentation = app.Presentations.Open(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623751f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
