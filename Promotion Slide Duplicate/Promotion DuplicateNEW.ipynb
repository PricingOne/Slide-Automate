{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a454f5-c4e1-460c-a27d-40aa00dfcac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"..\\general_functions\\generalFunctions.ipynb\"\n",
    "%run \"..\\Promotion Slide Duplicate\\Promotion Replacement Function.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999d128",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b6fbff-4bd3-4c15-8d5d-f3b65886bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = True\n",
    "promo_type = True\n",
    "display_share = False  # True if Available\n",
    "feature_share = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ff0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ManufOrTopC =\"Top Companies\"\n",
    "BrandOrTopB = \"Top Brands\"\n",
    "prodORitem=\"SKU\"\n",
    "\n",
    "client_manuf = [\"Edgewell Personal Care\"]\n",
    "client_brands = [\"Schick\", \"Equate\", \"Cremo\"]\n",
    "\n",
    "categories = [\"Manual Shave Men\"]\n",
    "sectors = [\"System\",\"Disposables\"]\n",
    "segments = [\"Razors\", \"Refills\", \"Disposables\"]\n",
    "subsegments= []\n",
    "subcategories= []\n",
    "\n",
    "decimals = 2\n",
    "sign = \"Before\"\n",
    "currency = '$'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    "\n",
    "customareas=''\n",
    "national = False\n",
    "areas = [\"RETAILER\"]\n",
    "regions_RET  =[\"Bj's And Sam's\",\"Walmart\"]\n",
    "channels_RET = []\n",
    "market_RET = []\n",
    "\n",
    "regions_CHAN = []\n",
    "channels_CHAN = []\n",
    "market_CHAN = []\n",
    "\n",
    "regions_CUST = []\n",
    "channels_CUST = []\n",
    "market_CUST = []\n",
    "\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending March  2025\"\n",
    "years = ['2023', '2024', '2025']\n",
    "\n",
    "subcatg_parent = \"Segment\"\n",
    "subcatg_parent_list = segments\n",
    " \n",
    "percent = 1000000\n",
    "percentstr=\"'000 000\"\n",
    "start_date = \"2022-04-01\"\t\n",
    "end_date = \"2025-04-01\"\n",
    "prodORitem = \"SKU\"\n",
    "\n",
    "promo_col = []\n",
    "selectedBrands = client_brands \n",
    "marketList = regions_RET + channels_RET + market_RET + regions_CHAN + channels_CHAN + market_CHAN \n",
    "notInScope = []\n",
    "OpenEditData=True\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d93407ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketList = regions_RET + channels_RET + market_RET + regions_CHAN + channels_CHAN + market_CHAN + regions_CUST + channels_CUST + market_CUST\n",
    "categoryList=categories +sectors+segments+subsegments+subcategories\n",
    "defaults = {\n",
    "    'Manual Shave Men': 6.69,\n",
    "    'Disposables': 7.98,\n",
    "    \"System\" : 5.03,\n",
    "    \"Razors\" : 2.80,\n",
    "    \"Refills\" : 6.58\n",
    "}\n",
    "diff_market_value = {\n",
    "\n",
    "}\n",
    "\n",
    "def totalsize (lis,defaultdic,diffmarketdic=[]):\n",
    "\n",
    "    max_total_size = {\n",
    "    f\"{category} | {market}\": diff_market_value.get(market.upper(), {}).get(category, defaults[category])\n",
    "    for market in lis\n",
    "    for category in defaults\n",
    "    }\n",
    "\n",
    "    return max_total_size  \n",
    "\n",
    "max_total_size=totalsize(marketList,defaults,diff_market_value)\n",
    "\n",
    "custom_colors = [\n",
    "    RGBColor(91, 159, 153),    # Darker teal\n",
    "    RGBColor(131, 199, 193),   # Brighter medium teal\n",
    "    RGBColor(168, 216, 212),   # Original light teal\n",
    "    RGBColor(198, 236, 232),   # Very light teal\n",
    "    RGBColor(111, 179, 173),\n",
    "    RGBColor(121, 189, 183)\n",
    "]\n",
    "# custom_colors = [\n",
    "#     RGBColor(111, 179, 173),  \n",
    "#     RGBColor(121, 189, 183),  \n",
    "#     RGBColor(168, 216, 212),  \n",
    "#     RGBColor(178, 226, 222),\n",
    "# ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1d5a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Manual Shave Men | Bj's And Sam's\": 6.69,\n",
       " \"Disposables | Bj's And Sam's\": 7.98,\n",
       " \"System | Bj's And Sam's\": 5.03,\n",
       " \"Razors | Bj's And Sam's\": 2.8,\n",
       " \"Refills | Bj's And Sam's\": 6.58,\n",
       " 'Manual Shave Men | Walmart': 6.69,\n",
       " 'Disposables | Walmart': 7.98,\n",
       " 'System | Walmart': 5.03,\n",
       " 'Razors | Walmart': 2.8,\n",
       " 'Refills | Walmart': 6.58}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8ab5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Scope = {\n",
    "    \"Category\": categories,\n",
    "    \"Sector\": sectors,\n",
    "    \"Segment\": segments,\n",
    "    \"Subsegment\": subsegments,\n",
    "    \"Subcategory\": subcategories\n",
    "}\n",
    "suffixes = [\"Category\", \"Sector\", \"Segment\",'SubSegment', 'SubCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0373cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectHeader(df):\n",
    "  # df = df.replace( np.nan, 0)\n",
    "  vals=df.values\n",
    "  vals=list(map(lambda x : all([type(i)==str for i in x ]),vals))\n",
    "  # print(vals)\n",
    "  break_point=0\n",
    "  for i,v in enumerate(vals):\n",
    "    if v:\n",
    "      break_point=i\n",
    "      # print(break_point)\n",
    "      break  \n",
    "  df.columns=df.iloc[break_point]\n",
    "  df=df.iloc[break_point+1:,:]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71fd5a3",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461dc096-382d-4c3b-91c1-8610a7cd684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "datasets_path = os.getcwd()+\"/Promotion Datasets NewEX/\"\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e341f",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d9ab22-4ebf-4b74-8859-4cad1e38a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningData(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "      \n",
    "        # df=DetectHeader(data[key])\n",
    "        df =data[key].copy()\n",
    "        # Set column names and skip the first row\n",
    "        if  df.columns.isna().any():\n",
    "            continue\n",
    "        # Perform specific cleaning operations based on the DataFrame columns and key\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if 'Top Brands' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df[f'{prodORitem}'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df.reset_index(drop=True)\n",
    "            \n",
    "            elif 'Top Brands' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                #df.fillna(0, inplace=True)\n",
    "                if normalized:\n",
    "                    df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])] = df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift Normalized IYA'].isna(), None, df['Value Uplift Normalized IYA'] - 1)\n",
    "                else:\n",
    "                    df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])] = df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift IYA'].isna(), None, df['Value Uplift IYA'] - 1)\n",
    "               \n",
    "                df['VSOD Evaluation vs YA'] = np.where(df['VSOD IYA'].isna(), None, df['VSOD IYA'] - 1)\n",
    "\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                df = df[~df['Top Brands'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "\n",
    "       \n",
    "                \n",
    "            elif 'End of Week' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df[f'{prodORitem}'] = df[f'{prodORitem}'].fillna(method='ffill')\n",
    "                if normalized:\n",
    "                    df = df[(df['Value Uplift (v. base) Normalized'] >= 0)]\n",
    "                else:\n",
    "                    df = df[(df['Value Uplift (v. base)'] >= 0)]\n",
    "                    df['End of Week'] = pd.to_datetime(df['End of Week'], errors='coerce')\n",
    "                    df = df[df['End of Week'].dt.year.isin([2023, 2024])]\n",
    "\n",
    "                # df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                # df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                new_start = \"2023-07-31\"\n",
    "                df = df[(df['End of Week'] >= new_start) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df[f'{prodORitem}'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df = df[df['Promo Sales'] > 1000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                    df =  df[df['Value Uplift (v. base) Normalized']<10]\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                    df = df[df['Value Uplift (v. base']<10]\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-22|-23|-24|Jan-25')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                print(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                # df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                if 'Sector' == df.columns[1]:\n",
    "                    df[direct_parent[\"Sector\"]].fillna(method='ffill', inplace= True)\n",
    "                    df['Sector'] = df['Sector'].replace(0, np.nan)\n",
    "                    df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                    df['Sector'] = df.apply(lambda row: row[direct_parent[\"Sector\"]] if 'Total' in row[direct_parent[\"Sector\"]] and row[direct_parent[\"Sector\"]] != categories[0] else row['Sector'], axis=1)\n",
    "\n",
    "                elif 'Segment' == df.columns[1]:\n",
    "                    df['Segment'] = df['Segment'].replace(0, np.nan)  \n",
    "                    df[direct_parent[\"Segment\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['Segment'] = df.apply(lambda row: row[direct_parent[\"Segment\"]] if 'Total' in row[direct_parent[\"Segment\"]] and row[direct_parent[\"Segment\"]] != categories[0] else row['Segment'], axis=1)\n",
    "                    df['Segment'].fillna(method='ffill', inplace=True)\n",
    "                elif 'SubSegment' == df.columns[1]:\n",
    "                    df['SubSegment'] = df['SubSegment'].replace(0, np.nan)\n",
    "                    df[direct_parent[\"SubSegment\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['SubSegment'] = df.apply(lambda row: row[direct_parent[\"SubSegment\"]] if 'Total' in row[direct_parent[\"SubSegment\"]] and row[direct_parent[\"SubSegment\"]] != categories[0] else row['SubSegment'], axis=1)\n",
    "                    df['SubSegment'].fillna(method='ffill', inplace=True)\n",
    "                elif 'SubCategory' == df.columns[1]:\n",
    "                    df['SubCategory'] = df['SubCategory'].replace(0, np.nan)\n",
    "                    df[direct_parent[\"SubCategory\"]].fillna(method='ffill', inplace= True)          \n",
    "                    df['SubCategory'] = df.apply(lambda row: row[direct_parent[\"SubCategory\"]] if 'Total' in row[direct_parent[\"SubCategory\"]] and row[direct_parent[\"SubCategory\"]] != categories[0] else row['SubCategory'], axis=1)\n",
    "                    df['SubCategory'].fillna(method='ffill', inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "\n",
    "        if df.shape[0] > 0:\n",
    "            cleaned_data[key] = df\n",
    "\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df05b63b-986d-473b-9a40-ef7798a932ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningdata_with_grand_total(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    cleaningdata_with_grand_total = {}\n",
    "    \n",
    "    # Iterate over each key-value pair in the input dictionary\n",
    "    for key in data:\n",
    "   \n",
    "        # df=DetectHeader(data[key])\n",
    "        df =data[key].copy()\n",
    "\n",
    "        if df.shape[0] > 0 and not 'National' in key:\n",
    "            if 'Top Brands' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                df[f'{prodORitem}'].fillna('', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "            \n",
    "            elif 'Top Brands' in df.columns:\n",
    "                df['Top Brands'] = df['Top Brands'].fillna(method='ffill')\n",
    "                if normalized:\n",
    "                    df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])] = df.loc[:,~ df.columns.isin(['VSOD IYA','Value Uplift Normalized IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift Normalized IYA'].isna(), None, df['Value Uplift Normalized IYA'] - 1)\n",
    "                else:\n",
    "                    df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])] = df.loc[:, df.columns.isin(['VSOD IYA','Value Uplift IYA'])].fillna(0)\n",
    "                    df['Promo Value Uplift vs YA'] = np.where(df['Value Uplift IYA'].isna(), None, df['Value Uplift IYA'] - 1)\n",
    "               \n",
    "                df['VSOD Evaluation vs YA'] = np.where(df['VSOD IYA'].isna(), None, df['VSOD IYA'] - 1)\n",
    "                df['Top Brands'] = df['Top Brands'].apply(lambda x: 'Grand Total' if 'Grand Total' in x else x.replace('Total', '').strip())\n",
    "                #df = df[~df['Top Brands'].str.contains('Total', case=False)]\n",
    "                df = df[df['Total Size'] == 0].reset_index(drop=True)\n",
    "\n",
    "            elif 'End of Week' in df.columns and f'{prodORitem}' in df.columns:\n",
    "                df[f'{prodORitem}'] = df[f'{prodORitem}'].fillna(method='ffill')\n",
    "                df = df[(df['End of Week'].str.contains('2023|2024')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df[~df[f'{prodORitem}'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                \n",
    "                df = df[df['Promo Sales'] > 1000]\n",
    "                if normalized:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base) Normalized'])\n",
    "                else:\n",
    "                    df = df.dropna(subset=['Value Uplift (v. base)'])\n",
    "                df.fillna(0, inplace=True)\n",
    "                df = df.reset_index(drop=True)\n",
    "                \n",
    "            elif 'End of Week' in df.columns:\n",
    "                df['End of Week'] = df['End of Week'].astype(str)\n",
    "                df = df[~df['End of Week'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df['End of Week'] = df['End of Week'].dt.strftime(\"%d-%b-%y\")\n",
    "                df = df[(df['End of Week'].str.contains('-21|-22|-23|Jan-24')) & (df['End of Week'].notna())]\n",
    "                df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "                df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)]\n",
    "                df = df.dropna()\n",
    "                \n",
    "            elif 'Grand Total' in df.columns:\n",
    "                df['Sector'].fillna(method='ffill', inplace=True)\n",
    "                df.fillna(0, inplace=True)\n",
    "            \n",
    "            # Check if the key matches specific categories and modify the key accordingly\n",
    "            if key.split(' | ')[0] in categories and len(key.split(' | ')) == 3:\n",
    "                modified_key = key.split(' | ')[1] + ' | ' + key.split(' | ')[2] + ' | ' + key.split(' | ')[0]\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[modified_key] = df\n",
    "            else:\n",
    "                if df.shape[0] > 0:\n",
    "                    cleaningdata_with_grand_total[key] = df\n",
    "    \n",
    "    return cleaningdata_with_grand_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7a1182-ecdd-40a8-8d9e-a110ad873ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VSOD_Clean(VSOD_Data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess VSOD data in a dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - VSOD_Data (dict): Dictionary containing VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned VSOD DataFrames.\n",
    "    \"\"\"\n",
    "    VSOD_cleaned = {}\n",
    "    for key in VSOD_Data:\n",
    "        # df=DetectHeader(VSOD_Data[key])\n",
    "        df =VSOD_Data[key].copy()\n",
    "\n",
    "        # print(key)\n",
    "        if 'Sector' == df.columns[1]:\n",
    "            df[direct_parent[\"Sector\"]] = df[direct_parent[\"Sector\"]].replace(0, np.nan)\n",
    "            df[direct_parent[\"Sector\"]].fillna(method='ffill', inplace = True)\n",
    "            df['Sector'] = df['Sector'].replace(0, np.nan)\n",
    "            df['Sector'] = df.apply(lambda row: row[direct_parent[\"Sector\"]] if 'Total' in row[direct_parent[\"Sector\"]] and row[direct_parent[\"Sector\"]] != categories[0] else row['Sector'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"Sector\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"Sector\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"Sector\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['Sector'].fillna(method='ffill', inplace=True)\n",
    "        elif 'Segment' == df.columns[1]:\n",
    "            df[direct_parent[\"Segment\"]].fillna(method='ffill', inplace=True)\n",
    "            df['Segment'] = df['Segment'].replace(0, np.nan)\n",
    "            # df=df[~df['Sector'].str.contains(r'\\btotal\\b', case=False) | df['Sector'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['Segment'] = df.apply(lambda row: row[direct_parent[\"Segment\"]] if 'Total' in row[direct_parent[\"Segment\"]] and row[direct_parent[\"Segment\"]] != categories[0] else row['Segment'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"Segment\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"Segment\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"Segment\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['Segment'].fillna(method='ffill', inplace=True)    \n",
    "        elif 'SubSegment' == df.columns[1]:\n",
    "            df['SubSegment'] = df['SubSegment'].replace(0, np.nan)\n",
    "            df[direct_parent[\"SubSegment\"]].fillna(method='ffill', inplace=True)\n",
    "            # df=df[~df['Segment'].str.contains(r'\\btotal\\b', case=False) | df['Segment'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubSegment'] = df.apply(lambda row: row[direct_parent[\"SubSegment\"]] if 'Total' in row[direct_parent[\"SubSegment\"]] and row[direct_parent[\"SubSegment\"]] != categories[0] else row['SubSegment'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"SubSegment\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"SubSegment\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"SubSegment\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubSegment'].fillna(method='ffill', inplace=True)\n",
    "        elif 'SubCategory' == df.columns[1]:\n",
    "            df['SubCategory'] = df['SubCategory'].replace(0, np.nan)\n",
    "            df[direct_parent[\"SubCategory\"]].fillna(method='ffill', inplace=True)\n",
    "            # df=df[~df['Segment'].str.contains(r'\\btotal\\b', case=False) | df['Segment'].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubCategory'] = df.apply(lambda row: row[direct_parent[\"SubCategory\"]] if 'Total' in row[direct_parent[\"SubCategory\"]] and row[direct_parent[\"SubCategory\"]] != categories[0] else row['SubCategory'], axis=1)\n",
    "            df = df[~(df[direct_parent[\"SubCategory\"]].str.contains(r'\\btotal\\b', case=False) & \n",
    "                    (df[direct_parent[\"SubCategory\"]] != categories[0])) | \n",
    "                    df[direct_parent[\"SubCategory\"]].str.contains(r'\\bGrand\\b', case=False)].reset_index(drop=True)\n",
    "            df['SubCategory'].fillna(method='ffill', inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] > 0:\n",
    "            VSOD_cleaned[key] = df\n",
    "    return VSOD_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d32522c1-88ce-4f84-b71c-4e777b408673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(VSODClient_Cleaned, VSODCleaned, col):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of DataFrames based on a common column.\n",
    "\n",
    "    Parameters:\n",
    "    - VSODClient_Cleaned (dict): Dictionary containing cleaned VSOD client DataFrames.\n",
    "    - VSODCleaned (dict): Dictionary containing cleaned VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing merged DataFrames.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for key in VSODClient_Cleaned:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        #merged_df = pd.merge(VSODCleaned[key], VSODClient_Cleaned[key], on=col, how='left')\n",
    "        merged_df = pd.merge(VSODClient_Cleaned[key],VSODCleaned[key], on=col, how='left')\n",
    "        #merged_df = merged_df.dropna(subset=['Grand Total'])\n",
    "        merged_df['Grand Total'] = merged_df['Grand Total'].fillna(0)\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            merged_dict[key] = merged_df     \n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7caef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified_promotionBrandsP12M_display = cleaningData(display_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1c2b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionBrandsP12M = cleaningData(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d32fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionBrandsP12M_total = cleaningdata_with_grand_total(promotions_brands_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ad3da6e-fb47-4873-b875-bd146d89d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionProductsP12M = cleaningData(promotions_products_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5159e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_promotionProductsP12M_updated = {}\n",
    "for key, df in modified_promotionProductsP12M.items():\n",
    "    df = df.copy()\n",
    "    df = df[df[f'{prodORitem}'] != '']\n",
    "    df = df[df['Promo Sales'] >= 10000]\n",
    "    df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "    if not df.empty:\n",
    "        modified_promotionProductsP12M_updated[key] = df\n",
    "modified_promotionProductsP12M_volumeuplift = modified_promotionProductsP12M_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b40a1e66-80f4-4dd0-9af8-a1a4e1cdc795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "modified_promotionEndOfWeek = cleaningData(promotions_EndOfWeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c86f6a2a-90f7-40b1-8f9f-723f5578455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_valueUplift = cleaningData(value_uplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3253f1fa-b573-493b-b3c6-d825cee538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning13New(data, size_fix=1):\n",
    "    \"\"\"\n",
    "    Clean and process data for specific brands and regions.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary containing raw data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing cleaned and processed data.\n",
    "    \"\"\"\n",
    "    data_cleaned = {}\n",
    "    \n",
    "    # Define maximum total size for each combination of product type and region\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        # Skip processing if the region is 'NATIONAL' or 'National'\n",
    "        if 'NATIONAL' in areas or 'National' in key:\n",
    "            continue\n",
    "        \n",
    "        new_data = []\n",
    "\n",
    "        df=data[key].copy()\n",
    "\n",
    "        df['Top Brands'].fillna(method='ffill', inplace=True)\n",
    "        \n",
    "        # Filter out rows where 'Top Brands' is 'Grand Total' or 'Other'\n",
    "        df = df[(df['Top Brands'] != 'Grand Total') & (df['Top Brands'] != 'Other')]\n",
    "        # Remove 'GR' suffix from 'Total Size' and convert it to integer\n",
    "        df['Total Size'] = df['Total Size'].str.extract('(\\d+)', expand=False)\n",
    "        df.fillna('0',inplace=True)\n",
    "        df['Total Size'] = df['Total Size'].astype(int)\n",
    "        df = df[df['Value Share'] > 0.01]\n",
    "        \n",
    "        # Sort data by 'Value Share' in descending order\n",
    "        df = df.sort_values(by='Value Share', ascending=False).reset_index(drop=True)\n",
    "        for i, brand in enumerate(df['Top Brands'].unique()):\n",
    "            # Determine the product key based on the first two elements of the key\n",
    "\n",
    "            product_key = key.split('|')[0] + '|' + key.split('|')[1]\n",
    "            max_size = max_total_size.get(product_key, None)            \n",
    "            # Filter rows for the current brand and check if total size is within the maximum allowed size\n",
    "            if max_size is not None:\n",
    "                \n",
    "                brand_df = df[(df['Top Brands'] == brand) & (df['Total Size'] <= max_size*size_fix)]\n",
    "            else:\n",
    "                brand_df = pd.DataFrame()\n",
    "                \n",
    "            # Calculate recruitment ratio if the brand has data and total size is greater than zero\n",
    "            #brand_total = df[(df['Top Brands'] == brand + ' Total')]['Promo Value'].values\n",
    "            brand_total = df[(df['Top Brands'].str.strip() == (brand + ' Total').strip())]['Promo Value'].values\n",
    "            \n",
    "            \n",
    "            if not brand_df.empty and brand_total.size > 0 and brand_total[0] > 0:\n",
    "                \n",
    "                brand_sum = brand_df['Promo Value'].sum() / brand_total[0]\n",
    "     \n",
    "                new_data.append({'Top Brands': brand, 'Recruitment': brand_sum, 'Consumption': 1 - brand_sum, 'Value Share': df['Value Share'][i], 'SUM':brand_df['Promo Value'].sum()})\n",
    "        \n",
    "        # Create a new DataFrame with cleaned data\n",
    "        new = pd.DataFrame(new_data)\n",
    "        new.fillna(0, inplace=True)\n",
    "        # Add cleaned data to the dictionary if it contains non-zero rows\n",
    "        if new.shape[0] != 0:\n",
    "            data_cleaned[key] = new\n",
    "        \n",
    "    return data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7da7fc3d-461c-4a38-8ae2-8d19805e5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newModifiedBrands = cleaning13New(promotions_brands_P12M,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "586375bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(VSODClient_Cleaned, VSODCleaned, col):\n",
    "    \"\"\"\n",
    "    Merge two dictionaries of DataFrames based on a common column.\n",
    "\n",
    "    Parameters:\n",
    "    - VSODClient_Cleaned (dict): Dictionary containing cleaned VSOD client DataFrames.\n",
    "    - VSODCleaned (dict): Dictionary containing cleaned VSOD DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing merged DataFrames.\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "    for key in VSODClient_Cleaned:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        #merged_df = pd.merge(VSODCleaned[key], VSODClient_Cleaned[key], on=col, how='left')\n",
    "        merged_df = pd.merge(VSODClient_Cleaned[key],VSODCleaned[key], on=col, how='left')\n",
    "        #merged_df = merged_df.dropna(subset=['Grand Total'])\n",
    "        merged_df['Grand Total'] = merged_df['Grand Total'].fillna(0)\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            merged_dict[key] = merged_df     \n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a92fb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_pivot_general(data_dict, pivot_col, value_col, aggfunc='sum', fill_value=pd.NA):\n",
    "    result = {}\n",
    "    for k, df in data_dict.items():\n",
    "        frtcol=df.columns[0]\n",
    "        seccol=df.columns[1]\n",
    "        id_cols=[frtcol,seccol]\n",
    "        df_clean = df.dropna(subset=id_cols + [pivot_col, value_col], how='any')\n",
    "        \n",
    "        pivot_df = df_clean.pivot_table(\n",
    "            index=id_cols,\n",
    "            columns=pivot_col,\n",
    "            values=value_col,\n",
    "            aggfunc=aggfunc,\n",
    "            fill_value=fill_value\n",
    "        ).reset_index()\n",
    "\n",
    "        if isinstance(pivot_df.columns, pd.MultiIndex):\n",
    "            pivot_df.columns = [col if not isinstance(col, tuple) else col[-1] for col in pivot_df.columns]\n",
    "        \n",
    "        # Sum brand columns (all columns after id_cols) to create 'Grand Total'\n",
    "        brand_cols = pivot_df.columns[len(id_cols):]\n",
    "        pivot_df['Grand Total'] = pivot_df[brand_cols].sum(axis=1)\n",
    "\n",
    "        result[k] = pivot_df\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ae1fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for each pivot call in a list of tuples:\n",
    "pivot_params = [\n",
    "    (Sector_client_VSOD, 'Top Brands', 'Sector_client_VSODNew'),\n",
    "    (Sector_manuf_VSOD, 'Top Companies', 'Sector_manuf_VSODNew'),\n",
    "    (Segment_client_VSOD, 'Top Brands', 'Segment_client_VSODNew'),\n",
    "    (Segment_manuf_VSOD, 'Top Companies', 'Segment_manuf_VSODNew'),\n",
    "    (SubSegment_client_VSOD, 'Top Brands', 'SubSegment_client_VSODNew'),\n",
    "    (SubSegment_manuf_VSOD, 'Top Companies', 'SubSegment_manuf_VSODNew'),\n",
    "    (SubCategory_client_VSOD, 'Top Brands', 'SubCategory_client_VSODNew'),\n",
    "    (SubCategory_manuf_VSOD, 'Top Companies', 'SubCategory_manuf_VSODNew')\n",
    "]\n",
    "\n",
    "# Prepare a dictionary to store results:\n",
    "pivot_results = {}\n",
    "\n",
    "for data_dict, pivot_col, result_name in pivot_params:\n",
    "    pivot_results[result_name] = dict_to_pivot_general(\n",
    "        data_dict=data_dict,\n",
    "        pivot_col=pivot_col,\n",
    "        value_col='VSOD',\n",
    "        aggfunc='sum',\n",
    "        fill_value=pd.NA\n",
    "    )\n",
    "\n",
    "# If you want to have these as separate variables in your workspace, you can unpack like:\n",
    "Sector_client_VSODNew = pivot_results['Sector_client_VSODNew']\n",
    "Sector_manuf_VSODNew = pivot_results['Sector_manuf_VSODNew']\n",
    "Segment_client_VSODNew = pivot_results['Segment_client_VSODNew']\n",
    "Segment_manuf_VSODNew = pivot_results['Segment_manuf_VSODNew']\n",
    "SubSegment_client_VSODNew = pivot_results['SubSegment_client_VSODNew']\n",
    "SubSegment_manuf_VSODNew = pivot_results['SubSegment_manuf_VSODNew']\n",
    "SubCategory_client_VSODNew = pivot_results['SubCategory_client_VSODNew']\n",
    "SubCategory_manuf_VSODNew = pivot_results['SubCategory_manuf_VSODNew']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d18ef6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors) >0:\n",
    "    a = VSOD_Clean(Sector_VSOD)\n",
    "    if len(Sector_client_VSODNew) >0:\n",
    "        b = cleaningData(Sector_client_VSODNew)\n",
    "        sect_vsod_merged = merging(b,a, col=[direct_parent[\"Sector\"],'Sector'])\n",
    "    else:\n",
    "        sect_vsod_merged = a\n",
    "    c = cleaningData(Sector_manuf_VSODNew)\n",
    "    for key in sect_vsod_merged:\n",
    "        merged_df = pd.merge(sect_vsod_merged[key], c[key], on=[direct_parent[\"Sector\"],'Sector'], how='left')\n",
    "        if merged_df.shape[0]>0:\n",
    "            sect_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(segments) >0:\n",
    "    a = VSOD_Clean(Segment_VSOD)\n",
    "    if len(Segment_client_VSODNew) > 0:\n",
    "        b = cleaningData(Segment_client_VSODNew)\n",
    "        seg_vsod_merged = merging(a,b, col=[direct_parent[\"Segment\"],'Segment'])\n",
    "    else:\n",
    "        seg_vsod_merged = a\n",
    "    \n",
    "    c = cleaningData(Segment_manuf_VSODNew)\n",
    "    for key in seg_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(seg_vsod_merged[key], c[key], on=[direct_parent[\"Segment\"],'Segment'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            seg_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(subsegments) >0:\n",
    "    a = VSOD_Clean(SubSegment_VSOD)\n",
    "    if len(SubSegment_client_VSODNew) > 0 :\n",
    "        b = cleaningData(SubSegment_client_VSODNew)\n",
    "        subseg_vsod_merged = merging(a,b, col=[direct_parent[\"SubSegment\"],'SubSegment'])\n",
    "    else:\n",
    "        subseg_vsod_merged = a\n",
    "    c = cleaningData(SubSegment_manuf_VSOD)\n",
    "    for key in subseg_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(subseg_vsod_merged[key], c[key], on=[direct_parent[\"SubSegment\"],'SubSegment'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            subseg_vsod_merged[key] = merged_df    \n",
    "\n",
    "if len(subcategories) >0:\n",
    "    a = VSOD_Clean(SubCategory_VSOD)\n",
    "    if len(SubCategory_client_VSODNew) > 0 :\n",
    "        b = cleaningData(SubCategory_client_VSODNew)\n",
    "        subcat_vsod_merged = merging(a,b, col=[direct_parent[\"SubCategory\"],'SubCategory'])\n",
    "    else:\n",
    "        subcat_vsod_merged = a\n",
    "    c = cleaningData(SubCategory_manuf_VSODNew)\n",
    "    for key in subcat_vsod_merged:\n",
    "        # Merge DataFrames based on 'Sector' column\n",
    "        merged_df = pd.merge(subcat_vsod_merged[key], c[key], on=[direct_parent[\"SubCategory\"],'SubCategory'], how='left')\n",
    "        merged_df = merged_df.fillna(0)\n",
    "        if merged_df.shape[0]>0:\n",
    "            subcat_vsod_merged[key] = merged_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f75fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitkeys(dic, lis, parent,clientlist):\n",
    "    \"\"\"\n",
    "    Splits the keys of a dictionary into new keys based on unique values in a specified column.\n",
    "    Parameters:\n",
    "    dic (dict): The input dictionary with DataFrames as values.\n",
    "    lis (list): A list of sector names to filter by (if needed).\n",
    "    parent (str): The column name used for splitting (e.g., 'Sector').\n",
    "    \n",
    "    Returns:\n",
    "    dict: A new dictionary with updated keys and filtered DataFrames.\n",
    "    \"\"\"\n",
    "    splitvsod = {}\n",
    "    for key in dic.keys():\n",
    "        for key in dic.keys():\n",
    "            # Split the key into parts and check if all parts are in the valid list\n",
    "    \n",
    "                # Get a copy of the current DataFrame\n",
    "            s = dic[key].copy()        \n",
    "            for value in s[parent].unique():\n",
    "                if isinstance(value, str) and not value.endswith(\"Total\"):\n",
    "                    new_key = f\"{key} | {value}\"                \n",
    "                    filtered_df = s[s[parent].isin([value, f\"{value} Total\"])]   \n",
    "                    for cli in clientlist:\n",
    "                        needed_col = [filtered_df.columns[0], filtered_df.columns[1], \"VSOD\", cli]\n",
    "                        missing_cols = [col for col in needed_col if col not in filtered_df.columns]\n",
    "                        \n",
    "                        if not missing_cols:\n",
    "                            filtered_dfnew = filtered_df[needed_col]\n",
    "                            splitvsod[new_key + \" | \" + cli] = filtered_dfnew\n",
    "                        else:\n",
    "                            print(f\"Skipping {cli}: missing columns {missing_cols}\")\n",
    "\n",
    "    keys_to_remove = [\n",
    "        k for k in splitvsod.keys() \n",
    "        if k.split(\" | \")[-2] not in lis\n",
    "    ]\n",
    "    for k in keys_to_remove:\n",
    "        del splitvsod[k]\n",
    "    return splitvsod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e97a11db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Equate: missing columns ['Equate']\n",
      "Skipping Equate: missing columns ['Equate']\n",
      "Skipping Equate: missing columns ['Equate']\n",
      "Skipping Equate: missing columns ['Equate']\n",
      "Skipping Equate: missing columns ['Equate']\n",
      "Skipping Equate: missing columns ['Equate']\n",
      "Skipping Equate: missing columns ['Equate']\n",
      "Skipping Equate: missing columns ['Equate']\n"
     ]
    }
   ],
   "source": [
    "client=client_brands+client_manuf\n",
    "if len(sectors)!=0:\n",
    "    sect_vsod_merged=splitkeys(sect_vsod_merged,categories,parent=direct_parent['Sector'],clientlist=client)\n",
    "if len(segments)!=0:\n",
    "    seg_vsod_merged=splitkeys(seg_vsod_merged,sectors,parent=direct_parent['Segment'],clientlist=client)\n",
    "if len(subsegments)!=0:\n",
    "    subseg_vsod_merged=splitkeys(subseg_vsod_merged,segments,parent=direct_parent['SubSegment'],clientlist=client)\n",
    "if len(subcategories)!=0:\n",
    "    subcat_vsod_merged=splitkeys(subcat_vsod_merged,segments,parent=direct_parent['SubCategory'],clientlist=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b271ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors):\n",
    "    sect_vsod_count =0\n",
    "    for key,df in sect_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                sect_vsod_count +=1\n",
    "    sect_vsod_count = sect_vsod_count *len(categories)\n",
    " \n",
    "if len(segments):\n",
    "    seg_vsod_count =0\n",
    "    for key,df in seg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                seg_vsod_count +=1\n",
    "    #seg_vsod_count = seg_vsod_count * len(sectors) \n",
    "    seg_vsod_count = seg_vsod_count           \n",
    " \n",
    "if len(subsegments) >0:\n",
    "    subseg_vsod_count =0\n",
    "    for key,df in subseg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subseg_vsod_count +=1\n",
    "    #subseg_vsod_count =subseg_vsod_count *len(segments)\n",
    "    subseg_vsod_count = subseg_vsod_count\n",
    " \n",
    "if len(subcategories) >0:\n",
    "    subcat_vsod_count =0\n",
    "    for key,df in subcat_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subcat_vsod_count +=1\n",
    "    #subcat_vsod_count = subcat_vsod_count * len(subsegments)\n",
    "    subcat_vsod_count = subcat_vsod_count\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dd04ba3-5570-4ad4-8d61-252872480a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandSortedTotalFinal={}\n",
    "promotionsBrandSortedTotal=dfSort(modified_promotionBrandsP12M, client_brands, \"Top Brands\", num=8,salesCol='Promo Value')\n",
    "for key,df in promotionsBrandSortedTotal.items():\n",
    "     df_client = selectClientBrands(promotionsBrandSortedTotal[key],'Top Brands', 'Promo Value')\n",
    "     number_of_brands_needed = max(6 - len(df_client),0)\n",
    "     df = df[~df['Top Brands'].isin(client_brands)]\n",
    "     df = df.sort_values(by='Promo Value', ascending=False).head(number_of_brands_needed)\n",
    "     df = pd.concat([df, df_client], ignore_index=True)\n",
    "     df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "     df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "     df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    "     df = df[df['Value Share'] > 0.01]\n",
    "        \n",
    "     df['VSOD Evaluation vs YA'] = df['VSOD Evaluation vs YA'].astype(float)\n",
    "     df['Promo Value Uplift vs YA'] = df['Promo Value Uplift vs YA'].astype(float)\n",
    "     \n",
    "     if df.shape[0] >0:\n",
    "          promotionsBrandSortedTotalFinal[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e66555e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandNOTSortedTotalFinal={}\n",
    "promotionsBrandNOTSortedTotalFinal=dfSort(modified_promotionBrandsP12M, client_brands, \"Top Brands\", num=8,salesCol='Promo Value')\n",
    "for key,df in modified_promotionBrandsP12M.items():\n",
    "     df = df.sort_values(by='Promo Value', ascending=False).reset_index(drop=True)\n",
    "     df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "     df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    "     df = df[df['Value Share'] > 0.01]\n",
    "     df['VSOD Evaluation vs YA'] = df['VSOD Evaluation vs YA'].astype(float)\n",
    "     df['Promo Value Uplift vs YA'] = df['Promo Value Uplift vs YA'].astype(float)\n",
    "     if df.shape[0] >0:\n",
    "          promotionsBrandNOTSortedTotalFinal[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75e0e42f-2500-4d43-9ed2-2b5ac567a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands_og = selectedBrands\n",
    "selectedBrands= selectedBrands + [\"Grand Total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab470824-4116-42ea-b87c-150650a1d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsSelected={key:modified_promotionBrandsP12M_total[key][modified_promotionBrandsP12M_total[key]['Top Brands'].isin(selectedBrands)].sort_values(by='Promo Value',ascending=False) for key in modified_promotionBrandsP12M_total.keys()   if all(cat != key.split(' | ')[0] for cat in categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5deb8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in promotionsBrandsSelected:\n",
    "    # Identify the Grand Total row (adjust this if necessary to match your data)\n",
    "    grand_total_row = promotionsBrandsSelected[key].loc[promotionsBrandsSelected[key]['Top Brands'] == 'Grand Total']\n",
    "    # Remove the Grand Total row from the dataframe\n",
    "    sorted_df = promotionsBrandsSelected[key].loc[promotionsBrandsSelected[key]['Top Brands'] != 'Grand Total']\n",
    "    # Concatenate the Grand Total row to the top of the dataframe\n",
    "    promotionsBrandsSelected[key] = pd.concat([grand_total_row, sorted_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf5988c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedBrands = selectedBrands_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d4a6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not including client brands\n",
    "promotionsNotBrandsSelected = {\n",
    "    key: modified_promotionBrandsP12M_total[key][\n",
    "        ~modified_promotionBrandsP12M_total[key]['Top Brands'].isin(selectedBrands)\n",
    "    ].sort_values(by='Value Share', ascending=False)\n",
    "    for key in modified_promotionBrandsP12M_total.keys()\n",
    "    if all(cat != key.split(' | ')[0] for cat in categories)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eabcdd35-a8e6-4667-8efe-17c5dea262ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAttribute(dic, marketList):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary of DataFrames and a list of markets, and concatenates\n",
    "    the DataFrames by adding a 'SOURCE' column to each DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are strings in the format 'market | source', and\n",
    "                values are DataFrames containing market data.\n",
    "    marketList (list): A list of market names (strings).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are market names and values are concatenated DataFrames\n",
    "          with an added 'SOURCE' column.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict to store the resulting DataFrames\n",
    "    marketDic = defaultdict(list)\n",
    "    \n",
    "    # Iterate through the list of markets\n",
    "    for market in marketList:\n",
    "        # Iterate through the items in the dictionary\n",
    "        for key, value in dic.items():\n",
    "            # Check if the market name matches the key's market part\n",
    "            if market == key.split(' | ')[1]:\n",
    "                # Extract the source part from the key and assign it to the 'SOURCE' column\n",
    "                value['SOURCE'] = list(set(key.split(' | ')) - set([market]))[0]\n",
    "                value = value[value['Value Share'] >0.01]\n",
    "                value = value[~value['Top Brands'].str.contains('Other')].reset_index(drop=True)\n",
    "                # Only include rows where 'SOURCE' is not 'National'\n",
    "                if (value['SOURCE'] != 'National').all():\n",
    "                    marketDic[market].append(value)\n",
    "\n",
    "        # Concatenate all DataFrames in the list for each market\n",
    "        if len(marketDic[market]) != 0:\n",
    "            marketDic[market] = pd.concat(marketDic[market])\n",
    "    \n",
    "    return marketDic\n",
    "\n",
    "def fillingMissingBrands(dic):\n",
    "    \"\"\"\n",
    "    This function fills in missing brands for each market and source combination in the\n",
    "    provided dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    dic (dict): A dictionary where keys are market names and values are DataFrames\n",
    "                containing market data with 'Top Brands' and 'SOURCE' columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: The input dictionary with missing brands filled in each DataFrame.\n",
    "    \"\"\"\n",
    "    # Iterate through the dictionary items\n",
    "    for key, value in dic.items():\n",
    "        # Get the unique list of top brands in the DataFrame\n",
    "        brandList = value['Top Brands'].unique().tolist()\n",
    "        # Iterate through the unique sources in the DataFrame\n",
    "        for source in value['SOURCE'].unique():\n",
    "            # Check if the number of unique brands for the source is less than the total unique brands\n",
    "            if value[value['SOURCE'] == source]['Top Brands'].nunique() != len(brandList):\n",
    "                # Find the missing brands for the source\n",
    "                missingBrand = list(set(brandList) - set(value[value['SOURCE'] == source]['Top Brands'].unique()))\n",
    "                # Create a DataFrame for the missing brands with the current source\n",
    "                missingBrand = pd.DataFrame({'Top Brands': missingBrand, 'SOURCE': source}).explode('Top Brands')\n",
    "                # Concatenate the missing brands DataFrame with the original DataFrame\n",
    "                value = pd.concat([value, missingBrand]).replace(np.nan, 0).reset_index(drop=True)\n",
    "        # Update the dictionary with the filled DataFrame\n",
    "        dic[key] = value\n",
    "    \n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6b2c4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Brands</th>\n",
       "      <th>Total Size</th>\n",
       "      <th>Promo Value</th>\n",
       "      <th>VSOD</th>\n",
       "      <th>VSOD IYA</th>\n",
       "      <th>Value Share</th>\n",
       "      <th>Promo Share</th>\n",
       "      <th>Value Uplift (v. base) Normalized</th>\n",
       "      <th>Value Uplift Normalized IYA</th>\n",
       "      <th>Volume Uplift (v. Base) Normalized</th>\n",
       "      <th>Volume Uplift Normalized IYA</th>\n",
       "      <th>Promo Value Uplift vs YA</th>\n",
       "      <th>VSOD Evaluation vs YA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grand Total</td>\n",
       "      <td>0</td>\n",
       "      <td>16505432.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.447</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245532</td>\n",
       "      <td>1.404748</td>\n",
       "      <td>0.424946</td>\n",
       "      <td>1.367674</td>\n",
       "      <td>0.404748</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schick</td>\n",
       "      <td>0</td>\n",
       "      <td>6252872.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.381</td>\n",
       "      <td>0.37415</td>\n",
       "      <td>0.378837</td>\n",
       "      <td>0.231346</td>\n",
       "      <td>0.982016</td>\n",
       "      <td>0.517432</td>\n",
       "      <td>1.186822</td>\n",
       "      <td>-0.017984</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Top Brands Total Size  Promo Value  VSOD  VSOD IYA  Value Share  \\\n",
       "0  Grand Total          0   16505432.0  0.55     1.447      1.00000   \n",
       "1       Schick          0    6252872.0  0.58     1.381      0.37415   \n",
       "\n",
       "   Promo Share  Value Uplift (v. base) Normalized  \\\n",
       "0     1.000000                           0.245532   \n",
       "1     0.378837                           0.231346   \n",
       "\n",
       "   Value Uplift Normalized IYA  Volume Uplift (v. Base) Normalized  \\\n",
       "0                     1.404748                            0.424946   \n",
       "1                     0.982016                            0.517432   \n",
       "\n",
       "   Volume Uplift Normalized IYA Promo Value Uplift vs YA VSOD Evaluation vs YA  \n",
       "0                      1.367674                 0.404748                 0.447  \n",
       "1                      1.186822                -0.017984                 0.381  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promotionsBrandsSelected['Disposables | Bj\\'s And Sam\\'s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5825f852-d5a8-4717-863c-60bce7831474",
   "metadata": {},
   "outputs": [],
   "source": [
    "promotionsBrandsWithMarket=concatAttribute(promotionsBrandsSelected,marketList)\n",
    "promotionsBrandsWithMarket = fillingMissingBrands(promotionsBrandsWithMarket)\n",
    "promotionsNotBrandsWithMarket=concatAttribute(promotionsNotBrandsSelected,marketList)\n",
    "promotionsNotBrandsWithMarket = fillingMissingBrands(promotionsNotBrandsWithMarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "762fb5e2-0b4d-442f-9531-040980af3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_market(data, Scope):\n",
    "    final = {}\n",
    "    for k,df in data.items():\n",
    "        for key, value in Scope.items():\n",
    "            df_market = df[df['SOURCE'].isin(value)]\n",
    "            df_market = df_market.reset_index(drop=True)\n",
    "            if df_market.shape[0] >0:\n",
    "                final[k + ' | ' + value[0]] = df_market\n",
    "    return final        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2a003a6-6da6-495c-a2d9-a9b0af5a7a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newpromotionsBrandsWithMarket = split_market(promotionsBrandsWithMarket,Scope)\n",
    "newpromotionsNotBrandsWithMarket = split_market(promotionsNotBrandsWithMarket,Scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e045f79-110e-4cb8-b237-b514151b9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAttributeNew(sorted):\n",
    "    \"\"\"\n",
    "    Concatenate DataFrames from a sorted dictionary based on exact matches of categories, sectors, segments,\n",
    "    subsegments, and subcategories. Adds a 'SOURCE' column to each DataFrame indicating its market.\n",
    "\n",
    "    Parameters:\n",
    "    sorted (dict): Dictionary with keys like 'category | sector | segment | brand' and values as DataFrames.\n",
    "    categories, sectors, segments, subsegments, subcategories (list): Lists of strings to match exactly.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with matched group names as keys and concatenated DataFrames as values.\n",
    "    \"\"\"\n",
    "    # Combine all lists and preserve order without duplicates\n",
    "    lis = list(dict.fromkeys(categories + sectors + segments + subsegments + subcategories))\n",
    "\n",
    "    marketDic = defaultdict(list)\n",
    "    concatenatedDic = {}\n",
    "\n",
    "    for i in lis:\n",
    "        for key, df in sorted.items():\n",
    "            parts = key.split(' | ')\n",
    "            if i in parts:\n",
    "                # Determine market label\n",
    "        \n",
    "                market_label = parts[1]  # category\n",
    "\n",
    "                df = df.copy()  # Avoid modifying original\n",
    "                df['SOURCE'] = market_label\n",
    "                marketDic[i].append(df)\n",
    "\n",
    "        if marketDic[i]:\n",
    "            concatenatedDic[i] = pd.concat(marketDic[i], ignore_index=True)\n",
    "\n",
    "    return concatenatedDic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28e29be2-2f65-4603-8858-1087e008b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = concatAttributeNew(modified_promotionBrandsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3ee231f-9b67-4eef-ad0a-ae6e7d75eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        df = modified_promotionProductsP12M[key]\n",
    "        df = df[df[f'{prodORitem}'] != '']\n",
    "        df=df.sort_values(by=['Promo Share'], ascending=False)\n",
    "        # Filter and sort the DataFrame\n",
    "        df['cumulative promo share'] = df['Promo Share'].cumsum()\n",
    "        df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "        df = df[df['VSOD'] >= 0.05]\n",
    "        df = df[df['cumulative promo share'] <= 0.8]\n",
    "        df = df.sort_values(by='Incr Value', ascending=False).reset_index(drop=True)\n",
    "        df = df.head(50)\n",
    "        df['index'] = str(df.index + 1)\n",
    "        df = df.reset_index(drop=True)\n",
    "        if df.shape[0] >0:\n",
    "            cleaned_data[key] = df\n",
    "        #else:\n",
    "            #print(key)\n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b8c6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_modified_promotionProductsP12M = filter_data(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3df8ceaa-7083-4015-8ccb-6202d9ac4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_Top(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        combined_df = pd.DataFrame() \n",
    "        for client in client_brands:\n",
    "            \n",
    "            df = modified_promotionProductsP12M[key]\n",
    "            # Filter the DataFrame for the current client brand\n",
    "            df = df[df[f'{prodORitem}'] != '']\n",
    "            df = df[df['Top Brands'] == client]\n",
    "            df = df.sort_values(by='Top Brands')\n",
    "            df['Promo Share'] = pd.to_numeric(df['Promo Share'], errors='coerce')\n",
    "            df['cumulative promo share'] = df.groupby('Top Brands')['Promo Share'].cumsum()\n",
    "            df = df[df['cumulative promo share'] <= 0.8]\n",
    "            df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "            df = df[df['VSOD'] >= 0.05]\n",
    "            if df.shape[0] >0:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "                combined_df = combined_df.sort_values(by='Incr Value', ascending=False).head(20).reset_index(drop=True)\n",
    "        if combined_df.shape[0] > 0:\n",
    "            cleaned_data[key] = combined_df.reset_index(drop=True)  # Store the combined DataFrame for the current key\n",
    "                \n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0241c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_Bot(modified_promotionProductsP12M):\n",
    "    cleaned_data = {}\n",
    "    for key in modified_promotionProductsP12M:\n",
    "        combined_df = pd.DataFrame() \n",
    "        for client in client_brands:\n",
    "            df = modified_promotionProductsP12M[key]\n",
    "            df = df[df[f'{prodORitem}'] != '']\n",
    "            df = df[df['Top Brands'] == client]\n",
    "            df = df.sort_values(by='Top Brands')\n",
    "            df['Promo Share'] = pd.to_numeric(df['Promo Share'], errors='coerce')\n",
    "            df['cumulative promo share'] = df.groupby('Top Brands')['Promo Share'].cumsum()\n",
    "            df = df[df['Discount Depth (%)'] >= 0.05]\n",
    "            df = df[df['VSOD'] >= 0.05]\n",
    "            if df.shape[0] >0:\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "                combined_df = combined_df.sort_values(by='Incr Value', ascending=False).tail(20).reset_index(drop=True)\n",
    "                combined_df = combined_df.sort_values(by ='Incr Value', ascending= True).reset_index(drop=True)\n",
    "        if combined_df.shape[0] > 0:\n",
    "            cleaned_data[key] = combined_df.reset_index(drop=True)  # Store the combined DataFrame for the current key\n",
    "    return cleaned_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5316486-6b68-430c-963f-11df7ef98c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20clientonly = filter_data_Top(modified_promotionProductsP12M)\n",
    "bottom20clientonly = filter_data_Bot(modified_promotionProductsP12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f3e9198-c36a-4544-8a00-92a1e2ab89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotionsEndOfWeekCleaning(promotions_EndOfWeek, notInScope, col='Top Brands'):\n",
    "    \"\"\"\n",
    "    Clean promotions end of week data.\n",
    "\n",
    "    Parameters:\n",
    "    promotions_EndOfWeek (dict): Dictionary containing promotions end of week data.\n",
    "    notInScope (list): List of items not in scope to be excluded.\n",
    "    col (str): Column name to check for filtering. Default is 'Top Brands'.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing cleaned promotions end of week data.\n",
    "    \"\"\"\n",
    "    promotionsEndOfWeek = {}\n",
    "    for key, value in promotions_EndOfWeek.items():\n",
    "        df = value.copy()\n",
    "        if df.shape[0] != 0:\n",
    "            modified_key = key\n",
    "            flag = False if any(element in modified_key for element in notInScope) else True\n",
    "            if flag:\n",
    "                promotionsEndOfWeek[modified_key] = df[df[col] != 'Grand Total'].reset_index(drop=True).replace(np.nan, 0)\n",
    "        else:\n",
    "            print(key, ' Is empty')\n",
    "    \n",
    "    return promotionsEndOfWeek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3bdd031-7086-48bc-b9b6-d9e09be82bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0    2023-10-01\n",
      "1    2023-10-08\n",
      "2    2023-10-15\n",
      "3    2023-10-22\n",
      "4    2023-10-29\n",
      "5    2023-11-05\n",
      "6    2023-11-12\n",
      "7    2023-11-19\n",
      "8    2023-11-26\n",
      "9    2023-12-03\n",
      "10   2023-12-10\n",
      "11   2023-12-17\n",
      "12   2023-12-24\n",
      "13   2023-12-31\n",
      "14   2024-01-07\n",
      "15   2024-01-14\n",
      "16   2024-01-21\n",
      "17   2024-01-28\n",
      "18   2024-02-04\n",
      "19   2024-02-11\n",
      "20   2024-02-18\n",
      "21   2024-02-25\n",
      "22   2024-03-03\n",
      "23   2024-03-10\n",
      "24   2024-03-17\n",
      "25   2024-03-24\n",
      "26   2024-03-31\n",
      "27   2024-04-07\n",
      "28   2024-04-14\n",
      "29   2024-04-21\n",
      "30   2024-04-28\n",
      "31   2024-05-05\n",
      "32   2024-05-12\n",
      "33   2024-05-26\n",
      "34   2024-06-02\n",
      "35   2024-06-09\n",
      "36   2024-06-16\n",
      "37   2024-06-23\n",
      "38   2024-06-30\n",
      "39   2024-07-14\n",
      "40   2024-08-11\n",
      "41   2024-09-22\n",
      "42   2024-11-03\n",
      "43   2024-12-15\n",
      "44   2024-12-22\n",
      "45   2024-12-29\n",
      "46   2025-01-12\n",
      "47   2025-01-19\n",
      "Name: End of Week, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0    2023-01-08\n",
      "1    2023-01-15\n",
      "2    2023-01-22\n",
      "3    2023-01-29\n",
      "4    2023-02-05\n",
      "        ...    \n",
      "93   2024-12-29\n",
      "94   2025-01-05\n",
      "95   2025-01-12\n",
      "96   2025-01-19\n",
      "97   2025-01-26\n",
      "Name: End of Week, Length: 98, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "103   2024-12-29\n",
      "104   2025-01-05\n",
      "105   2025-01-12\n",
      "106   2025-01-19\n",
      "107   2025-01-26\n",
      "Name: End of Week, Length: 108, dtype: datetime64[ns]\n",
      "0     2023-01-08\n",
      "1     2023-01-15\n",
      "2     2023-01-22\n",
      "3     2023-01-29\n",
      "4     2023-02-05\n",
      "         ...    \n",
      "102   2024-12-29\n",
      "103   2025-01-05\n",
      "104   2025-01-12\n",
      "105   2025-01-19\n",
      "106   2025-01-26\n",
      "Name: End of Week, Length: 107, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "mod=cleaningData(promotions_EndOfWeek)\n",
    "promotionsEndOfWeekCleaned=promotionsEndOfWeekCleaning(mod,notInScope,col='End of Week')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e7408ac-109a-48e1-83ee-1f3acda454ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "brandMarketCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat in key.split(' | ')[0] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[0] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[0] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[0] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in promotionsEndOfWeekCleaned.keys() if any(cat == key.split(' | ')[0] for cat in subcategories )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33070cde-4b18-410c-bf87-00d132014a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeDates(dfList, promotionsEndOfWeekCleaned):\n",
    "    \"\"\"\n",
    "    Complete dates for each dataframe in dfList based on promotionsEndOfWeekCleaned.\n",
    "    Parameters:\n",
    "    dfList (list): List of dataframe keys.\n",
    "    promotionsEndOfWeekCleaned (dict): Dictionary containing cleaned promotions end of week data.\n",
    "    Returns:\n",
    "    tuple: Tuple containing EndOfWeekcompletDate (dictionary), dfGroup (list), and dic (dictionary).\n",
    "    \"\"\"\n",
    "    # Create a list of unique brand-category combinations\n",
    "    brandCatList = sorted(set(key.split(' | ')[0] + ' | ' + key.split(' | ')[2] for key in dfList))\n",
    "    EndOfWeekcompletDate = {}\n",
    "    dfGroup = []\n",
    "    dic = defaultdict(int)\n",
    "    for key in brandCatList:\n",
    "        for name in dfList:\n",
    "            if (key.split(' | ')[0] == name.split(' | ')[0]) and (key.split(' | ')[1] == name.split(' | ')[2]):\n",
    "                dic[key] += 1\n",
    "                \n",
    "    # Iterate over unique brand-category combinations\n",
    "    for name in dic.keys():\n",
    "        # Get dataframe keys associated with the current brand-category combination\n",
    "        dfName = [key for key in dfList if name == (key.split(' | ')[0] + ' | ' + key.split(' | ')[2])]\n",
    "        uniqueDates = pd.concat([promotionsEndOfWeekCleaned[key] for key in dfName])[['End of Week']].drop_duplicates()\n",
    "        if uniqueDates.shape[0] > 0:\n",
    "            dfCompleteDates = {}\n",
    "            dfGroup.append(dfName)\n",
    "            for key in dfName:\n",
    "                EndOfWeekcompletDate[key] = pd.merge(uniqueDates, promotionsEndOfWeekCleaned[key], how='left').replace(np.nan, 0).sort_values(by='End of Week').reset_index(drop = True)\n",
    "    return EndOfWeekcompletDate, dfGroup, dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26bde262-eab5-4afc-ab44-9c3658cb974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory,catGroup,catDuplication=completeDates(brandMarketCategory,promotionsEndOfWeekCleaned)\n",
    "if len(sectors) != 0:\n",
    "    dfSector,secGroup,secDuplication=completeDates(brandMarketSector,promotionsEndOfWeekCleaned)\n",
    "if len(segments) != 0:\n",
    "    dfSegment,segGroup,segDuplication=completeDates(brandMarketSegment,promotionsEndOfWeekCleaned)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment,subsegGroup,subsegDuplication=completeDates(brandMarketSubSegment,promotionsEndOfWeekCleaned)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory,subcatGroup,subcatDuplication=completeDates(brandMarketSubCategory,promotionsEndOfWeekCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4108f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sectors):\n",
    "    sect_vsod_count =0\n",
    "    for key,df in sect_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                sect_vsod_count +=1\n",
    "    sect_vsod_count = sect_vsod_count *len(categories)\n",
    " \n",
    "if len(segments):\n",
    "    seg_vsod_count =0\n",
    "    for key,df in seg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                seg_vsod_count +=1\n",
    "    seg_vsod_count = seg_vsod_count           \n",
    " \n",
    "if len(subsegments) >0:\n",
    "    subseg_vsod_count =0\n",
    "    for key,df in subseg_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subseg_vsod_count +=1\n",
    "    subseg_vsod_count = subseg_vsod_count\n",
    " \n",
    "if len(subcategories) >0:\n",
    "    subcat_vsod_count =0\n",
    "    for key,df in subcat_vsod_merged.items():\n",
    "        client_manuf_brands = client_brands + client_manuf\n",
    "        for client in client_manuf_brands:\n",
    "            if client in df.columns:\n",
    "                subcat_vsod_count +=1\n",
    "    subcat_vsod_count = subcat_vsod_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43c6f94a-d00f-46ad-9011-678073e6bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoRet ={}\n",
    "if len(categories)!=0:\n",
    "    first_key, first_value = next(iter(catDuplication.items()))\n",
    "    PromoRet.update({first_key: first_value})\n",
    "if len(sectors)!=0:\n",
    "    sec_key, sec_value = next(iter(secDuplication.items()))\n",
    "    PromoRet.update({sec_key:sec_value})\n",
    "if len(segments)!=0:\n",
    "    third_key, third_value = next(iter(segDuplication.items()))\n",
    "    PromoRet.update({third_key: third_value})\n",
    "if len(subsegments)!=0:\n",
    "    fourth_key, fourth_value = next(iter(subsegDuplication.items()))\n",
    "    PromoRet.update({fourth_key:fourth_value})\n",
    "if len(subcategories)!=0:\n",
    "    fifth_key, fifth_value = next(iter(subcatDuplication.items()))\n",
    "    PromoRet.update({fifth_key:fifth_value })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a728a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "PromoSalesTypes_data = {}\n",
    "for key in brands_promo_type.keys():\n",
    "    df=brands_promo_type[key].copy()\n",
    "    df[\"Promo Sales\"] = pd.to_numeric(df[\"Promo Sales\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"Value Share\"] = pd.to_numeric(df[\"Value Share\"], errors=\"coerce\").fillna(0)\n",
    "    df = df[df['Promo Type'].notna()]\n",
    "    brand_totals = df.groupby(\"Top Brands\")['Promo Sales'].sum()\n",
    "    df[\"Brand Total Sales\"] = df[\"Top Brands\"].map(brand_totals)\n",
    "    df[\"% Promo Sales\"] = df[\"Promo Sales\"] / df[\"Brand Total Sales\"]\n",
    "\n",
    "    df = df[~df['Top Brands'].str.contains('Others|Grand Total', case=False)]\n",
    "    df = df[df['Value Share'] > 0.01]\n",
    "    df = df[df['Promo Sales'] > 0]\n",
    "    # Select client brands and additional brands needed to make 10 brands\n",
    "    df_client = selectClientBrands(brands_promo_type[key],'Top Brands', 'Value Share')\n",
    "    comp_brand = df[~df['Top Brands'].isin(cb for cb in client_brands)].drop_duplicates(\"Top Brands\")\n",
    "    if not df_client.empty:\n",
    "        comp_brand = comp_brand.nlargest(10-df_client[\"Top Brands\"].nunique(), \"Value Share\")[\"Top Brands\"].to_list()\n",
    "        # Concatenate client brands and additional brands\n",
    "        df = df[df[\"Top Brands\"].isin(comp_brand + client_brands)]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.sort_values(\"Value Share\", ascending=False).reset_index(drop=True)\n",
    "        df = df[~df['Promo Type'].fillna('').str.contains('NONE/PL|Undefined|Nan', na=False)]\n",
    "        # print(comp_brand)\n",
    "        if df.shape[0]:\n",
    "            PromoSalesTypes_data[key] =df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88c1ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lis = []\n",
    "cat_lis = []\n",
    "if categories:\n",
    "    for i in range(len(catGroup)):\n",
    "        cat_lis += genrateIndexList(catGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(cat_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup)):\n",
    "        sec_lis += genrateIndexList(secGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(sec_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "seg_lis=[]\n",
    "if segments:\n",
    "    for i in range(len(segGroup)):\n",
    "        seg_lis += genrateIndexList(segGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(seg_lis)\n",
    "\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "subseg_lis =[]\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup)):\n",
    "        subseg_lis +=  genrateIndexList(subsegGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(subseg_lis)\n",
    "else:\n",
    "    final_lis.append([])\n",
    "\n",
    "subcat_lis =[]\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup)):\n",
    "        subcat_lis +=  genrateIndexList(subcatGroup[i], chartIndex=14, chartCount=4)[0]\n",
    "    final_lis.append(subcat_lis)\n",
    "else:\n",
    "    final_lis.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd4fa9",
   "metadata": {},
   "source": [
    "### New Slide 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0c9e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonthYear_clean(data,column):\n",
    "    month_year_data={}\n",
    "    for key in data.keys():\n",
    "        df=data[key].copy()\n",
    "        df = df[~((df[column].str.contains('Total', case=False)) & (df[column] != categories[0]))].reset_index(drop=True)\n",
    "\n",
    "        # df = df[~df[column].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "        df['year'] = pd.to_datetime(df['MonthYear'], format='%b-%y').dt.year\n",
    "        df['year'] = df['year'].astype(int)\n",
    "        yearly_avg_sales = df.groupby(['year', column])['Value Sales'].transform('mean').reset_index(drop=True) \n",
    "        yearly_avg_sales = yearly_avg_sales.replace(0, float('nan'))\n",
    "        # Calculate 'Sales index' and handle NaN values gracefully\n",
    "        df['Sales index'] = (df['Value Sales'] / yearly_avg_sales * 100).fillna(0).astype(int)\n",
    "        if df.shape[0]>0:\n",
    "            month_year_data[key] = df\n",
    "    return month_year_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db1737be",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_month_year=MonthYear_clean(Category_MonthYear,column='Category')\n",
    "sector_month_year = MonthYear_clean(Sector_MonthYear,column='Sector')\n",
    "segment_month_year = MonthYear_clean(Segment_MonthYear,column='Segment')\n",
    "subcat_month_year = MonthYear_clean(SubCategory_MonthYear,column='SubCategory')\n",
    "subseg_month_year = MonthYear_clean(SubSegment_MonthYear,column='SubSegment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ac48172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_month_year(data, column):\n",
    "    final_month_year ={}\n",
    "    for key,df in data.items():\n",
    "        for sec in df[column].unique():\n",
    "            newkey = key + ' | ' + sec\n",
    "            new_df = df[df[column] == sec].reset_index(drop=True)\n",
    "            if new_df.shape[0] > 0:\n",
    "                final_month_year[newkey] = new_df\n",
    "    return final_month_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "265d60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_month_year1=split_month_year(category_month_year,'Category')\n",
    "sector_month_year1 = split_month_year(sector_month_year,'Sector')\n",
    "segment_month_year1 = split_month_year(segment_month_year,'Segment')\n",
    "subseg_month_year1 = split_month_year(subseg_month_year,'SubSegment')\n",
    "subcat_month_year1 = split_month_year(subcat_month_year,'SubCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b92a3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_year1 = {}\n",
    "month_year1.update(sector_month_year1)\n",
    "month_year1.update(segment_month_year1)\n",
    "month_year1.update(subcat_month_year1)\n",
    "month_year1.update(subseg_month_year1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a94833af",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarketCategory = [key for key in category_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in sector_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in segment_month_year1.keys() if any(cat == key.split(' | ')[2]  for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in subseg_month_year1.keys() if any(cat == key.split(' | ')[2] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in subcat_month_year1.keys() if any(cat == key.split(' | ')[2] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17cb72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeDates1(dfList, promotionsEndOfWeekCleaned,column=\"Sector\"):\n",
    "    \"\"\"\n",
    "    Complete dates for each dataframe in dfList based on promotionsEndOfWeekCleaned.\n",
    "\n",
    "    Parameters:\n",
    "    dfList (list): List of dataframe keys.\n",
    "    promotionsEndOfWeekCleaned (dict): Dictionary containing cleaned promotions end of week data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing EndOfWeekcompletDate (dictionary), dfGroup (list), and dic (dictionary).\n",
    "    \"\"\"\n",
    "    # Create a list of unique brand-category combinations\n",
    "    brandCatList = sorted(set(key.split(' | ')[0]  for key in dfList))\n",
    "    # Initialize dictionaries and lists\n",
    "    EndOfWeekcompletDate = {}\n",
    "    dfGroup = []\n",
    "    dic = defaultdict(int)\n",
    "    \n",
    "    # Count occurrences of each brand-category combination\n",
    "    for key in brandCatList:\n",
    "        for name in dfList:\n",
    "            if (key.split(' | ')[0] == name.split(' | ')[0]):\n",
    "                \n",
    "                dic[key] += 1\n",
    "                \n",
    "    # Iterate over unique brand-category combinations\n",
    "    for name in dic.keys():\n",
    "\n",
    "        if column == \"Sector\" :\n",
    "            \n",
    "            # Filter dataframe keys associated with the current brand-category combination\n",
    "            dfName = [key for key in dfList if name == key.split(' | ')[0] and len(name.split(' | ')) == 1]\n",
    "        else:\n",
    "            \n",
    "            dfName = [key for key in dfList if name == key.split(' | ')[0]  ]\n",
    "        \n",
    "        # Extract unique dates from all associated dataframes\n",
    "        uniqueDates = pd.concat([promotionsEndOfWeekCleaned[key] for key in dfName])[['MonthYear']].drop_duplicates()\n",
    "        # Initialize dictionary for complete dates for each dataframe\n",
    "        dfCompleteDates = {}\n",
    "        # Add dataframe keys to the group list\n",
    "        dfGroup.append(dfName)\n",
    "        # Populate EndOfWeekcompletDate dictionary with dataframes merged on unique dates\n",
    "        for key in dfName:\n",
    "            EndOfWeekcompletDate[key] = pd.merge(uniqueDates, promotionsEndOfWeekCleaned[key], how='left')#.replace(np.nan, 0)\n",
    "            column = EndOfWeekcompletDate[key].columns[1]\n",
    "            year = EndOfWeekcompletDate[key].columns[3]\n",
    "            monthyear = EndOfWeekcompletDate[key].columns[0]\n",
    "            EndOfWeekcompletDate[key][column] = EndOfWeekcompletDate[key][column].fillna(method='ffill')      \n",
    "            EndOfWeekcompletDate[key][year] = pd.to_datetime(EndOfWeekcompletDate[key][monthyear], format='%b-%y').dt.year\n",
    "            EndOfWeekcompletDate[key] = EndOfWeekcompletDate[key].fillna(0)\n",
    "    return EndOfWeekcompletDate, dfGroup, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b1814ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCategory0,categoryGroup0,categoryDuplication0=completeDates1(brandMarketCategory,category_month_year1,column=\"Category\")\n",
    "if len(sectors) != 0:\n",
    "    dfSector0,secGroup0,secDuplication0=completeDates1(brandMarketSector,sector_month_year1,column=\"Sector\")\n",
    "if len(segments) != 0:\n",
    "    dfSegment0,segGroup0,segDuplication0=completeDates1(brandMarketSegment,segment_month_year1,column=\"Segment\")\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment0,subsegGroup0,subsegDuplication0=completeDates1(brandMarketSubSegment,subseg_month_year1,column=\"Subsegment\")\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory0,subcatGroup0,subcatDuplication0=completeDates1(brandMarketSubCategory,subcat_month_year1,column=\"Subcategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99912278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupingkeys(data):\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    for sublist in data:\n",
    "        for entry in sublist:\n",
    "            prefix = \" | \".join(entry.split(\" | \")[:2])  # Extract first two parts\n",
    "            grouped[prefix].append(entry)\n",
    "    result = list(grouped.values())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55074d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryGroup0=groupingkeys(categoryGroup0)\n",
    "if len(sectors) != 0:\n",
    "    secGroup0=groupingkeys(secGroup0)\n",
    "if len(segments) != 0:\n",
    "    segGroup0=groupingkeys(segGroup0)\n",
    "if len(subsegments) != 0:\n",
    "    subsegGroup0=groupingkeys(subsegGroup0)\n",
    "if len(subcategories) != 0:\n",
    "    subcatGroup0=groupingkeys(subcatGroup0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 20], [21, 21]]\n",
      "[[20, 20], [21, 21], [20, 20, 21, 21]]\n",
      "[[20, 20], [21, 21], [20, 20, 21, 21], []]\n"
     ]
    }
   ],
   "source": [
    "final_lis0 = []\n",
    "category_lis = []\n",
    "if categories:\n",
    "    for i in range(len(categoryGroup0)):\n",
    "        category_lis += genrateIndexList(categoryGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(category_lis)  # Append empty list if sectors is False\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup0)):\n",
    "        sec_lis += genrateIndexList(secGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(sec_lis)  # Append empty list if sectors is False\n",
    "print(final_lis0)\n",
    "\n",
    "seg_lis = []\n",
    "if segments:\n",
    "    for i in range(len(segGroup0)):\n",
    "        seg_lis += genrateIndexList(segGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(seg_lis)  # Append empty list if segments is False\n",
    "print(final_lis0)\n",
    "\n",
    "subseg_lis = []\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup0)):\n",
    "        subseg_lis += genrateIndexList(subsegGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(subseg_lis)  # Append empty list if subsegments is False\n",
    "print(final_lis0)\n",
    "\n",
    "subcat_lis = []\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup0)):\n",
    "        subcat_lis += genrateIndexList(subcatGroup0[i], chartIndex=19, chartCount=6)[0]\n",
    "final_lis0.append(subcat_lis)  # Append empty list if subcategories is False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc50143",
   "metadata": {},
   "source": [
    "### New slide 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3653fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_endofweek_P12M = {}\n",
    "past_12_months = pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n",
    "for key in modified_promotionEndOfWeek.keys():\n",
    "    df=modified_promotionEndOfWeek[key].copy()\n",
    "    df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "    filtered_df = df[df['End of Week'].dt.strftime('%b-%y').isin(past_12_months)]\n",
    "    if filtered_df.shape[0] >0:\n",
    "        modified_endofweek_P12M[key] = filtered_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06d18d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarketCategory= [key for key in modified_endofweek_P12M.keys() if any(cat in key.split(' | ')[0] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[0] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[0] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[0] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in modified_endofweek_P12M.keys() if any(cat == key.split(' | ')[0] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ace0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory1,catGroup1,catDuplication1=completeDates(brandMarketCategory,modified_endofweek_P12M)\n",
    "if len(sectors) != 0:\n",
    "    dfSector1,secGroup1,secDuplication1=completeDates(brandMarketSector,modified_endofweek_P12M)\n",
    "if len(segments) != 0:\n",
    "    dfSegment1,segGroup1,segDuplication1=completeDates(brandMarketSegment,modified_endofweek_P12M)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment1,subsegGroup1,subsegDuplication1=completeDates(brandMarketSubSegment,modified_endofweek_P12M)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory1,subcatGroup1,subcatDuplication1=completeDates(brandMarketSubCategory,modified_endofweek_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8479ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promofrequencyclean(data):        \n",
    "        modified_dfCategory1 = {}\n",
    "        for k in data.keys():\n",
    "                chart_df=data[k].copy()\n",
    "                chart_df['Weekly VSOD'] = np.where((chart_df['VSOD']>.2)&(chart_df['Value Uplift (v. base) Normalized'] != ''),1,None)\n",
    "                chart_df['try'] = 0\n",
    "                chart_df['New Uplift'] = 0\n",
    "                chart_df['try'] = np.where((chart_df['Value Uplift (v. base) Normalized']>=2),1.8,chart_df['Value Uplift (v. base) Normalized'])\n",
    "                chart_df['New Uplift'] = np.where((chart_df['Weekly VSOD']==1)&(chart_df['Value Uplift (v. base) Normalized']>0.05),chart_df['try'],None)\n",
    "                if not chart_df['Weekly VSOD'].isnull().all():\n",
    "                        modified_dfCategory1[k]= chart_df \n",
    "        return modified_dfCategory1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ad97599",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories)!=0: \n",
    "    modified_dfCategory1=promofrequencyclean(dfCategory1)\n",
    "if len(sectors)!=0: \n",
    "    modified_dfSector1=promofrequencyclean(dfSector1)\n",
    "if len(segments)!=0: \n",
    "    modified_dfSegment1=promofrequencyclean(dfSegment1)\n",
    "if len(subsegments)!=0: \n",
    "    modified_dfSubSegment1=promofrequencyclean(dfSubSegment1)\n",
    "if len(subcategories)!=0: \n",
    "    modified_dfSubCategory1=promofrequencyclean(dfSubCategory1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f0aae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "brandMarketCategory= [key for key in modified_dfCategory1.keys() if any(cat in key.split(' | ')[0] for cat in categories )]\n",
    "if len(sectors) != 0:\n",
    "    brandMarketSector = [key for key in modified_dfSector1.keys() if any(cat == key.split(' | ')[0] for cat in sectors )]\n",
    "if len(segments) != 0:\n",
    "    brandMarketSegment = [key for key in modified_dfSegment1.keys() if any(cat == key.split(' | ')[0] for cat in segments )]\n",
    "if len(subsegments) != 0:\n",
    "    brandMarketSubSegment = [key for key in modified_dfSubSegment1.keys() if any(cat == key.split(' | ')[0] for cat in subsegments )]\n",
    "if len(subcategories) != 0:\n",
    "    brandMarketSubCategory = [key for key in modified_dfSubCategory1.keys() if any(cat == key.split(' | ')[0] for cat in subcategories )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "31f3763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories) != 0:\n",
    "    dfCategory1,catGroup1,catDuplication1=completeDates(brandMarketCategory,modified_endofweek_P12M)\n",
    "if len(sectors) != 0:\n",
    "    dfSector1,secGroup1,secDuplication1=completeDates(brandMarketSector,modified_endofweek_P12M)\n",
    "if len(segments) != 0:\n",
    "    dfSegment1,segGroup1,segDuplication1=completeDates(brandMarketSegment,modified_endofweek_P12M)\n",
    "if len(subsegments) != 0:\n",
    "    dfSubSegment1,subsegGroup1,subsegDuplication1=completeDates(brandMarketSubSegment,modified_endofweek_P12M)\n",
    "if len(subcategories) != 0:\n",
    "    dfSubCategory1,subcatGroup1,subcatDuplication1=completeDates(brandMarketSubCategory,modified_endofweek_P12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2bbe9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_region(data):\n",
    "    # Define categories for grouping\n",
    "    market_groups = {\n",
    "        \"RETAILER_REGIONS\": regions_RET,\n",
    "        \"RETAILER_CHANNELS\": channels_RET,\n",
    "        \"RETAILER_MARKET\": market_RET,\n",
    "        \"CHANNEL_REGIONS\": regions_CHAN,\n",
    "        \"CHANNEL_CHANNELS\": channels_CHAN,\n",
    "        \"CHANNEL_MARKET\": market_CHAN,\n",
    "        f\"{customareas}_REGIONS\": regions_CUST,\n",
    "        f\"{customareas}_CHANNELS\": channels_CUST,\n",
    "        f\"{customareas}_MARKET\": market_CUST,\n",
    "    }\n",
    "    result = []\n",
    "    for sublist in data:\n",
    "        for category, keywords in market_groups.items():\n",
    "            # Filter items matching the current category\n",
    "            base_category = category.split(\"_\")[0]\n",
    "\n",
    "            group = [\n",
    "                f\"{item} | {base_category}\" for item in sublist if item.split(\" | \")[-1] in keywords\n",
    "            ]\n",
    "            if group:  # Append only non-empty groups\n",
    "                result.append(group)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "016f5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categories)>0:\n",
    "    catGroup1 = group_by_region(catGroup1)\n",
    "if len(sectors)>0:\n",
    "    secGroup1 = group_by_region(secGroup1)\n",
    "if len(segments)>0:\n",
    "    segGroup1 = group_by_region(segGroup1)\n",
    "if len(subsegments)>0:\n",
    "    subsegGroup1 = group_by_region(subsegGroup1)\n",
    "if len(subcategories)>0:\n",
    "    subcatGroup1 = group_by_region(subcatGroup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95b943e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lis1 = []\n",
    "cat_lis = []\n",
    "if categories:\n",
    "    for i in range(len(catGroup1)):\n",
    "        cat_lis += genrateIndexList(catGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(cat_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "sec_lis = []\n",
    "if sectors:\n",
    "    for i in range(len(secGroup1)):\n",
    "        sec_lis += genrateIndexList(secGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(sec_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "seg_lis=[]\n",
    "if segments:\n",
    "    for i in range(len(segGroup1)):\n",
    "        seg_lis += genrateIndexList(segGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(seg_lis)\n",
    "\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "subseg_lis =[]\n",
    "if subsegments:\n",
    "    for i in range(len(subsegGroup1)):\n",
    "        subseg_lis +=  genrateIndexList(subsegGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(subseg_lis)\n",
    "else:\n",
    "    final_lis1.append([])\n",
    "\n",
    "subcat_lis =[]\n",
    "if subcategories:\n",
    "    for i in range(len(subcatGroup1)):\n",
    "        subcat_lis +=  genrateIndexList(subcatGroup1[i], chartIndex=25, chartCount=4)[0]\n",
    "    final_lis1.append(subcat_lis)\n",
    "else:\n",
    "    final_lis1.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "21faef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "retailer=regions_RET+channels_RET+market_RET\n",
    "channels=regions_CHAN+channels_CHAN+channels_CHAN\n",
    "customarea=regions_CUST+channels_CUST+market_CUST\n",
    "def addarea(modified_dfCategory1,retailer,market=\"RETAILER\"):\n",
    "    keys_to_modify = [k for k in modified_dfCategory1.keys() if k.split(\" | \")[-1] in retailer]\n",
    "    for k in keys_to_modify:\n",
    "        new_key = k + \" | \"+ market\n",
    "        modified_dfCategory1[new_key] = modified_dfCategory1[k]  \n",
    "        del modified_dfCategory1[k]       \n",
    "    return modified_dfCategory1      \n",
    "if len(categories)>0:            \n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,retailer,market=\"RETAILER\")\n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,channels,market=\"CHANNELS\")\n",
    "    modified_dfCategory1=addarea(modified_dfCategory1,customarea,market=f\"{customareas}\")\n",
    "\n",
    "if len(sectors)>0:            \n",
    "    modified_dfSector1=addarea(modified_dfSector1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSector1=addarea(modified_dfSector1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSector1=addarea(modified_dfSector1,customarea,market=f\"{customareas}\")\n",
    "if len(segments)>0:            \n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSegment1=addarea(modified_dfSegment1,customarea,market=f\"{customareas}\")\n",
    "if len(subsegments)>0:            \n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSubSegment1=addarea(modified_dfSubSegment1,customarea,market=f\"{customareas}\")\n",
    "if len(subcategories)>0:            \n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,retailer,market=\"RETAILER\")\n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,channels,market=\"CHANNELS\")\n",
    "    modified_dfSubCategory1=addarea(modified_dfSubCategory1,customarea,market=f\"{customareas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec5211df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(modified_promotionBrandsP12M.keys()):  # Convert to list to avoid runtime errors\n",
    "    df = modified_promotionBrandsP12M[k].copy()\n",
    "    # Filter rows based on 'Top Brands'\n",
    "    df = df[~df['Top Brands'].str.contains('Others', case=False, na=False)]\n",
    "    df = df[~df['Top Brands'].str.contains('Grand Total', case=False, na=False)]\n",
    "    df = df[df['Value Share'] > 0.01]\n",
    "    if not df.empty:\n",
    "        modified_promotionBrandsP12M[k] = df\n",
    "    else:\n",
    "        del modified_promotionBrandsP12M[k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee2a74",
   "metadata": {},
   "source": [
    "\n",
    "## Slide duplication: index, duplication and section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ceca67f7-038d-44e1-98aa-cbc9a32d8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [*[0]*5,\n",
    "         #*[1]*5,\n",
    "         *[1]*5,\n",
    "         *[2]*5,\n",
    "         *[3]*5,\n",
    "         *[4]*5,\n",
    "         *[5]*5,\n",
    "         *[6]*5,\n",
    "         *[7]*5,\n",
    "         *[8]*4,\n",
    "         *[9]*5,\n",
    "         *[10]*5,\n",
    "         *[11]*5,\n",
    "         *[12]*5,\n",
    "         *[13]*5,\n",
    "         *[14]*5,\n",
    "         *final_lis,\n",
    "         *[19]*5,\n",
    "         *final_lis0,\n",
    "         *final_lis1,\n",
    "         *[0]*5,\n",
    "         *[1]*5,\n",
    "         *[2]*5,\n",
    "         *[9]*5,\n",
    "         *[10]*5,\n",
    "        #  *[11]*5,\n",
    "         *[12]*5,\n",
    "         *[13]*5\n",
    "         #*[14]*5\n",
    "        ]\n",
    "duplication = combine_duplications(Scope,count_df,[#modified_promotionBrandsP12M, #0\n",
    "                                                   promotionsBrandSortedTotalFinal, #1\n",
    "                                                   newpromotionsNotBrandsWithMarket, #2\n",
    "                                                   concated, #3\n",
    "                                                   modified_promotionProductsP12M_volumeuplift, #4\n",
    "                                                   new_modified_promotionProductsP12M, #5\n",
    "                                                   new_modified_promotionProductsP12M, #6\n",
    "                                                   top20clientonly, #7\n",
    "                                                   bottom20clientonly,#8\n",
    "                                                   modified_promotionBrandsP12M, #10\n",
    "                                                   newModifiedBrands, #11\n",
    "                                                   PromoSalesTypes_data if promo_type else None,#12\n",
    "                                                   modified_promotionBrandsP12M if feature_share else None, #13\n",
    "                                                   modified_promotionBrandsP12M if display_share else None, #14\n",
    "                                                   modified_promotionEndOfWeek,#15\n",
    "                                                   PromoRet, #16-19\n",
    "                                                   modified_valueUplift, #20\n",
    "                                                   #month_year1,#21\n",
    "                                                   #modified_endofweek_P12M, #22\n",
    "                                                   #modified_promotionBrandsP12M, #0 with no client\n",
    "                                                   promotionsBrandNOTSortedTotalFinal, #1 with no client\n",
    "                                                   newpromotionsNotBrandsWithMarket, #2 with no client\n",
    "                                                   concated, #3 with no client\n",
    "                                                   modified_promotionBrandsP12M, # 10 with no client\n",
    "                                                   newModifiedBrands, #11 with no client\n",
    "                                                   #PromoSalesTypes_data if promo_type else None,#12  with no client\n",
    "                                                   modified_promotionBrandsP12M if feature_share else None, #13  with no client\n",
    "                                                   modified_promotionBrandsP12M if display_share else None #14 with no client\n",
    "                                                  ])\n",
    "section_names = [#\"Promo Value Sale\",#0\n",
    "                 \"Promo Evolution\", #1\n",
    "                 \"VSOD Summary by Sector\" , #2\n",
    "                 \"Value uplift by retailer by brand\", #3 \n",
    "                 \"Volume Uplift vs discount depth\",#4\n",
    "                 \"Value Uplift vs Promo Efficiency Quadrant\", #5\n",
    "                 \"Top 20 promotions\", #6\n",
    "                 \"Top 20 promotions CLIENT ONLY\", #7\n",
    "                 \"Bottom 20 promotions CLIENT ONLY\", #8\n",
    "                 \"Promo share vs Value Share\", #10\n",
    "                 \"Promo Sales by total size\",#11\n",
    "                 \"Promo Sales by promo type\", #12\n",
    "                 \"Feature Share vs Fair Share\", #13\n",
    "                 \"Display Share vs Fair Share\", #14\n",
    "                 \"Promo Frequency learnings\", #15\n",
    "                 \"Promo sales per retailer\", #16-19\n",
    "                 \"Value Uplift vs discount depth\" ,#20\n",
    "                 #\"Seasonality Index\",#21\n",
    "                 #\"Promotional Frequency Analysis\", #22\n",
    "                 #\"Promo Value Sale no client prio\",\n",
    "                 \"Promo Evolution no client prio\",\n",
    "                 \"VSOD Summary by Sector no client prio\",\n",
    "                 \"Value uplift by retailer by brand no client prio\",\n",
    "                \"Promo share vs Value Share no client prio\", #10\n",
    "                 \"Promo Sales by total size no client prio\",#11\n",
    "                 #\"Promo Sales by promo type no client prio\", #12\n",
    "                 \"Feature Share vs Fair Share no client prio\", #13\n",
    "                 \"Display Share vs Fair Share no client prio\" #14\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "#duplication.insert(89, 0)\n",
    "\n",
    "if len(sectors) > 0:\n",
    "       #duplication.insert(45,(len(client_manuf)+len(client_brands))*len(categories)* len(marketList))\n",
    "       duplication.insert(40, sect_vsod_count)\n",
    "if len(segments) > 0:\n",
    "        #duplication.insert(46,(len(client_manuf)+len(client_brands))*len(sectors)* len(marketList)) \n",
    "         duplication.insert(41, seg_vsod_count)\n",
    " \n",
    "else:\n",
    "    duplication.insert(41,0)  \n",
    "  \n",
    "if len(subsegments) > 0:\n",
    "        #duplication.insert(47,(len(client_manuf)+len(client_brands))*len(segments)* len(marketList))\n",
    "        duplication.insert(42, subseg_vsod_count)\n",
    "\n",
    "else:\n",
    "    duplication.insert(42,0)\n",
    "\n",
    "if len(subcategories) > 0:\n",
    "        #duplication.insert(48,(len(client_manuf)+len(client_brands))*len(segments)* len(marketList))\n",
    "        duplication.insert(43, subcat_vsod_count)\n",
    "\n",
    "else:\n",
    "    duplication.insert(43,0)\n",
    "\n",
    "\n",
    "duplication.insert(84,1)\n",
    "duplication.insert(85, 1)\n",
    "duplication.insert(86, 1)\n",
    "duplication.insert(87, 1)\n",
    "duplication.insert(88, 0)\n",
    "duplication.insert(89, 1)\n",
    "duplication.insert(90, 1)\n",
    "duplication.insert(91, 1)\n",
    "duplication.insert(92, 1)\n",
    "duplication.insert(93, 0)\n",
    "\n",
    "section_names = [f\"{name} {suffix}\" for name in section_names for suffix in suffixes]\n",
    "\n",
    "section_names.insert(40,'Volume Sold on Deal Sector')\n",
    "section_names.insert(41,'Volume Sold on Deal Segment')\n",
    "section_names.insert(42,'Volume Sold on Deal SubSegment')\n",
    "section_names.insert(43,'Volume Sold on Deal SubCategory')\n",
    "\n",
    "section_names.insert(84,'Seasonality Index Category')\n",
    "section_names.insert(85,'Seasonality Index Sector')\n",
    "section_names.insert(86,'Seasonality Index Segment')\n",
    "section_names.insert(87,'Seasonality Index Subsegment')\n",
    "section_names.insert(88,'Seasonality Index Subcategory')\n",
    "\n",
    "section_names.insert(89,'Promotional Frequency Analysis Category')\n",
    "section_names.insert(90,'Promotional Frequency Analysis Sector')\n",
    "section_names.insert(91,'Promotional Frequency Analysis Segment')\n",
    "section_names.insert(92,'Promotional Frequency Analysis Subsegment')\n",
    "section_names.insert(93,'Promotional Frequency Analysis Subcategory')\n",
    "#section_names.insert(94,'Promotional Frequency Analysis Subcategory')\n",
    "\n",
    "duplication[77]=1\n",
    "#index = [i for i in index if i != []]\n",
    "# duplication = [i for i in duplication if i != []]\n",
    "#\n",
    "\n",
    "path = os.getcwd() + '//Promotion base.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated.pptx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bbcf3",
   "metadata": {},
   "source": [
    "### Deck 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a93e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, [17, 18], [16, 17, 16, 18], [16, 17, 16, 18, 18], [], [], 19, 19, 19, 19, 19, [20, 20], [21, 21], [20, 20, 21, 21], [], [], [28, 28], [27, 28, 27, 29], [27, 28, 27, 28, 29], [], [], 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13]\n",
      "[2, 4, 6, 0, 0, 0, 2, 2, 0, 0, 1, 2, 3, 0, 0, 2, 4, 6, 0, 0, 2, 4, 6, 0, 0, 2, 4, 6, 0, 0, 2, 4, 5, 0, 0, 2, 4, 5, 0, 0, 7, 14, 0, 0, 2, 4, 6, 0, 0, 1, 2, 3, 0, 0, 2, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 11, 15, 0, 0, 1, 1, 1, 1, 0, 5, 7, 10, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 4, 6, 0, 0, 0, 2, 2, 0, 0, 1, 2, 3, 0, 0, 2, 4, 6, 0, 0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['Promo Evolution Category', 'Promo Evolution Sector', 'Promo Evolution Segment', 'Promo Evolution SubSegment', 'Promo Evolution SubCategory', 'VSOD Summary by Sector Category', 'VSOD Summary by Sector Sector', 'VSOD Summary by Sector Segment', 'VSOD Summary by Sector SubSegment', 'VSOD Summary by Sector SubCategory', 'Value uplift by retailer by brand Category', 'Value uplift by retailer by brand Sector', 'Value uplift by retailer by brand Segment', 'Value uplift by retailer by brand SubSegment', 'Value uplift by retailer by brand SubCategory', 'Volume Uplift vs discount depth Category', 'Volume Uplift vs discount depth Sector', 'Volume Uplift vs discount depth Segment', 'Volume Uplift vs discount depth SubSegment', 'Volume Uplift vs discount depth SubCategory', 'Value Uplift vs Promo Efficiency Quadrant Category', 'Value Uplift vs Promo Efficiency Quadrant Sector', 'Value Uplift vs Promo Efficiency Quadrant Segment', 'Value Uplift vs Promo Efficiency Quadrant SubSegment', 'Value Uplift vs Promo Efficiency Quadrant SubCategory', 'Top 20 promotions Category', 'Top 20 promotions Sector', 'Top 20 promotions Segment', 'Top 20 promotions SubSegment', 'Top 20 promotions SubCategory', 'Top 20 promotions CLIENT ONLY Category', 'Top 20 promotions CLIENT ONLY Sector', 'Top 20 promotions CLIENT ONLY Segment', 'Top 20 promotions CLIENT ONLY SubSegment', 'Top 20 promotions CLIENT ONLY SubCategory', 'Bottom 20 promotions CLIENT ONLY Category', 'Bottom 20 promotions CLIENT ONLY Sector', 'Bottom 20 promotions CLIENT ONLY Segment', 'Bottom 20 promotions CLIENT ONLY SubSegment', 'Bottom 20 promotions CLIENT ONLY SubCategory', 'Volume Sold on Deal Sector', 'Volume Sold on Deal Segment', 'Volume Sold on Deal SubSegment', 'Volume Sold on Deal SubCategory', 'Promo share vs Value Share Category', 'Promo share vs Value Share Sector', 'Promo share vs Value Share Segment', 'Promo share vs Value Share SubSegment', 'Promo share vs Value Share SubCategory', 'Promo Sales by total size Category', 'Promo Sales by total size Sector', 'Promo Sales by total size Segment', 'Promo Sales by total size SubSegment', 'Promo Sales by total size SubCategory', 'Promo Sales by promo type Category', 'Promo Sales by promo type Sector', 'Promo Sales by promo type Segment', 'Promo Sales by promo type SubSegment', 'Promo Sales by promo type SubCategory', 'Feature Share vs Fair Share Category', 'Feature Share vs Fair Share Sector', 'Feature Share vs Fair Share Segment', 'Feature Share vs Fair Share SubSegment', 'Feature Share vs Fair Share SubCategory', 'Display Share vs Fair Share Category', 'Display Share vs Fair Share Sector', 'Display Share vs Fair Share Segment', 'Display Share vs Fair Share SubSegment', 'Display Share vs Fair Share SubCategory', 'Promo Frequency learnings Category', 'Promo Frequency learnings Sector', 'Promo Frequency learnings Segment', 'Promo Frequency learnings SubSegment', 'Promo Frequency learnings SubCategory', 'Promo sales per retailer Category', 'Promo sales per retailer Sector', 'Promo sales per retailer Segment', 'Promo sales per retailer SubSegment', 'Promo sales per retailer SubCategory', 'Value Uplift vs discount depth Category', 'Value Uplift vs discount depth Sector', 'Value Uplift vs discount depth Segment', 'Value Uplift vs discount depth SubSegment', 'Value Uplift vs discount depth SubCategory', 'Seasonality Index Category', 'Seasonality Index Sector', 'Seasonality Index Segment', 'Seasonality Index Subsegment', 'Seasonality Index Subcategory', 'Promotional Frequency Analysis Category', 'Promotional Frequency Analysis Sector', 'Promotional Frequency Analysis Segment', 'Promotional Frequency Analysis Subsegment', 'Promotional Frequency Analysis Subcategory', 'Promo Evolution no client prio Category', 'Promo Evolution no client prio Sector', 'Promo Evolution no client prio Segment', 'Promo Evolution no client prio SubSegment', 'Promo Evolution no client prio SubCategory', 'VSOD Summary by Sector no client prio Category', 'VSOD Summary by Sector no client prio Sector', 'VSOD Summary by Sector no client prio Segment', 'VSOD Summary by Sector no client prio SubSegment', 'VSOD Summary by Sector no client prio SubCategory', 'Value uplift by retailer by brand no client prio Category', 'Value uplift by retailer by brand no client prio Sector', 'Value uplift by retailer by brand no client prio Segment', 'Value uplift by retailer by brand no client prio SubSegment', 'Value uplift by retailer by brand no client prio SubCategory', 'Promo share vs Value Share no client prio Category', 'Promo share vs Value Share no client prio Sector', 'Promo share vs Value Share no client prio Segment', 'Promo share vs Value Share no client prio SubSegment', 'Promo share vs Value Share no client prio SubCategory', 'Promo Sales by total size no client prio Category', 'Promo Sales by total size no client prio Sector', 'Promo Sales by total size no client prio Segment', 'Promo Sales by total size no client prio SubSegment', 'Promo Sales by total size no client prio SubCategory', 'Feature Share vs Fair Share no client prio Category', 'Feature Share vs Fair Share no client prio Sector', 'Feature Share vs Fair Share no client prio Segment', 'Feature Share vs Fair Share no client prio SubSegment', 'Feature Share vs Fair Share no client prio SubCategory', 'Display Share vs Fair Share no client prio Category', 'Display Share vs Fair Share no client prio Sector', 'Display Share vs Fair Share no client prio Segment', 'Display Share vs Fair Share no client prio SubSegment', 'Display Share vs Fair Share no client prio SubCategory']\n",
      "129\n",
      "129\n",
      "129\n",
      "[[17, 18], [16, 17, 16, 18], [16, 17, 16, 18, 18], [], []]\n",
      "[[28, 28], [27, 28, 27, 29], [27, 28, 27, 28, 29], [], []]\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "print(index)\n",
    "print(duplication)\n",
    "print(section_names)\n",
    "print(len(index))\n",
    "print(len(duplication))\n",
    "print(len(section_names))\n",
    "print(final_lis)\n",
    "print(final_lis1)\n",
    "print(sum(duplication))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "474f4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830c75f",
   "metadata": {},
   "source": [
    "## Replace duplicated slides with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78376167-8225-4f4f-af41-cd366f3e2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation(new_pre)\n",
    "posItr = 0\n",
    "ind =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promoValueSales(prs, promotionsBrandDF, numOfDuplicates, position=0):\n",
    "    \"\"\"\n",
    "    Generate PowerPoint slides for promo value sales.\n",
    "\n",
    "    Args:\n",
    "    - prs (pptx.presentation): PowerPoint presentation object.\n",
    "    - promotionsBrandDF (dict): Dictionary of DataFrames containing promotion data for different markets.\n",
    "    - numOfDuplicates (int): Number of slides to duplicate for different markets.\n",
    "    - position (int): Position to start adding slides in the presentation.\n",
    "\n",
    "    Returns:\n",
    "    - Replace the slides with new data\n",
    "    \"\"\"\n",
    "    # Loop through each slide number\n",
    "    slidenum = 0\n",
    "    for key,df in promotionsBrandDF.items():\n",
    "        # Retrieve DataFrame for the current market\n",
    "        df = promotionsBrandDF[key].reset_index(drop=True)\n",
    "        \n",
    "        # Remove rows with 'Others' in 'Top Brands' column and filter by 'Value Share'\n",
    "        df = df[~df['Top Brands'].str.contains('Others', case=False)]\n",
    "        df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    "        df = df[df['Value Share'] > 0.01]\n",
    "     \n",
    "  \n",
    "        # Select client brands\n",
    "        df_client = selectClientBrands(df,'Top Brands', 'Promo Value')\n",
    "        number_of_brands_needed = max(6 - len(df_client),0)\n",
    "     \n",
    "        # Filter top brands and concatenate with client brands\n",
    "        df = df[~df['Top Brands'].isin(client_brands)]\n",
    "        df = df.sort_values(by='Promo Value', ascending=False).head(number_of_brands_needed)\n",
    "        df = pd.concat([df, df_client], ignore_index=True)\n",
    "        df = df.sort_values(by='Promo Value', ascending=False)\n",
    "    \n",
    "        \n",
    "        # Update title\n",
    "        shapes = prs.slides[slidenum + position].shapes\n",
    "        titlNumber = get_shape_number(shapes, \"Promo Value Sales | Category | National | P12M\")\n",
    "        headerNumber = get_shape_number(shapes, \"Promo Value Sales (Replace With SO WHAT)\")\n",
    "        shapes[titlNumber - 1].text = data_source\n",
    "        shapes[headerNumber].text_frame.paragraphs[0].font.size = Pt(16)\n",
    "        shapes[headerNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "        shapes[titlNumber].text = shapes[titlNumber].text.replace('Category', key.split(' | ')[0]).replace(\n",
    "            'National', key.split(' | ')[1])\n",
    "        shapes[titlNumber].text_frame.paragraphs[0].font.size = Pt(12)\n",
    "        shapes[titlNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "        \n",
    "        # Create table and chart\n",
    "        tables, charts = createTableAndChart(shapes)\n",
    "        table = tables[0].table\n",
    "        \n",
    "        # Remove unnecessary rows\n",
    "        num_rows_to_remove = len(table.rows) - df['Top Brands'].nunique() - 1\n",
    "        table_height = get_table_height(table)\n",
    "        for _ in range(num_rows_to_remove):\n",
    "            if len(table.rows) > 1:\n",
    "                row = table.rows[1]\n",
    "                remove_row(table, row)\n",
    "        \n",
    "        # Adjust row heights\n",
    "        total_row_height = table_height - table.rows[0].height\n",
    "        num_rows = len(table.rows) - 1\n",
    "        if num_rows > 0:\n",
    "            cell_height = total_row_height / num_rows\n",
    "            for row in range(1, table.rows.__len__()):\n",
    "                table.rows[row].height = int(cell_height)\n",
    "        \n",
    "        # Populate table cells\n",
    "        for i, row in enumerate(table.rows):\n",
    "            for j, cell in enumerate(row.cells):\n",
    "                if i == 0:  # Header row\n",
    "                    continue\n",
    "                if j == 0:  # Brand column\n",
    "                    cell.text = df['Top Brands'].iloc[i - 1]\n",
    "                    cell.text_frame.paragraphs[0].font.name = 'Nexa Bold'\n",
    "                elif j == 1:  # Promo Value sales column\n",
    "                    value = df['Promo Value'].iloc[i - 1]\n",
    "                    if len(str(value)) > 3:\n",
    "                        formatted_value = '{:,}'.format(int(value))\n",
    "                        cell.text = str(formatted_value)\n",
    "                        cell.text_frame.paragraphs[0].font.name = 'Nexa Book'\n",
    "                    else:\n",
    "                        cell.text = str(df['Promo Value'].iloc[i - 1])\n",
    "                        cell.text_frame.paragraphs[0].font.name = 'Nexa Book'\n",
    "                elif j == 2:  # Volume Sold on Deal (VSOD) column\n",
    "                    cell.text = str(int(round(df['VSOD'].replace(np.nan, 0).iloc[i - 1] * 100, 0))) + '%'\n",
    "                    cell.text_frame.paragraphs[0].font.name = 'Nexa Book'\n",
    "                else:  # VSOD IYA column\n",
    "                    cell.text = str(int(round(df['VSOD IYA'].replace(np.nan, 0).iloc[i - 1] * 100, 0)))\n",
    "                    cell.text_frame.paragraphs[0].font.name = 'Nexa Book'\n",
    "                # Set font size and alignment\n",
    "                cell.text_frame.paragraphs[0].font.size = Pt(8)\n",
    "                cell.text_frame.paragraphs[0].alignment = PP_ALIGN.CENTER\n",
    "        slidenum +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49aea0-21ff-4bf4-9427-d2f7a6b8a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 2\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(promotionsBrandSortedTotalFinal,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in promotionsBrandSortedTotalFinal.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            promoEvolutionNew(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6850ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 12\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd4fcae-6d85-4244-82e5-9a19f05b6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 3\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newpromotionsBrandsWithMarket,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newpromotionsBrandsWithMarket.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VSOD1(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92914c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 16\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valueUpliftRetailer(prs, concated, numOfDuplicates, position=0):\n",
    "    \"\"\"\n",
    "    This function updates a PowerPoint presentation with value uplift by brand for different categories/sectors.\n",
    "    It modifies the slide shapes and populates tables and charts with the value uplift data.\n",
    " \n",
    "    Parameters:\n",
    "    prs (Presentation): The PowerPoint presentation object to modify.\n",
    "    concated (dict): A dictionary where keys are category/sector names and values are DataFrames containing value uplift data.\n",
    "    numOfDuplicates (int): The number of slides to duplicate and update.\n",
    "    position (int, optional): The starting slide position in the presentation. Defaults to 0.\n",
    " \n",
    "    Returns:\n",
    "    Replace the slides with new data\n",
    "    \"\"\"\n",
    "    for key, slide_num in zip(concated, range(numOfDuplicates)):\n",
    "        # Get the DataFrame for the current category/sector\n",
    "        df = concated[key]\n",
    "        df = df[df['Promo Value'] > 0].reset_index(drop=True)\n",
    "        df = df[~df['Top Brands'].str.contains('Grand Total', case=False)]\n",
    " \n",
    "        # Get the current slide and its shapes\n",
    "        slide = prs.slides[slide_num + position]\n",
    "        shapes = slide.shapes\n",
    "       \n",
    "        # Update the title shape with the category/sector name\n",
    "        titleNumber = get_shape_number(shapes, \"Value Uplift by brand | Category/Sector | P12M\")\n",
    "        headereNumber = get_shape_number(shapes, \"Value uplift by retailer by brand (Replace With SO WHAT)\")\n",
    "        shapes[titleNumber-1].text = data_source\n",
    "        shapes[titleNumber].text = shapes[titleNumber].text.replace('Category/Sector', key)\n",
    "        shapes[titleNumber].text_frame.paragraphs[0].font.size = Pt(12)\n",
    "        shapes[titleNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "        shapes[headereNumber].text_frame.paragraphs[0].font.size = Pt(16)\n",
    "        shapes[headereNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    " \n",
    "        # Create and retrieve table and chart shapes\n",
    "        tables, charts = createTableAndChart(slide.shapes)\n",
    "        chart = charts[0].chart\n",
    "        table = tables[0].table\n",
    "       \n",
    "        # Get the list of markets (sources)\n",
    "        market_list = list(df['SOURCE'].unique())\n",
    "       \n",
    "        # Adjust the number of columns in the table\n",
    "        num_cols_to_remove = len(table.columns) - len(market_list)\n",
    "        table_width = get_table_width(table)\n",
    "        for _ in range(num_cols_to_remove):\n",
    "            if len(table.columns):  # Skip removing the first row if there is more than one row\n",
    "                col = table.columns[0]\n",
    "                remove_col(table, col)\n",
    "        if num_cols_to_remove:\n",
    "            total_col_width = table_width\n",
    "            num_cols = len(table.columns)\n",
    "            if num_cols > 0:\n",
    "                cell_width = total_col_width / num_cols\n",
    "                for col in range(0, table.columns.__len__()):\n",
    "                    table.columns[col].width = int(cell_width)\n",
    " \n",
    "        # Populate the table with market names\n",
    "        for row_number, row in enumerate(table.rows, start=0):\n",
    "            for column_num, cell in enumerate(row.cells):\n",
    "                value = market_list[column_num]\n",
    "                cell.text = str(value)\n",
    "                set_cell_font(cell, 'Nexa Bold', 8)\n",
    "                cell.text_frame.paragraphs[0].alignment = PP_ALIGN.CENTER\n",
    "                cell.text_frame.paragraphs[0].font.color.rgb = RGBColor(87, 85, 85)\n",
    "                cell.text_frame.paragraphs[0].font.bold = False\n",
    "       \n",
    "        # Prepare chart data \n",
    "        chart_data = BubbleChartData()\n",
    "        chart_data.categories = [i for i in range((df['SOURCE'].nunique() * 2) + 1)]\n",
    "        series = chart_data.add_series('Average Value Uplift')\n",
    "        catPos = [i for i in range(1, len(chart_data.categories), 2)]\n",
    " \n",
    "        # Populate the chart with value uplift data\n",
    "        filtered_brands_list = []\n",
    "\n",
    "        for i, source in enumerate(df['SOURCE'].unique()):\n",
    "            dfSource = df[df['SOURCE'] == source]\n",
    "            dfSource = dfSource[~dfSource['Top Brands'].str.contains('Others', case=False)]\n",
    "            dfSource = dfSource[dfSource['Value Share'] > 0.01]\n",
    "            df_client = selectClientBrands(dfSource,'Top Brands', 'Value Share')\n",
    "            number_of_brands_needed = max(10 - len(df_client),0)\n",
    "            dfSource = dfSource[~dfSource['Top Brands'].isin(client_brands)]\n",
    "            dfSource = dfSource.sort_values(by='Value Share', ascending=False).head(number_of_brands_needed)\n",
    "            dfSource = pd.concat([dfSource, df_client], ignore_index=True)\n",
    "            series.has_data_labels = True\n",
    "            start = len(series)\n",
    "            brands = dfSource['Top Brands'].unique()\n",
    "             # ✅ Collect filtered brands\n",
    "            if isinstance(dfSource, pd.DataFrame) and 'Top Brands' in dfSource.columns:\n",
    "                filtered_brands_list.append(dfSource[['Top Brands']].copy())\n",
    "            # print(key,filtered_brands_list)\n",
    "# Concatenate all DataFrames and get unique brand names\n",
    "            all_top_brands = pd.concat(filtered_brands_list, ignore_index=True)\n",
    "            # unique_brands = all_top_brands['Top Brands'].tolist()\n",
    "\n",
    "            # print(key, filtered_brands_list)\n",
    "\n",
    "            # brands = dfSource['Top Brands'].unique()\n",
    "            # brandslis = dfSource['Top Brands'].tolist()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            for brand in brands:\n",
    "                if normalized:\n",
    "                    series.add_data_point(catPos[i], dfSource[dfSource['Top Brands'] == brand]['Value Uplift (v. base) Normalized'].unique()[0], dfSource[dfSource['Top Brands'] == brand]['Promo Value'].unique()[0])\n",
    "                else: \n",
    "                    series.add_data_point(catPos[i], dfSource[dfSource['Top Brands'] == brand]['Value Uplift (v. base)'].unique()[0], dfSource[dfSource['Top Brands'] == brand]['Promo Value'].unique()[0])\n",
    "            chart.replace_data(chart_data)\n",
    "            value_axis = chart.category_axis\n",
    "            value_axis.minimum_scale = 0\n",
    "            value_axis.maximum_scale = (df['SOURCE'].nunique() * 2)\n",
    " \n",
    "            # chart.replace_data(chart_data)\n",
    "            \n",
    "            # all_top_brands = pd.concat(filtered_brands_list, ignore_index=True)\n",
    "            \n",
    "            # print(f\"{key}: All Top Brands after filtering and concat:\\n\", all_top_brands['Top Brands'])\n",
    "            xlsx_file=BytesIO()\n",
    "            with chart_data._workbook_writer._open_worksheet(xlsx_file) as (workbook, worksheet):\n",
    "                chart_data._workbook_writer._populate_worksheet(workbook, worksheet)\n",
    "                worksheet.write(0, 4, \"labels\")\n",
    "                worksheet.write_column(1, 4,all_top_brands['Top Brands'], None)\n",
    "            chart._workbook.update_from_xlsx_blob(xlsx_file.getvalue())\n",
    " \n",
    "            stop = len(chart.series[0].points)\n",
    "            clibrands=all_top_brands['Top Brands']\n",
    "            for j in range(start, stop):\n",
    "                point = chart.series[0].points[j]\n",
    "                # data_label = point.data_label\n",
    "                # data_label.has_text_frame = True\n",
    "                # data_label.text_frame.text = brands[j % len(brands)]\n",
    "                # data_label.text_frame.paragraphs[0].runs[0].font.size = Pt(8)\n",
    "                # data_label.position = XL_LABEL_POSITION.CENTER\n",
    "                point.format.fill.solid()\n",
    "                if clibrands[j % len(clibrands)] in client_brands:\n",
    "                    point.format.fill.fore_color.rgb = RGBColor(126, 202, 196)\n",
    "                else:\n",
    "                    point.format.fill.fore_color.rgb = RGBColor(230, 229, 229)\n",
    " \n",
    "        # Set the axis scale for the chart\n",
    "        value_axis = chart.category_axis\n",
    "        value_axis.minimum_scale = 0\n",
    "        value_axis.maximum_scale = (df['SOURCE'].nunique() * 2)\n",
    " \n",
    "        # chart.replace_data(chart_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8dc777-7e02-4573-b8f9-000bb04fcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 4\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(concated,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in concated.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUpliftRetailer(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27312e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 22\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ee2a2-5cdf-41f3-ad8a-95fb9baacc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 5\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionProductsP12M_volumeuplift,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionProductsP12M_volumeuplift.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VolumeUplift(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f24f2e-b093-4fe6-8605-1ef06f69e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 6\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(new_modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in new_modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            ValueUpliftvsPromoEfficiencyQuadrant(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c3cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 46\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba714fd4-a9da-45d9-9eb0-1a1889324bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 7\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(new_modified_promotionProductsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in new_modified_promotionProductsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            top20(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477805d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 58\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90b497-4ca3-4467-9965-3eeba68fef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 8\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(top20clientonly,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in top20clientonly.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            top20Client(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578ca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 69\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108f604-685c-4fd8-aab4-5c66967a67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 9\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(bottom20clientonly,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in bottom20clientonly.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            bot20Client(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16765a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 80\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a2079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "if len(sectors)>0:\n",
    "    newVolumeSold(prs, sect_vsod_merged, position=posItr, parent=direct_parent['Sector'], child = 'Sector')\n",
    "    print(posItr)\n",
    "    posItr += sect_vsod_count\n",
    "    ind +=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a0620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 87\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41024fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(segments)>0:\n",
    "    newVolumeSold(prs, seg_vsod_merged, position=posItr, parent=direct_parent['Segment'], child = 'Segment')\n",
    "    posItr += seg_vsod_count\n",
    "    ind +=1\n",
    "    \n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 101\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subsegments)>0:\n",
    "    newVolumeSold(prs, subseg_vsod_merged, position=posItr, parent=direct_parent['SubSegment'], child = 'SubSegment')\n",
    "    posItr += subseg_vsod_count\n",
    "    ind+=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 101\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subcategories)>0:\n",
    "    newVolumeSold(prs, subcat_vsod_merged, position=posItr, parent=direct_parent['SubCategory'], child = 'SubCategory')\n",
    "    posItr += subcat_vsod_count\n",
    "    ind+=1\n",
    "else:\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee42400-58ae-4311-8ac7-9914c0ae666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 11\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoShare_vs_ValueShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b6a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 113\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569341f7-9b94-4bf5-9114-fe10a6d0cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Shave Men | Walmart\n",
      "6\n",
      "System | Walmart\n",
      "6\n",
      "Disposables | Walmart\n",
      "6\n",
      "Disposables | Walmart\n",
      "6\n",
      "Razors | Walmart\n",
      "6\n",
      "Refills | Walmart\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# slide 12\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newModifiedBrands,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newModifiedBrands.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoSalesTotalSize(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 119\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6331b0-3674-471a-8da0-d10719174ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Shave Men | Bj's And Sam's\n",
      "Manual Shave Men | Walmart\n",
      "System | Bj's And Sam's\n",
      "Disposables | Bj's And Sam's\n",
      "System | Walmart\n",
      "Disposables | Walmart\n",
      "Disposables | Bj's And Sam's\n",
      "Razors | Bj's And Sam's\n",
      "Disposables | Walmart\n",
      "Razors | Walmart\n",
      "Refills | Walmart\n"
     ]
    }
   ],
   "source": [
    "# slide 13\n",
    "if promo_type:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(PromoSalesTypes_data,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in PromoSalesTypes_data.items() if key in dict[key1]}\n",
    "            if filtered_dict:\n",
    "                PromoSalesTypes(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 130\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555338fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 14\n",
    "if feature_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                featureShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ecb5b3-8308-43ec-b4aa-16c2e7b0797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# slide 15\n",
    "if display_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                displayShare(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16eefd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 130\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffadfa-5f94-4967-84f6-54c1d2ab7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 16\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionEndOfWeek,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionEndOfWeek.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoFrequency(prs,filtered_dict,duplication[ind],position=sum(duplication[:ind]))\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 163\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43caf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if categories:\n",
    "    catFinal = sorted(splitDfsPromo(dfCategory,(client_manuf) ,genrateIndexList(catGroup[0])[0]))\n",
    "    catFinal = catFinal+sorted(splitDfsPromo(dfCategory,(client_brands) ,genrateIndexList(catGroup[0])[0]))\n",
    "    catFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 163\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05117abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sectors:\n",
    "    secFinal = sorted(splitDfsPromo(dfSector,(client_manuf)  ,genrateIndexList(secGroup[0])[0]))\n",
    "    secFinal = secFinal + sorted(splitDfsPromo(dfSector,(client_brands)  ,genrateIndexList(secGroup[0])[0]))\n",
    "    secFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5855f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 163\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if segments:\n",
    "    segFinal = sorted(splitDfsPromo(dfSegment,(client_manuf)  ,genrateIndexList(segGroup[0])[0]))\n",
    "    segFinal = segFinal+sorted(splitDfsPromo(dfSegment,(client_brands)  ,genrateIndexList(segGroup[0])[0]))\n",
    "    segFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if subsegments:\n",
    "    subsegFinal = sorted(splitDfsPromo(dfSubSegment,(client_manuf)  ,genrateIndexList(subsegGroup[0])[0]))\n",
    "    subsegFinal = subsegFinal + sorted(splitDfsPromo(dfSubSegment,(client_brands)  ,genrateIndexList(subsegGroup[0])[0]))\n",
    "    subsegFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3153b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if subcategories:\n",
    "    subcatFinal = sorted(splitDfsPromo(dfSubCategory,(client_manuf) ,genrateIndexList(subcatGroup[0])[0]))\n",
    "    subcatFinal = subcatFinal+sorted(splitDfsPromo(dfSubCategory,(client_brands) ,genrateIndexList(subcatGroup[0])[0]))\n",
    "    subcatFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f04b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 163\n"
     ]
    }
   ],
   "source": [
    "print(ind,posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c980e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Manual Shave Men | Edgewell Personal Care | Bj's And Sam's\",\n",
       "  \"Manual Shave Men | Schick | Bj's And Sam's\",\n",
       "  \"Manual Shave Men | Cremo | Bj's And Sam's\"],\n",
       " ['Manual Shave Men | Edgewell Personal Care | Walmart',\n",
       "  'Manual Shave Men | Schick | Walmart',\n",
       "  'Manual Shave Men | Equate | Walmart',\n",
       "  'Manual Shave Men | Cremo | Walmart']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febe962-91f9-4b04-80a0-986f63399c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Manual Shave Men | Edgewell Personal Care | Bj's And Sam's\", \"Manual Shave Men | Schick | Bj's And Sam's\", \"Manual Shave Men | Cremo | Bj's And Sam's\"], ['Manual Shave Men | Edgewell Personal Care | Walmart', 'Manual Shave Men | Schick | Walmart', 'Manual Shave Men | Equate | Walmart', 'Manual Shave Men | Cremo | Walmart']]\n",
      "4 75 165\n",
      "5 76 169\n",
      "0 77 174\n",
      "0 78 174\n"
     ]
    }
   ],
   "source": [
    "#Slide 17\n",
    "#split catGroup into Lists depends on num of charts \n",
    "catGroupSplit = splitListpromo(dfCategory, catGroup, [i-14 for i in index[ind]])\n",
    "print(catGroupSplit)\n",
    "promoSalesPerRetailer(prs,dfCategory,len(index[ind]),catGroupSplit,position=sum(duplication[:ind]))\n",
    "posItr = sum(duplication[:ind]) + len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split secGroup into Lists depends on num of charts \n",
    "if len(sectors) != 0: \n",
    "    secGroupSplit = splitListpromo(dfSector, secGroup, [i-14 for i in index[ind]])\n",
    "    promoSalesPerRetailer(prs,dfSector,len(index[ind]),secGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split segGroup into Lists depends on num of charts \n",
    "if len(segments) != 0: \n",
    "    segGroupSplit = splitListpromo(dfSegment, segGroup, [i-14 for i in index[ind]])\n",
    "    promoSalesPerRetailer(prs,dfSegment,len(index[ind]),segGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split subsegGroup into Lists depends on num of charts \n",
    "if len(subsegments) != 0:\n",
    "    subsegGroupSplit = splitListpromo(dfSubSegment, subsegGroup, [i-14 for i in index[ind]])\n",
    "\n",
    "    promoSalesPerRetailer(prs,dfSubSegment,len(index[ind]),subsegGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(len(index[ind]),ind, posItr)\n",
    "\n",
    "#split subcatGroup into Lists depends on num of charts \n",
    "if len(subcategories) != 0:\n",
    "    subcatGroupSplit = splitListpromo(dfSubCategory, subcatGroup, [i-14 for i in index[ind]])\n",
    "    print(subcatGroupSplit)\n",
    "    promoSalesPerRetailer(prs,dfSubCategory,len(index[ind]),subcatGroupSplit,position=posItr)\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf01c1-6ec8-405c-bb46-928310996eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 179\n",
      "80 186\n",
      "81 196\n",
      "82 196\n",
      "83 196\n"
     ]
    }
   ],
   "source": [
    "# slide 21\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_valueUplift,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_valueUplift.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUplift(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        print(ind,posItr)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1031c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 1 196\n"
     ]
    }
   ],
   "source": [
    "print(ind, duplication[ind], posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edgewell Personal Care\n",
      "Edgewell Personal Care\n",
      "198 2\n"
     ]
    }
   ],
   "source": [
    "if len(categories)>0:\n",
    "    seasonality(prs,dfCategory0, len(index[ind]), categoryGroup0, position=posItr,slideby=\"Category\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Shave Men\n",
      "Manual Shave Men\n",
      "200 4\n"
     ]
    }
   ],
   "source": [
    "if len(sectors)>0:\n",
    "    seasonality(prs, dfSector0, len(index[ind]), secGroup0, position=posItr,slideby=\"Sector\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c95329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disposables\n",
      "Disposables\n",
      "System\n",
      "System\n",
      "204 0 87\n"
     ]
    }
   ],
   "source": [
    "if len(segments)>0:\n",
    "    seasonality(prs, dfSegment0, len(index[ind]), segGroup0, position=posItr,slideby=\"Segment\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "print(posItr, len(index[ind]),ind)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffb4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 0 88\n"
     ]
    }
   ],
   "source": [
    "if len(subsegments) != 0:\n",
    "    seasonality(prs,dfSubSegment0,len(index[ind]),subsegGroup0,position=posItr,slideby=\"SubSegment\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n",
    "\n",
    "print(posItr, len(index[ind]),ind)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(subcategories) != 0:\n",
    "    seasonality(prs,dfSubCategory0,len(index[ind]),subcatGroup0,position=posItr,slideby=\"SubCategory\")\n",
    "    posItr += len(index[ind])\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fdd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 89 1 204\n"
     ]
    }
   ],
   "source": [
    "print(len(index[ind]), ind, duplication[ind], posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 89\n"
     ]
    }
   ],
   "source": [
    "print(posItr,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date1 = pd.to_datetime(end_date)\n",
    "start_date1 = end_date1 - pd.DateOffset(months=12)\n",
    " \n",
    "# Generate all weekly periods (weekly ends, e.g., Sundays) between start and end\n",
    "week_ends = pd.date_range(start=start_date1, end=end_date1, freq='W-SUN')\n",
    " \n",
    "# Convert to list of dates (if needed)\n",
    "week_ends_list = week_ends.to_list()\n",
    "all_weeks_df = pd.DataFrame({'End of Week': week_ends_list})\n",
    "\n",
    "def add_all_weeks(data):\n",
    "    final_data ={}\n",
    "    for key,df in data.items():\n",
    "        df_full = all_weeks_df.merge(df, on='End of Week', how='left')\n",
    "        df_full.fillna(0, inplace=True)\n",
    "        final_data[key] = df_full\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab704be",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dfCategory1 = add_all_weeks(modified_dfCategory1)\n",
    "modified_dfSector1 = add_all_weeks(modified_dfSector1)\n",
    "modified_dfSegment1 = add_all_weeks(modified_dfSegment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f6b6cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "4 206\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "5 210\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0 215\n",
      "0 215\n"
     ]
    }
   ],
   "source": [
    "catGroup1Split = splitListpromo(modified_dfCategory1, catGroup1, [i-25 for i in index[ind]])\n",
    "\n",
    "Promotional_Frequency(prs,modified_dfCategory1,len(index[ind]),catGroup1Split,position=posItr)\n",
    "posItr +=len(catGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "#Sector Replace\n",
    "if len(sectors) != 0: \n",
    "    secGroup1Split = splitListpromo(modified_dfSector1, secGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSector1,len(index[ind]),secGroup1Split,position=posItr)\n",
    "    posItr += len(secGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "\n",
    "if len(segments) != 0: \n",
    "    segGroup1Split = splitListpromo(modified_dfSegment1, segGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSegment1,len(index[ind]),segGroup1Split,position=posItr)\n",
    "    posItr += len(segGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "if len(subsegments) != 0:\n",
    "    subsegGroup1Split = splitListpromo(modified_dfSubSegment1, subsegGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSubSegment1,len(index[ind]),subsegGroup1Split,position=posItr)\n",
    "    posItr += len(subsegGroup1Split)\n",
    "ind+=1\n",
    "print(len(index[ind]), posItr)\n",
    "\n",
    "if len(subcategories) != 0:\n",
    "    subcatGroup1Split = splitListpromo(modified_dfSubCategory1, subcatGroup1, [i-25 for i in index[ind]])\n",
    "    Promotional_Frequency(prs,modified_dfSubCategory1,len(index[ind]),subcatGroup1Split,position=posItr)\n",
    "    posItr += len(subcatGroup1Split)\n",
    "ind+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c746a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slide 1 with no client brands\n",
    "# for key,value in Scope.items():\n",
    "#     dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "#     for key1,value1 in dict.items():\n",
    "#         filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "#         if filtered_dict:\n",
    "#             promoValueSales_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "#             posItr += len(filtered_dict)\n",
    "#         ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "09d3bb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b39417e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 2 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(promotionsBrandNOTSortedTotalFinal,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in promotionsBrandNOTSortedTotalFinal.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            promoEvolutionNew(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "        ind +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0fb051d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "42070784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 3 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newpromotionsNotBrandsWithMarket,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newpromotionsNotBrandsWithMarket.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            VSOD1(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "61072cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7e53b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 4 with no client brands\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(concated,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in concated.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            valueUpliftRetailer_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ead65353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "995b3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 11 with no client prio\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoShare_vs_ValueShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "        ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8d61cb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "print(posItr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "26d694f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 249\n"
     ]
    }
   ],
   "source": [
    "print(ind, posItr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8cd77382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "250 115\n",
      "6\n",
      "7\n",
      "6\n",
      "9\n",
      "252 116\n",
      "6\n",
      "9\n",
      "6\n",
      "9\n",
      "6\n",
      "8\n",
      "255 117\n",
      "255 118\n",
      "255 119\n"
     ]
    }
   ],
   "source": [
    "# slide 12 with no client prio\n",
    "for key,value in Scope.items():\n",
    "    dict = {key: count_df(newModifiedBrands,value) }\n",
    "    for key1,value1 in dict.items():\n",
    "        filtered_dict = {key: value for key, value in newModifiedBrands.items() if key in dict[key1]}\n",
    "        if filtered_dict:\n",
    "            PromoSalesTotalSize_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "        posItr += len(filtered_dict)\n",
    "    ind +=1\n",
    "    print(posItr,ind)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7e142e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 13 with no client prio\n",
    "# if promo_type:\n",
    "#     for key,value in Scope.items():\n",
    "#         dict = {key: count_df(PromoSalesTypes_data,value) }\n",
    "#         for key1,value1 in dict.items():\n",
    "#             filtered_dict = {key: value for key, value in PromoSalesTypes_data.items() if key in dict[key1]}\n",
    "#             if filtered_dict:\n",
    "#                 PromoSalesTypes_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "#             posItr += len(filtered_dict)\n",
    "#             ind +=1\n",
    "# else:\n",
    "#     ind +=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f3c723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 14 with no client prio\n",
    "if feature_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                featureShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bfbef8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide 15 with no client prio\n",
    "if display_share:\n",
    "    for key,value in Scope.items():\n",
    "        dict = {key: count_df(modified_promotionBrandsP12M,value) }\n",
    "        for key1,value1 in dict.items():\n",
    "            filtered_dict = {key: value for key, value in modified_promotionBrandsP12M.items() if key in dict[key1]}\n",
    "            if filtered_dict:    \n",
    "                displayShare_no(prs,filtered_dict,duplication[ind],position=posItr)\n",
    "            posItr += len(filtered_dict)\n",
    "            ind +=1\n",
    "else:\n",
    "    ind +=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217af932",
   "metadata": {},
   "source": [
    "## Output slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c56e0982-087b-439b-a549-736abdbb54b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 17: Opened Excel workbook: Book1\n",
      "Slide 18: Opened Excel workbook: Book1\n",
      "Slide 19: Opened Excel workbook: Book1\n",
      "Slide 20: Opened Excel workbook: Book1\n",
      "Slide 21: Opened Excel workbook: Book1\n",
      "Slide 22: Opened Excel workbook: Book1\n",
      "Slide 23: Opened Excel workbook: Book1\n",
      "Slide 24: Opened Excel workbook: Book1\n",
      "Slide 25: Opened Excel workbook: Book1\n",
      "Slide 26: Opened Excel workbook: Book1\n",
      "Slide 27: Opened Excel workbook: Book1\n",
      "Slide 28: Opened Excel workbook: Book1\n",
      "Slide 29: Opened Excel workbook: Book1\n",
      "Slide 30: Opened Excel workbook: Book1\n",
      "Slide 31: Opened Excel workbook: Book1\n",
      "Slide 32: Opened Excel workbook: Book1\n",
      "Slide 33: Opened Excel workbook: Book1\n",
      "Slide 34: Opened Excel workbook: Book1\n",
      "Slide 35: Opened Excel workbook: Book1\n",
      "Slide 36: Opened Excel workbook: Book1\n",
      "Slide 37: Opened Excel workbook: Book1\n",
      "Slide 38: Opened Excel workbook: Book1\n",
      "Slide 39: Opened Excel workbook: Book1\n",
      "Slide 40: Opened Excel workbook: Book1\n",
      "Slide 41: Opened Excel workbook: Book1\n",
      "Slide 42: Opened Excel workbook: Book1\n",
      "Slide 43: Opened Excel workbook: Book1\n",
      "Slide 44: Opened Excel workbook: Book1\n",
      "Slide 45: Opened Excel workbook: Book1\n",
      "Slide 46: Opened Excel workbook: Book1\n",
      "Slide 175: Opened Excel workbook: Book1\n",
      "Slide 175: Opened Excel workbook: Book1\n",
      "Slide 176: Opened Excel workbook: Book1\n",
      "Slide 176: Opened Excel workbook: Book1\n",
      "Slide 177: Opened Excel workbook: Book1\n",
      "Slide 177: Opened Excel workbook: Book1\n",
      "Slide 178: Opened Excel workbook: Book1\n",
      "Slide 178: Opened Excel workbook: Book1\n",
      "Slide 179: Opened Excel workbook: Book1\n",
      "Slide 179: Opened Excel workbook: Book1\n",
      "Slide 180: Opened Excel workbook: Book1\n",
      "Slide 180: Opened Excel workbook: Book1\n",
      "Slide 181: Opened Excel workbook: Book1\n",
      "Slide 181: Opened Excel workbook: Book1\n",
      "Slide 182: Opened Excel workbook: Book1\n",
      "Slide 182: Opened Excel workbook: Book1\n",
      "Slide 183: Opened Excel workbook: Book1\n",
      "Slide 183: Opened Excel workbook: Book1\n",
      "Slide 184: Opened Excel workbook: Book1\n",
      "Slide 184: Opened Excel workbook: Book1\n",
      "Slide 185: Opened Excel workbook: Book1\n",
      "Slide 185: Opened Excel workbook: Book1\n",
      "Slide 186: Opened Excel workbook: Book1\n",
      "Slide 186: Opened Excel workbook: Book1\n",
      "Slide 187: Opened Excel workbook: Book1\n",
      "Slide 187: Opened Excel workbook: Book1\n",
      "Slide 188: Opened Excel workbook: Book1\n",
      "Slide 188: Opened Excel workbook: Book1\n",
      "Slide 189: Opened Excel workbook: Book1\n",
      "Slide 189: Opened Excel workbook: Book1\n",
      "Slide 190: Opened Excel workbook: Book1\n",
      "Slide 190: Opened Excel workbook: Book1\n",
      "Slide 191: Opened Excel workbook: Book1\n",
      "Slide 191: Opened Excel workbook: Book1\n",
      "Slide 192: Opened Excel workbook: Book1\n",
      "Slide 192: Opened Excel workbook: Book1\n",
      "Slide 193: Opened Excel workbook: Book1\n",
      "Slide 193: Opened Excel workbook: Book1\n",
      "Slide 194: Opened Excel workbook: Book1\n",
      "Slide 194: Opened Excel workbook: Book1\n",
      "Slide 195: Opened Excel workbook: Book1\n",
      "Slide 195: Opened Excel workbook: Book1\n",
      "Slide 196: Opened Excel workbook: Book1\n",
      "Slide 196: Opened Excel workbook: Book1\n",
      "Slide 232: Opened Excel workbook: Book1\n",
      "Slide 233: Opened Excel workbook: Book1\n",
      "Slide 234: Opened Excel workbook: Book1\n",
      "Slide 235: Opened Excel workbook: Book1\n",
      "Slide 236: Opened Excel workbook: Book1\n",
      "Slide 237: Opened Excel workbook: Book1\n"
     ]
    }
   ],
   "source": [
    "outputPath=os.getcwd() + f\"\\\\Promotion {client_manuf[0]}.pptx\"\n",
    "prs.save(outputPath)\n",
    "# # app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "final=os.getcwd() +f\"\\\\Promotion {client_manuf[0]}.pptx\"\n",
    "open_chart_data_in_excel(final,outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f413d6eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\aleaa\\\\Documents\\\\Slide-Automate\\\\Promotion Slide Duplicate/ValueUpliftvsDepth/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[159], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m datasets_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;241m+\u001b[39m path1\n\u001b[1;32m----> 7\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(datasets_path\u001b[38;5;241m+\u001b[39md, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:\\\\Users\\\\aleaa\\\\Documents\\\\Slide-Automate\\\\Promotion Slide Duplicate/ValueUpliftvsDepth/'"
     ]
    }
   ],
   "source": [
    "%run \"..\\general_functions\\generalFunctions.ipynb\"\n",
    "%run \"..\\Promotion Slide Duplicate\\Promotion Replacement Function.ipynb\"\n",
    "\n",
    "path1 = r\"/ValueUpliftvsDepth/\"\n",
    "loaded_data = {}\n",
    "datasets_path = os.getcwd()+ path1\n",
    "datasets = os.listdir(datasets_path)\n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        loaded_data[d.split('.')[0]] = pd.read_csv(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valueUplift_dict = {} # value\n",
    "i=0\n",
    "for key, df in loaded_data.items():\n",
    "    data = DetectHeader(df)\n",
    "    columns_to_ffill = [col for col in data.columns if 'item' in col.lower() or 'product' in col.lower()]\n",
    "    data[columns_to_ffill] = data[columns_to_ffill].fillna(method='ffill')\n",
    "    data = data[~data['Item'].str.contains('Total', case=False)].reset_index(drop=True)\n",
    "    for item in data['Item'].unique():\n",
    "        df = data[data['Item'] == item]\n",
    "        df['Discount Depth (%)'] = df['Discount Depth (%)'].str.replace('%','').astype(float) /100\n",
    "        df['Promo Price/Unit'] = df['Promo Price/Unit'].str.replace('£','').astype(float)\n",
    "        if normalized:\n",
    "            df['Value Uplift (v. base) Normalized'] = df['Value Uplift (v. base) Normalized'].str.replace('%','').astype(float) /100\n",
    "        else:\n",
    "            df['Value Uplift (v. base)'] = df['Value Uplift (v. base)'].str.replace().str.replace('%','').astype(float) /100\n",
    "        df = df[df['End of Week'] != '0']\n",
    "        df['End of Week'] = pd.to_datetime(df['End of Week'])\n",
    "        df = df[(df['End of Week'] >= start_date) & (df['End of Week'] <= end_date)].reset_index(drop=True)\n",
    "        if df.shape[0]>0 and not df['Discount Depth (%)'].isna().all():\n",
    "            df = df.fillna(0).reset_index(drop = True)\n",
    "            new_key = key+'_'+ item\n",
    "            valueUplift_dict[new_key] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = 'before'\n",
    "decimals = 2\n",
    "currency = '£'\n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending July 2024\"\n",
    "\n",
    "\n",
    "index = [20]\n",
    "duplication = [len(valueUplift_dict.keys())]\n",
    "section_names = [\"Value Uplift by product\"]\n",
    "path = os.getcwd() + '//Promotion base Oct 2024.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated value.pptx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation(new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each key-slide_num pair in modified_valueUplift\n",
    "for key, slide_num in zip(valueUplift_dict, range(len(valueUplift_dict.keys()))):\n",
    "        # Access the slide to be modified\n",
    "        slide = prs.slides[slide_num]\n",
    "        \n",
    "        # Extract data for the current key\n",
    "        df = valueUplift_dict[key]\n",
    "        #df = df[df['Value Uplift (v. base) Normalized'] !=0 ]\n",
    "        # Get shapes in the slide\n",
    "        shapes = slide.shapes\n",
    "        \n",
    "        # Find and update title shape\n",
    "        titleNumber = get_shape_number(shapes, \"Value Uplift vs discount depth | By Event | Category/Sector | Brand | Coop Alleanza | P12M\")\n",
    "        datasourcenum = get_shape_number(shapes, \"Data Source | Trade Panel\")\n",
    "        headerNumber = get_shape_number(shapes, 'Value Uplift vs discount depth (Replace With SO WHAT)')\n",
    "        if titleNumber is not None:\n",
    "            shapes[datasourcenum].text = data_source\n",
    "            shapes[titleNumber].text = shapes[titleNumber].text.replace('Category/Sector', key.split('_')[2]) \\\n",
    "                .replace('Brand | Coop Alleanza ', df['Item'][0])\n",
    "            shapes[titleNumber].text_frame.paragraphs[0].font.size = Pt(12)\n",
    "            shapes[titleNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "            shapes[headerNumber].text_frame.paragraphs[0].font.size = Pt(16)\n",
    "            shapes[headerNumber].text_frame.paragraphs[0].font.name = 'Nexa Bold (Headings)'\n",
    "\n",
    "        # Create table and chart objects\n",
    "        tables, charts = createTableAndChart(slide.shapes)\n",
    "        chart1 = charts[0].chart  # First chart\n",
    "        chart2 = charts[1].chart  # Second chart\n",
    "        \n",
    "        # Extract data for charts\n",
    "        category = df['Item'].tolist()\n",
    "        x_values_discount = df['Discount Depth (%)'].tolist()\n",
    "        x_values_price = df['Promo Price/Unit'].tolist()\n",
    "        if normalized:\n",
    "            y_values = df['Value Uplift (v. base) Normalized'].tolist()\n",
    "        else:\n",
    "            y_values = df['Value Uplift (v. base)'].tolist()\n",
    "\n",
    "        \n",
    "        x_values_discount = [mround_numpy(value, 0.05) for value in x_values_discount]\n",
    "        x_values_price = [mround_numpy(value, 0.5) for value in x_values_price]\n",
    "        #Update first chart with Discount Depth vs Value Uplift data\n",
    "        chart_data1 = XyChartData()\n",
    "        series1 = chart_data1.add_series('Scatter')\n",
    "        for i in range(len(category)):\n",
    "            series1.add_data_point(x_values_discount[i], y_values[i])\n",
    "        chart1.replace_data(chart_data1)\n",
    "        \n",
    "        # Access the X-axis\n",
    "        \n",
    "        xlsx_file = BytesIO()\n",
    "        with chart_data1._workbook_writer._open_worksheet(xlsx_file) as (workbook, worksheet):\n",
    "            chart_data1._workbook_writer._populate_worksheet(workbook, worksheet)\n",
    "            worksheet.write(0, 4, \"Item\")\n",
    "            worksheet.write_column(1, 4, df['Item'].to_list(), None)\n",
    "            worksheet.write(0, 5, \"End of Week\")\n",
    "            worksheet.write_column(1, 5, df['End of Week'].to_list(), None)\n",
    "\n",
    "        chart1._workbook.update_from_xlsx_blob(xlsx_file.getvalue())\n",
    "\n",
    "        # Update second chart with Promo Price/Unit vs Value Uplift data\n",
    "        chart_data2 = XyChartData()\n",
    "        series2 = chart_data2.add_series('Scatter')\n",
    "        for i in range(len(category)):\n",
    "            series2.add_data_point(x_values_price[i], y_values[i])\n",
    "        chart2.replace_data(chart_data2)\n",
    "        \n",
    "        x_axis = chart2.category_axis\n",
    "        \n",
    "        # Loop through each X-axis category label and format as currency\n",
    "        if sign.lower() == 'before':\n",
    "            x_axis.tick_labels.number_format = f'\"{currency}\"#,##0.00'  if decimals == 2 else f'\"{currency}\"#,##0'\n",
    "        else:\n",
    "            x_axis.tick_labels.number_format = f'#,##0.00\"{currency}\"'  if decimals == 2 else f'#,##0\"{currency}\"'\n",
    "       \n",
    "        #x_axis.has_major_gridlines = False  # Optional: remove gridlines\n",
    "\n",
    "        xlsx_file = BytesIO()\n",
    "        with chart_data2._workbook_writer._open_worksheet(xlsx_file) as (workbook, worksheet):\n",
    "            chart_data2._workbook_writer._populate_worksheet(workbook, worksheet)\n",
    "            worksheet.write(0, 4, \"Item\")\n",
    "            worksheet.write_column(1, 4, df['Item'].to_list(), None)\n",
    "            worksheet.write(0, 5, \"End of Week\")\n",
    "            worksheet.write_column(1, 5, df['End of Week'].to_list(), None)\n",
    "        chart2._workbook.update_from_xlsx_blob(xlsx_file.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath=os.getcwd() + \"\\\\Promotion EdgeWell ValueUplift.pptx\"\n",
    "prs.save(outputPath)\n",
    "app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "presentation = app.Presentations.Open(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623751f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
