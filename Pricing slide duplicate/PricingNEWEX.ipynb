{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 13,
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
   "id": "d8130bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import adodbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 14,
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
   "id": "0d77d233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Apr-24\", \"May-24\", \"Jun-24\", \"Jul-24\", \"Aug-24\", \"Sep-24\", \"Oct-24\", \"Nov-24\", \"Dec-24\", \"Jan-25\", \"Feb-25\", \"Mar-25\"}\n",
      "Provider=MSOLAP.8;Data Source=powerbi://api.powerbi.com/v1.0/myorg/Edgewell;Initial Catalog=Edgewell US Male Dataset;Timeout=900;\n"
     ]
    }
   ],
   "source": [
    "client_manuf = [\"Edgewell Personal Care\"]\n",
    "client_brands = [\"Schick\",\"Equate\",\"Cremo\"]\n",
    "decimals = 2\n",
    "sign = \"Before\"\n",
    "currency = '$'\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    "\n",
    "prodORitem=\"SKU\"\n",
    "categories =[\"Manual Shave Men\"]\n",
    "sectors = [\"System\",\"Disposables\"]\n",
    "segments = [\"Razors\",\"Refills\",\"Disposables\"]\n",
    "subsegments= []\n",
    "subcategories= []\n",
    "\n",
    "customareas=''\n",
    "national = True\n",
    "areas = ['NATIONAL','RETAILER']\n",
    "\n",
    "regions_RET  =[\"Bj's And Sam's\",\"Walmart\"]\n",
    "channels_RET = []\n",
    "market_RET = []\n",
    " \n",
    "regions_CHAN = []\n",
    "channels_CHAN = []\n",
    "market_CHAN = []\n",
    " \n",
    "regions_CUST = []\n",
    "channels_CUST = []\n",
    "market_CUST = []\n",
    " \n",
    "data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending March  2025\"\n",
    "end_date = \"2025-04-01\"\n",
    "years = {2023,2024,2025}\n",
    "\n",
    "ManufOrTopC =\"Top Companies\"\n",
    "BrandOrTopB = \"Brand\"\n",
    "\n",
    "past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "past_3_months = pd.date_range(end=end_date, periods=3, freq='ME').strftime('%b-%y').tolist()\n",
    "past_36_months = pd.date_range(end=end_date, periods=36, freq='ME').strftime('%b-%y').tolist()\n",
    "\n",
    "National=[\"NATIONAL\"]if national else []\n",
    "regions = regions_RET + regions_CHAN + regions_CUST\n",
    "channels = channels_RET + channels_CHAN + channels_CUST\n",
    "markets = market_RET + market_CHAN + market_CUST\n",
    "brands_only = True  # Get the Data of SKU Share by brands level only\n",
    "\n",
    "entity_hierarchy = [\n",
    "    (\"Area\",National),\n",
    "    (\"Region\", regions),\n",
    "    (\"Channel\", channels),\n",
    "    (\"Market\", markets)\n",
    "]\n",
    "hierarchy_levels = [\n",
    "    (\"Category\", categories),\n",
    "    (\"Sector\", sectors),\n",
    "    (\"Segment\", segments),\n",
    "    (\"SubSegment\", subsegments),\n",
    "    (\"SubCategory\", subcategories)\n",
    " \n",
    "]\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}\n",
    "server = \"powerbi://api.powerbi.com/v1.0/myorg/Edgewell\"\n",
    "dataset_name = \"Edgewell US Male Dataset\"\n",
    "p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "p3m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_3_months) + \"}\"\n",
    "p36m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_36_months) + \"}\"\n",
    "\n",
    "print(p12m_dax)\n",
    "path=os.path.join(os.getcwd(),\"Pricing Datasets NewEX\")\n",
    "\n",
    "conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=900;\"\n",
    "print(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9bcad",
   "metadata": {},
   "source": [
    "## Price Positioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63c9c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_positioning_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_positioning_manuf.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorManuf,entity_name,entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    "    key =  f\"{entity_type} | {entity_name}\"\n",
    "\n",
    "\n",
    "    columns = [\n",
    "        \"Relative Price\",'Av Price/Unit','Value Sales','Value Share','Value Share DYA','Av Price/KG','IYA Price/KG'\n",
    "    ]\n",
    "    \n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{BrandorManuf}]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Grand total query (not grouped by BrandOrTopB, just Category)\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)         \n",
    "            df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]  # Remove zero rows\n",
    "\n",
    "            grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "            grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]  # Remove zero rows\n",
    "\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "\n",
    "            # Reorder columns if necessary\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "\n",
    "            # Concatenate the two\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "            outputdic[key] = df  \n",
    "\n",
    "            \n",
    "            print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(BrandorManuf,entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys=[]\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            # print(hierby,value,entity)                                    \n",
    "                            key = f\"{value} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(execute_dax_query,BrandorManuf, entity,value,area, hierby)\n",
    "                            futures[future] = key\n",
    "       \n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        # Insert results in original order\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        if BrandorManuf==f'{BrandOrTopB}':\n",
    "            filename =  f\"price_positioning_brands.pkl\"\n",
    "        else:\n",
    "            filename =  f\"price_positioning_manuf.pkl\"\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy,hierarchy_levels) \n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy,hierarchy_levels) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fc172",
   "metadata": {},
   "source": [
    "## Sector/Segment/SubSegment Leaderships (SINGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e87c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Category saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\Category_leadership.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\Sector_leadership.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\Segment_leadership.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\SubSegment_leadership.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\SubCategory_leadership.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Category saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\Category_total_leadership.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\Sector_total_leadership.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\Segment_total_leadership.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\SubSegment_total_leadership.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\SubCategory_total_leadership.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby, client, total=True):\n",
    "    outputdic = {}\n",
    "    key = f\"{client} | {entity_name}\" if client else f\"{entity_name}\"\n",
    "\n",
    "    columns = [\n",
    "        \"Value Share\", \"Value Sales\", \"Av Price/KG\", \"WoB %\", \"Gross Margin %\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if not total and client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        grand_tot[df.columns[0]] = 'Grand Total'\n",
    "        grand_tot = grand_tot[df.columns]\n",
    "        df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, total=False):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        \n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                if client_brands: \n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            for client in client_brands:\n",
    "                                key = f\"{client} | {entity}\"\n",
    "                                ordered_keys.append(key)\n",
    "                                future = executor.submit(\n",
    "                                    execute_dax_query, entity, area, hierby, client, total\n",
    "                                )\n",
    "                                futures[future] = key\n",
    "                else:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query, entity, area, hierby, '', total\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            if client_brands:\n",
    "                filename = f\"{hierby}_leadership.pkl\"\n",
    "            else:\n",
    "                filename = f\"{hierby}_total_leadership.pkl\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "# Execute\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands)\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels, total=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263f3c8",
   "metadata": {},
   "source": [
    "## Sector/Segment Leaderships"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
   "id": "362fe3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\brand_Segment.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\brand_SubSegment.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\brand_SubCategory.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\manuf_Segment.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\manuf_SubSegment.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\manuf_SubCategory.pkl.\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\total_Segment.pkl.\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\total_SubSegment.pkl.\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\total_SubCategory.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby,direct_parent, client,manuf, total=False):\n",
    "    outputdic = {}\n",
    "    key = f\"{client} | {entity_name}\" if client else (f\"{manuf} | {entity_name}\" if manuf else f\"{entity_name}\")\n",
    "\n",
    "    columns = [\n",
    "        \"Value Share\", \"Value Sales\", \"Av Price/KG\", \"WoB %\", \"Gross Margin %\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if not total and client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''    \n",
    "    manuf_filter=''\n",
    "    if not total and manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{hierby}]\n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent, client_brands=None,client_manuf=None, total=False):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\" or direct_parent[hierby]==\"Category\":\n",
    "                    continue\n",
    "            if isinstance(hier_values, list):\n",
    "                if client_brands and not total: \n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            for client in client_brands:\n",
    "                                key = f\"{client} | {entity}\"\n",
    "                                ordered_keys.append(key)\n",
    "                                future = executor.submit(\n",
    "                                    execute_dax_query, entity, area, hierby,direct_parent, client,'', ''\n",
    "                                )\n",
    "                                futures[future] = key\n",
    "                if client_manuf and not total:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            for manuf in client_manuf:\n",
    "                                key = f\"{manuf} | {entity}\"\n",
    "                                ordered_keys.append(key)\n",
    "                                future = executor.submit(\n",
    "                                    execute_dax_query, entity, area, hierby,direct_parent, '',manuf,''\n",
    "                                )\n",
    "                                futures[future] = key               \n",
    "                elif total==True:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{entity}\"\n",
    "                            print(\"SALMA YA SALAMA\")\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query, entity, area, hierby, direct_parent,'','', total\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            if client_brands:\n",
    "                filename = f\"brand_{hierby}.pkl\"\n",
    "            elif client_manuf:\n",
    "                filename = f\"manuf_{hierby}.pkl\"\n",
    "            else:\n",
    "                filename = f\"total_{hierby}.pkl\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent, client_brands=client_brands)\n",
    "if client_manuf:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent, client_manuf=client_manuf)   \n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent, total=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0800646",
   "metadata": {},
   "source": [
    "## Shelf & Avg Price / KG By Brand"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 15,
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
   "id": "3eb2ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Sector_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
<<<<<<< HEAD
=======
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Segment_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubSegment_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
<<<<<<< HEAD
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Segment_top_brands.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubSegment_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubCategory_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Sector_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Segment_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubSegment_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubCategory_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Sector_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Segment_top_manuf.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubSegment_top_manuf.pkl.\n",
=======
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubCategory_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Sector_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
<<<<<<< HEAD
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubCategory_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Sector_all_manuf.pkl.\n",
=======
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Segment_all_brands.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubSegment_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubCategory_all_brands.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Sector_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Segment_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubSegment_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
<<<<<<< HEAD
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
=======
      "Query executed successfully for NATIONAL.\n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubCategory_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Sector_all_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_Segment_all_manuf.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubSegment_all_manuf.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
<<<<<<< HEAD
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubCategory_all_manuf.pkl.\n"
=======
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubSegment_all_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\shelf_SubCategory_all_manuf.pkl.\n"
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
     ]
    }
   ],
   "source": [
    "def execute_dax_query(manuforbrand,entity_name,entity_type, area, hierby, incall=True):\n",
    "    outputdic = {}\n",
<<<<<<< HEAD
    "    key =  f\"{entity_type} | {entity_name}\" \n",
=======
    "    key =  f\"{entity_type} | {entity_name}\"\n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
    "    if manuforbrand==f'{BrandOrTopB}':\n",
    "        columns = [\n",
    "            \"Relative Price\", \"Av Price/Unit\", \"Value Sales\", \"IYA Price/KG\", \"Base Price/KG\",\"Av Price/KG\",\"Value Share\",\"WoB %\",\"Value Share DYA\"\n",
    "        ]\n",
    "    else:\n",
    "        columns = [\"Base Price/KG\",\"Av Price/KG\",\"Value Share\",\"WoB %\",\"Value Share DYA\"]\n",
    " \n",
    " \n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    " \n",
    "    # Prepare client brand filter conditionally\n",
    "    scope_filter = \"\"\n",
    "    if incall:\n",
    "        scope_filter = f'''\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        '''\n",
    "    else:\n",
    "        scope_filter = f'''\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\"),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\"\n",
    "        '''\n",
    " \n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}],\n",
    "                    Products[{manuforbrand}]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            {scope_filter}\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]\n",
    " \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            {scope_filter}\n",
    "        )\n",
    "    \"\"\"\n",
    "   \n",
    "    # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            {scope_filter}\n",
    " \n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    " \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    " \n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
<<<<<<< HEAD
    "\n",
=======
    " \n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "       \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    " \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    " \n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    " \n",
    "        if not grand_tot.empty:\n",
    "            # Ensure 'Grand Total' label is added to the first column of grand_tot\n",
    "            grand_tot[df_with_totals.columns[0]] = 'Grand Total'\n",
    " \n",
    "            # Keep only columns that exist in df_with_totals\n",
    "            common_columns = [col for col in df_with_totals.columns if col in grand_tot.columns]\n",
    "            grand_tot = grand_tot[common_columns]\n",
    " \n",
    "            # Align grand_tot with df_with_totals in case column order matters\n",
    "            grand_tot = grand_tot.reindex(columns=df_with_totals.columns)\n",
    " \n",
    "        df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "        outputdic[key] = df\n",
    " \n",
    " \n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    " \n",
    "    return outputdic\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "def process_dax_queries(manuforbrand,entity_hierarchy, hierarchy_levels, incall=False):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        hierarchy_dict = dict(hierarchy_levels)\n",
    "\n",
    "        for hierby, _ in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    "                    \n",
    "            dfs_results = {} \n",
=======
    " \n",
    " \n",
    "def process_dax_queries(manuforbrand,entity_hierarchy, hierarchy_levels, incall=False):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        hierarchy_dict = dict(hierarchy_levels)\n",
    " \n",
    "        for hierby, _ in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    "                   \n",
    "            dfs_results = {}\n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "            # Get the parent level name (e.g., 'Segment')\n",
    "            parent_level = direct_parent[hierby]\n",
<<<<<<< HEAD
    "\n",
=======
    " \n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
    "            # Get the list of values associated with that parent level\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    "            if isinstance(parent_values, list):\n",
    "                for value in parent_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{value} | {entity}\"                          \n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query,manuforbrand, entity, value, area, hierby,incall\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    " \n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "            if manuforbrand==f'{BrandOrTopB}':\n",
    "                if incall==False:\n",
    "                    filename = f\"shelf_{hierby}_top_brands.pkl\"\n",
    "                else:\n",
    "                    filename = f\"shelf_{hierby}_all_brands.pkl\"\n",
    "            else:\n",
    "                if incall==False:\n",
    "                    filename = f\"shelf_{hierby}_top_manuf.pkl\"\n",
    "                else:\n",
    "                    filename = f\"shelf_{hierby}_all_manuf.pkl\"\n",
    " \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "           \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "\n",
=======
    " \n",
    " \n",
    " \n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
    "# Execute\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,incall=False)\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels, incall=True)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,incall=False)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels, incall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5b360",
   "metadata": {},
   "source": [
    "## Price Point Distribution By Product (Item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_Sector_p12m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n",
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_Segment_p12m.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_SubSegment_p12m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_SubCategory_p12m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_Sector_p3m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n",
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_Segment_p3m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n",
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_SubSegment_p3m.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma Hany\\AppData\\Local\\Temp\\ipykernel_8868\\3009806179.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, maintotal_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_SubCategory_p3m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_Sector_manuf_p12m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_Segment_manuf_p12m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_SubSegment_manuf_p12m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_SubCategory_manuf_p12m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_Sector_manuf_p3m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_Segment_manuf_p3m.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_SubSegment_manuf_p3m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_SubCategory_manuf_p3m.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, entity_type, area, hierby, client, manuf,p12m=True):\n",
    "    outputdic = {}\n",
    "\n",
    "    key = f\"{client} | {entity_name} | {entity_type}\" if client else f\"{manuf} | {entity_name} | {entity_type}\" \n",
    "    if p12m :\n",
    "        timeper=p12m_dax\n",
    "    else:\n",
    "        timeper=p3m_dax    \n",
    "    columns = [\"Base Price/Unit\", \"Base Price/KG\", \"Value Sales\", \"Gross Margin %\"]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    client_filter=''\n",
    "    if client:\n",
    "        client_filter = f'''\n",
    "                FILTER(\n",
    "                    Products,\n",
    "                    Products[{BrandOrTopB}] = \"{client}\" &&\n",
    "                    Products[{ManufOrTopC}] = \"{manuf}\"\n",
    "                ),\n",
    "            '''\n",
    "   \n",
    "    manuf_filter = f'FILTER(Products, Products[Top Companies] = \"{manuf}\"),' if manuf else \"\"\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}],\n",
    "                    Products[{prodORitem}],\n",
    "                    Products[Total Size]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Parent total query\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products, Products[{hierby}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{8}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    itemtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products,Products[{prodORitem}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(VALUES(Products[Category]), {column_exprs}),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(dax_query)\n",
    "                df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(parenttotal_dax_query)\n",
    "                maintotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "          \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(itemtotal_dax_query)\n",
    "                itemtotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(grandtotal_query)\n",
    "                grand_tot = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "        for dataframe in [df,maintotal_df,itemtotal_df, grand_tot]:\n",
    "            dataframe.columns = dataframe.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            dataframe.dropna(how='all', inplace=True)\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, maintotal_df], ignore_index=True)\n",
    "      \n",
    "        if not itemtotal_df.empty:\n",
    "            itemtotal_df.iloc[:, 0] = itemtotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, itemtotal_df], ignore_index=True)\n",
    "            \n",
    "        if not grand_tot.empty:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan\n",
    "            grand_tot = grand_tot.reindex(columns=df.columns)\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, client_manuf=None,p12m=True):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    "\n",
    "            hierarchy_dict = dict(hierarchy_levels)\n",
    "            parent_level = direct_parent.get(hierby)\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    "\n",
    "            if not parent_values:\n",
    "                continue\n",
    "\n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "\n",
    "            # Determine prodORitem per level\n",
    "\n",
    "            for value in parent_values:\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        if client_brands:\n",
    "                            for client in client_brands:\n",
    "                                for manuf in client_manuf:\n",
    "                                    key = f\"{client} | {entity} | {value}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    futures[executor.submit(\n",
    "                                        execute_dax_query, entity, value, area, hierby, client, manuf,p12m\n",
    "                                    )] = key\n",
    "                        elif client_manuf:\n",
    "                            for manuf in client_manuf:\n",
    "                                key = f\"{manuf} | {entity} | {value}\"\n",
    "                                ordered_keys.append(key)\n",
    "                                futures[executor.submit(\n",
    "                                    execute_dax_query, entity, value, area, hierby, None, manuf, p12m\n",
    "                                )] = key\n",
    "\n",
    "            dfs_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                dfs_results.update(result)\n",
    "\n",
    "            dfs_results = {key: dfs_results[key] for key in ordered_keys if key in dfs_results}\n",
    "            if p12m:\n",
    "                 filename = f\"price_distribution_{hierby}_{'p12m' if client_brands else 'manuf_p12m'}.pkl\"\n",
    "            else:\n",
    "                 filename = f\"price_distribution_{hierby}_{'p3m' if client_brands else 'manuf_p3m'}.pkl\"\n",
    "                     \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "\n",
    "# Example Invocation\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands,client_manuf=client_manuf,p12m=True)\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands,client_manuf=client_manuf,p12m=False)\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf,p12m=True)\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf,p12m=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c703548",
   "metadata": {},
   "source": [
    "## Price Point Distribution By Product Scraped(Item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1671bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def execute_dax_query(entity_name, area, hierby, client, manuf):\n",
    "#     outputdic={}\n",
    "#     if not categories or len(categories) == 0:\n",
    "#         raise ValueError(\"categories list must have at least one element\")\n",
    "\n",
    "#     columns = [\n",
    "#         \"Scraped Av. Price/Unit\", \"Scraped Av. Price/KG\"\n",
    "#     ]\n",
    "#     column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "#     client_filter = ''\n",
    "#     if client:\n",
    "#         client_filter = f'FILTER(Products, Products[{BrandOrTopB}] = \"{client}\")'\n",
    "\n",
    "#     manuf_filter = ''\n",
    "#     if manuf:\n",
    "#         manuf_filter = f'FILTER(Products, Products[Top Companies] = \"{manuf}\")'\n",
    "\n",
    "#     # Compose CALCULATETABLE filter arguments list (only non-empty filters)\n",
    "#     filter_args = [f'FILTER(Products, Products[Category] = \"{categories[0]}\")']\n",
    "#     if client_filter:\n",
    "#         filter_args.append(client_filter)\n",
    "#     if manuf_filter:\n",
    "#         filter_args.append(manuf_filter)\n",
    "#     filter_args.append(f'TREATAS({{ \"{entity_name}\" }}, Market[{area}])')\n",
    "#     filter_args.append('FILTER(\\'Scope\\', \\'Scope\\'[Scope] = \"Category\")')\n",
    "\n",
    "#     filters_str = \",\\n            \".join(filter_args)\n",
    "\n",
    "#     dax_query = f\"\"\"\n",
    "#         EVALUATE\n",
    "#         VAR BaseTable =\n",
    "#             CROSSJOIN(\n",
    "#                 VALUES(Calendar[End of Week]),\n",
    "#                 VALUES(Products[{prodORitem}]),\n",
    "#                 VALUES(Products[Total Size])\n",
    "#             )\n",
    "#         VAR FilteredTable =\n",
    "#             CALCULATETABLE(\n",
    "#                 BaseTable,\n",
    "#                 {filters_str}\n",
    "#             )\n",
    "#         RETURN\n",
    "#             ADDCOLUMNS(\n",
    "#                 FilteredTable,\n",
    "#                 {column_exprs}\n",
    "#             )\n",
    "#         \"\"\"\n",
    "\n",
    "#     parenttotal_dax_query = f\"\"\"\n",
    "#             EVALUATE\n",
    "#             CALCULATETABLE(\n",
    "#                 ADDCOLUMNS(\n",
    "#                     SUMMARIZECOLUMNS(\n",
    "#                         Products[{hierby}]\n",
    "\n",
    "#                     ),\n",
    "#                 {column_exprs}\n",
    "#             ),\n",
    "#             Products[Category] = \"{categories[0]}\",\n",
    "#             {client_filter}\n",
    "#             {manuf_filter}\n",
    "#             TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "#             FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "#         )\n",
    "#     \"\"\"\n",
    "#         # Grand total query\n",
    "#     grandtotal_query = f\"\"\"\n",
    "#         EVALUATE\n",
    "#         CALCULATETABLE(\n",
    "#             ADDCOLUMNS(\n",
    "#                 VALUES(Products[Category]),\n",
    "#                 {column_exprs}\n",
    "#             ),\n",
    "#             Products[Category] = \"{categories[0]}\",\n",
    "#             {client_filter}\n",
    "#             {manuf_filter}\n",
    "#             TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "#         )\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         with adodbapi.connect(conn_str) as conn:\n",
    "#             conn.timeout = 500  # Increase timeout to 300 seconds\n",
    "#             with conn.cursor() as cursor:\n",
    "#                 cursor.execute(dax_query)\n",
    "#                 columns_result = [desc[0] for desc in cursor.description]\n",
    "#                 data = cursor.fetchall()\n",
    "                \n",
    "#         with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "#             cursor.execute(parenttotal_dax_query)\n",
    "#             maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "#             maintotal_data = cursor.fetchall()\n",
    "            \n",
    "#         with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "#             cursor.execute(grandtotal_query)\n",
    "#             grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "#             grandtotal_data = cursor.fetchall()\n",
    "\n",
    "#         df = pd.DataFrame(data, columns=columns_result)\n",
    "#         df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "#         df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "#         maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "#         maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "#         maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "#         grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "#         grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "#         grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "#         if maintotal_df.shape[1] > 1:\n",
    "#             maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "#         if maintotal_df.empty:\n",
    "#             outputdic[key] = maintotal_df\n",
    "#             return outputdic\n",
    "#         if not maintotal_df.empty:\n",
    "#             df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "#         if not grand_tot.empty:\n",
    "#             # Create a dict for the first two columns\n",
    "#             grand_tot[df.columns[0]] = 'Grand Total'\n",
    "#             grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "#             # Ensure all required columns exist, fill missing ones with NaN\n",
    "#             for col in df.columns:\n",
    "#                 if col not in grand_tot.columns:\n",
    "#                     grand_tot[col] = np.nan\n",
    "\n",
    "#             # Reorder columns exactly as in df\n",
    "#             grand_tot = grand_tot[df.columns]\n",
    "#             df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "#         outputdic[key] = df\n",
    "\n",
    "#         print(f\"Query executed successfully for {entity_name}.\")\n",
    "#     except adodbapi.DatabaseError as db_error:\n",
    "#         print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "#     return outputdic\n",
    "\n",
    "\n",
    "# def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None,client_manuf=None):\n",
    "#     with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#         dfs_results = {} \n",
    "#         futures = {}\n",
    "#         ordered_keys = []\n",
    "#         for hierby, hier_values in hierarchy_levels:\n",
    "#             if hierby == \"Category\":\n",
    "#                     continue\n",
    "#             if isinstance(hier_values, list):\n",
    "#                 if client_brands: \n",
    "#                     for area, entity_list in entity_hierarchy:\n",
    "#                         for entity in entity_list:\n",
    "#                             for manuf in client_manuf:\n",
    "#                                 for client in client_brands: \n",
    "#                                     key = f\"{client} | {entity}\"\n",
    "#                                     ordered_keys.append(key)\n",
    "#                                     future = executor.submit(\n",
    "#                                         execute_dax_query, entity, area, hierby, client,manuf\n",
    "#                                     )\n",
    "#                                     futures[future] = key\n",
    "#                 else:\n",
    "#                     for area, entity_list in entity_hierarchy:\n",
    "#                         for entity in entity_list:\n",
    "#                             for manuf in client_manuf:\n",
    "#                                 key = f\"{manuf} | {entity}\"\n",
    "#                                 ordered_keys.append(key)\n",
    "#                                 future = executor.submit(\n",
    "#                                     execute_dax_query, entity, area, hierby, '',manuf\n",
    "#                                 )\n",
    "#                                 futures[future] = key               \n",
    "\n",
    "\n",
    "#             temp_results = {}\n",
    "#             for future in as_completed(futures):\n",
    "#                 result = future.result()\n",
    "#                 temp_results.update(result)\n",
    "\n",
    "#             for key in ordered_keys:\n",
    "#                 if key in temp_results:\n",
    "#                     dfs_results[key] = temp_results[key]\n",
    "\n",
    "#             if client_brands:\n",
    "#                 filename = f\"price_distribution_scraped_{hierby}.pkl\"\n",
    "#             else:\n",
    "#                 filename = f\"price_distribution_scraped_{hierby}_manuf.pkl\"\n",
    "\n",
    "\n",
    "#             output_file = f\"{path}\\\\{filename}\"\n",
    "#             with open(output_file, \"wb\") as f:\n",
    "#                 pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "#             print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "# if client_brands:\n",
    "#     process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands,client_manuf=client_manuf)\n",
    "# if client_manuf:\n",
    "#     process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45219ecd",
   "metadata": {},
   "source": [
    "## Price Point Distribution By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf5d184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_by_brands_category.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    "\n",
    "    key = f\"{entity_name} | {entity_type}\" \n",
    "\n",
    "\n",
    "    columns = \"Av Price/Unit\", \"Value Share\"\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{BrandOrTopB}],\n",
    "                    Products[Pack Size]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Parent total query\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products, Products[{BrandOrTopB}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "        \n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(VALUES(Products[Category]), {column_exprs}),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            \n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # 1. Main query\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            data_columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        # 2. Parent total query\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "\n",
    "        # 3. Grand total query\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        # Now build DataFrames safely\n",
    "        df = pd.DataFrame(data, columns=data_columns)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "  \n",
    "        if not grand_tot.empty:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan\n",
    "            grand_tot = grand_tot.reindex(columns=df.columns)\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys=[]\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            # print(hierby,value,entity)                                    \n",
    "                            key = f\"{entity} | {value}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(execute_dax_query, entity,value,area, hierby)\n",
    "                            futures[future] = key\n",
    "       \n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        # Insert results in original order\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        filename = f\"price_distribution_by_brands_category.pkl\"\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy,hierarchy_levels) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617cd93",
   "metadata": {},
   "source": [
    "## By brand For Sec,Seg,SubSeg... (Slide 7)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 12,
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
   "id": "10d078e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_by_brands_Sector.\n",
      "Query executed successfully for Walmart.\n",
=======
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_by_brands_Sector.\n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
<<<<<<< HEAD
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_by_brands_Segment.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
=======
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_by_brands_Segment.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_by_brands_SubSegment.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
<<<<<<< HEAD
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_by_brands_SubCategory.\n"
=======
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_distribution_by_brands_SubCategory.\n"
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    " \n",
    "    key = f\"{entity_name} | {entity_type}\"\n",
    "    columns = \"Av Price/Unit\", \"Value Share\"\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "             \n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{BrandOrTopB}],\n",
    "                    Products[{hierby}],\n",
    "                    Products[Pack Size]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "    # Parent total query\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products, Products[{BrandOrTopB}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    itemtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products,Products[{BrandOrTopB}],\n",
    "                Products[{hierby}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(VALUES(Products[Category]), {column_exprs}),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    " \n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(dax_query)\n",
    "                df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    " \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(parenttotal_dax_query)\n",
    "                maintotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "         \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(itemtotal_dax_query)\n",
    "                itemtotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    " \n",
    " \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(grandtotal_query)\n",
    "                grand_tot = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    " \n",
    "        for dataframe in [df,maintotal_df,itemtotal_df, grand_tot]:\n",
    "            dataframe.columns = dataframe.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            dataframe.dropna(how='all', inplace=True)\n",
<<<<<<< HEAD
    "        \n",
=======
    "       \n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
    "            cols_to_check = [\"Av Price/Unit\", \"Value Share\"]\n",
    "            dataframe.drop(\n",
    "                index=dataframe[\n",
    "                    (dataframe[cols_to_check].abs() < 1e-6).all(axis=1)\n",
    "                ].index,\n",
    "                inplace=True\n",
    "            )    \n",
<<<<<<< HEAD
    "            \n",
    "\n",
=======
    "           \n",
    " \n",
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
    "        if not maintotal_df.empty:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, maintotal_df], ignore_index=True)\n",
    "     \n",
    "        if not itemtotal_df.empty:\n",
    "            itemtotal_df.iloc[:, 1] = itemtotal_df.iloc[:, 1].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, itemtotal_df], ignore_index=True)\n",
    "           \n",
    "        if not grand_tot.empty:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan\n",
    "            grand_tot = grand_tot.reindex(columns=df.columns)\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    " \n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    " \n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    " \n",
    "    return outputdic\n",
    " \n",
    " \n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    " \n",
    "            hierarchy_dict = dict(hierarchy_levels)\n",
    "            parent_level = direct_parent.get(hierby)\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    " \n",
    "            if not parent_values:\n",
    "                continue\n",
    " \n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    " \n",
    "            # Determine prodORitem per level\n",
    " \n",
    "            for value in parent_values:\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        for manuf in client_manuf:\n",
    "                            key = f\"{entity} | {value}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            futures[executor.submit(\n",
    "                                execute_dax_query, entity, value, area, hierby\n",
    "                            )] = key\n",
    " \n",
    "            dfs_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                dfs_results.update(result)\n",
    " \n",
    "            dfs_results = {key: dfs_results[key] for key in ordered_keys if key in dfs_results}\n",
    "           \n",
    "            filename = f\"price_distribution_by_brands_{hierby}\"\n",
    "                     \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    " \n",
    " \n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels)\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43544eb7",
   "metadata": {},
   "source": [
    "## Price Point Comparison by Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d08f455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_point_by_brands_items_P12M.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_point_by_brands_items_P3M.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_point_by_manuf_items_P12M.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_point_by_manuf_items_P3M.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(ManuforBrand,entity_name, entity_type, area, hierby, p12m=True):\n",
    "    outputdic = {}\n",
    "\n",
    "    key = f\"{entity_name} | {entity_type}\"\n",
    "\n",
    "    if p12m:\n",
    "        timeper = p12m_dax\n",
    "        columns = [\"Value Share\", \"Gross Margin %\"]\n",
    "        row_fields = [\n",
    "            f\"Products[{ManuforBrand}]\",\n",
    "            f\"Products[{prodORitem}]\"\n",
    "        ]\n",
    "    else:\n",
    "        timeper = p3m_dax\n",
    "        columns = [\"Base Price/Unit\", \"Base Price/KG\", \"Value Sales\", \"Value Share\"]\n",
    "        row_fields = [\n",
    "            f\"Products[{ManuforBrand}]\",\n",
    "            f\"Products[{prodORitem}]\",\n",
    "            \"Products[Total Size]\"\n",
    "        ]\n",
    "\n",
    "    summarize_fields = \",\\n                        \".join(row_fields)\n",
    "    column_exprs = \",\\n                \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    {summarize_fields}\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Parent total query\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products, Products[{ManuforBrand}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    itemtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products,Products[{prodORitem}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE( \n",
    "            ADDCOLUMNS(VALUES(Products[Category]), {column_exprs}),\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "            \n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        with adodbapi.connect(conn_str) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(dax_query)\n",
    "                df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(parenttotal_dax_query)\n",
    "                maintotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "          \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(itemtotal_dax_query)\n",
    "                itemtotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(grandtotal_query)\n",
    "                grand_tot = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "          \n",
    "            numeric_columns = columns\n",
    "            for i, dataframe in enumerate([df, maintotal_df, itemtotal_df, grand_tot]):\n",
    "                dataframe.columns = dataframe.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "                dataframe.dropna(how='all', inplace=True)\n",
    "\n",
    "                if not dataframe.empty:\n",
    "                    # Force conversion to numeric in case some values are strings\n",
    "                    dataframe[numeric_columns] = dataframe[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "                    # Now drop rows where all numeric values are 0.0\n",
    "                    mask_all_zero = (dataframe[numeric_columns].fillna(0.0) == 0.0).all(axis=1)\n",
    "                    dataframe = dataframe[~mask_all_zero]\n",
    "\n",
    "                if i == 0:\n",
    "                    df = dataframe\n",
    "                elif i == 1:\n",
    "                    maintotal_df = dataframe\n",
    "                elif i == 2:\n",
    "                    itemtotal_df = dataframe\n",
    "                else:\n",
    "                    grand_tot = dataframe\n",
    "\n",
    "\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, maintotal_df], ignore_index=True)\n",
    "            \n",
    "        if not p12m:\n",
    "            if not itemtotal_df.empty:\n",
    "                itemtotal_df.iloc[:, 0] = itemtotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "                df = pd.concat([df, itemtotal_df], ignore_index=True)\n",
    "            \n",
    "        if not grand_tot.empty:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan\n",
    "            grand_tot = grand_tot.reindex(columns=df.columns)\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(ManuforBrand,entity_hierarchy, hierarchy_levels,p12m):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys=[]\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            # print(hierby,value,entity)                                    \n",
    "                            key = f\"{entity} | {value}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(execute_dax_query,ManuforBrand, entity,value,area, hierby,p12m)\n",
    "                            futures[future] = key\n",
    "\n",
    "        dfs_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            dfs_results.update(result)\n",
    "\n",
    "        dfs_results = {key: dfs_results[key] for key in ordered_keys if key in dfs_results}\n",
    "        if ManuforBrand ==f'{BrandOrTopB}' :\n",
    "            if p12m:\n",
    "                    filename = f\"price_point_by_brands_items_P12M.pkl\"\n",
    "            else:\n",
    "                    filename = f\"price_point_by_brands_items_P3M.pkl\"\n",
    "        else:\n",
    "            if p12m:\n",
    "                    filename = f\"price_point_by_manuf_items_P12M.pkl\"\n",
    "            else:\n",
    "                    filename = f\"price_point_by_manuf_items_P3M.pkl\"           \n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "\n",
    "# Example Invocation\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,p12m=True)\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,p12m=False)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,p12m=True)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,p12m=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50029e3",
   "metadata": {},
   "source": [
    "## Price Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88086e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames saved to c:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\price_correlation_P3Y.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    "    key =  f\"{entity_type} | {entity_name}\"\n",
    "\n",
    "    columns = [\n",
    "        \"Volume Share\",'Av Price/KG','Value Share'\n",
    "    ]\n",
    "    \n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                CROSSJOIN(\n",
    "                    DISTINCT(Products[{BrandOrTopB}]),\n",
    "                    DISTINCT(Calendar[QuarterStart]),\n",
    "                    DISTINCT(Calendar[End of Week])\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p36m_dax}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    fristcoltot_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    products,\n",
    "                    Products[{BrandOrTopB}]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p36m_dax}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    secondcoltot_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Calendar,\n",
    "                    Calendar[QuarterStart]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p36m_dax}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Grand total query (not grouped by BrandOrTopB, just Category)\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[Category]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p36m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(fristcoltot_query)\n",
    "            fristttotal_columns = [desc[0] for desc in cursor.description]\n",
    "            fristtotal_data = cursor.fetchall()       \n",
    "             \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(secondcoltot_query)\n",
    "            sectotal_columns = [desc[0] for desc in cursor.description]\n",
    "            sectotal_data = cursor.fetchall()  \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)         \n",
    "            df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]  # Remove zero rows\n",
    "\n",
    "            maintotal_df = pd.DataFrame(fristtotal_data, columns=fristttotal_columns)\n",
    "            maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "            sectotal_df = pd.DataFrame(sectotal_data, columns=sectotal_columns)\n",
    "            sectotal_df.columns = sectotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            sectotal_df = sectotal_df.loc[~(sectotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "            grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "            grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "            if maintotal_df.shape[1] > 1:\n",
    "                maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            \n",
    "            if sectotal_df.shape[1] > 1:\n",
    "                first_col = sectotal_df.columns[0]\n",
    "                # Ensure datetime\n",
    "                sectotal_df[first_col] = pd.to_datetime(sectotal_df[first_col], errors='coerce')\n",
    "\n",
    "                # Format date and append ' total'\n",
    "                sectotal_df[first_col] = sectotal_df[first_col].dt.strftime('%Y-%m-%d') + \" Total\"\n",
    "\n",
    "\n",
    "\n",
    "            if sectotal_df.empty:\n",
    "                outputdic[key] = sectotal_df\n",
    "                return outputdic\n",
    "\n",
    "            if maintotal_df.empty:\n",
    "                outputdic[key] = maintotal_df\n",
    "                return outputdic\n",
    "        \n",
    "            \n",
    "            if not maintotal_df.empty:\n",
    "                df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "                \n",
    "            df_with_totals['QuarterStart'] = pd.to_datetime(df_with_totals['QuarterStart']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "            if not sectotal_df.empty:\n",
    "                dfsec_with_totals = pd.concat([df_with_totals,sectotal_df], ignore_index=True)\n",
    "\n",
    "            \n",
    "\n",
    "            grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "            grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]  # Remove zero rows\n",
    "          \n",
    "            if not grand_tot.empty:\n",
    "                # Set first column value to 'Grand Total', second column to NaN\n",
    "                grand_tot[df.columns[0]] = 'Grand Total'\n",
    "                grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "                # Restrict grand_tot to only columns that appear in df\n",
    "                grand_tot = grand_tot[df.columns.intersection(grand_tot.columns)]\n",
    "\n",
    "            # Concatenate all together\n",
    "            df = pd.concat([dfsec_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df\n",
    "\n",
    "\n",
    "            \n",
    "            print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys=[]\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            # print(hierby,value,entity)                                    \n",
    "                            key = f\"{value} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(execute_dax_query, entity,value,area, hierby)\n",
    "                            futures[future] = key\n",
    "       \n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        # Insert results in original order\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        filename =  f\"price_correlation_P3Y.pkl\"\n",
    "\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy,hierarchy_levels) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1314c",
   "metadata": {},
   "source": [
    "# Read "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 14,
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
   "id": "4f13d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
<<<<<<< HEAD
    "loaded_data = {}\n",
    "datasets_path =r\"c:\\Users\\aleaa\\Documents\\Slide-Automate\\Pricing slide duplicate\\Pricing Datasets NewEX\\\\\"\n",
    "datasets = os.listdir(datasets_path)    \n",
    "for d in datasets:\n",
    "    with open(datasets_path+d, 'rb') as handle:\n",
    "        globals()[d.split('.')[0]] = pd.read_pickle(handle)"
=======
    "# loaded_data = {}\n",
    "# datasets_path =r\"C:\\Users\\aleaa\\Desktop\\Slide-Automate Extraction\\Pricing\\Pricing Datasets NewEX\\\\\"\n",
    "# datasets = os.listdir(datasets_path)    \n",
    "# for d in datasets:\n",
    "#     with open(datasets_path+d, 'rb') as handle:\n",
    "#         globals()[d.split('.')[0]] = pd.read_pickle(handle)"
>>>>>>> 3a667dfd500cb929365e48aaca602f30fc73244a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573df431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Relative Price</th>\n",
       "      <th>Av Price/Unit</th>\n",
       "      <th>Value Sales</th>\n",
       "      <th>IYA Price/KG</th>\n",
       "      <th>Base Price/KG</th>\n",
       "      <th>Av Price/KG</th>\n",
       "      <th>Value Share</th>\n",
       "      <th>WoB %</th>\n",
       "      <th>Value Share DYA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disposables</td>\n",
       "      <td>Gillette</td>\n",
       "      <td>0.421</td>\n",
       "      <td>30.1140</td>\n",
       "      <td>18840686.0</td>\n",
       "      <td>1.047</td>\n",
       "      <td>0.6712</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>1.556966e-01</td>\n",
       "      <td>0.1557</td>\n",
       "      <td>3.253487e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>System</td>\n",
       "      <td>Gillette</td>\n",
       "      <td>1.614</td>\n",
       "      <td>38.2887</td>\n",
       "      <td>87444504.0</td>\n",
       "      <td>1.004</td>\n",
       "      <td>2.6168</td>\n",
       "      <td>2.4393</td>\n",
       "      <td>7.226283e-01</td>\n",
       "      <td>0.7226</td>\n",
       "      <td>-2.361869e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disposables</td>\n",
       "      <td>Schick</td>\n",
       "      <td>0.600</td>\n",
       "      <td>18.1432</td>\n",
       "      <td>11787957.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0201</td>\n",
       "      <td>0.9068</td>\n",
       "      <td>9.741391e-02</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>-2.392982e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>System</td>\n",
       "      <td>Harry's</td>\n",
       "      <td>1.391</td>\n",
       "      <td>23.1168</td>\n",
       "      <td>2025791.0</td>\n",
       "      <td>0.951</td>\n",
       "      <td>2.2693</td>\n",
       "      <td>2.1014</td>\n",
       "      <td>1.674083e-02</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>1.608945e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Disposables</td>\n",
       "      <td>Pbg Pl</td>\n",
       "      <td>0.254</td>\n",
       "      <td>19.9690</td>\n",
       "      <td>65638.0</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.3845</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>5.424226e-04</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-1.070477e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>System</td>\n",
       "      <td>Pbg Pl</td>\n",
       "      <td>1.017</td>\n",
       "      <td>20.0119</td>\n",
       "      <td>31979.0</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.5378</td>\n",
       "      <td>1.5369</td>\n",
       "      <td>2.642697e-04</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>-1.049923e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Disposables</td>\n",
       "      <td>Bic</td>\n",
       "      <td>1.088</td>\n",
       "      <td>17.2226</td>\n",
       "      <td>811701.0</td>\n",
       "      <td>0.794</td>\n",
       "      <td>1.8573</td>\n",
       "      <td>1.6445</td>\n",
       "      <td>6.707776e-03</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>6.078811e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>System</td>\n",
       "      <td>Dollar Shave Club</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.6585</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.330</td>\n",
       "      <td>1.6486</td>\n",
       "      <td>0.7091</td>\n",
       "      <td>2.256031e-06</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.261744e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>System</td>\n",
       "      <td>Cremo</td>\n",
       "      <td>1.192</td>\n",
       "      <td>9.6452</td>\n",
       "      <td>299.0</td>\n",
       "      <td>0.656</td>\n",
       "      <td>2.8647</td>\n",
       "      <td>1.8012</td>\n",
       "      <td>2.470891e-06</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-4.922875e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>System</td>\n",
       "      <td>Comfort 3</td>\n",
       "      <td>0.115</td>\n",
       "      <td>5.7727</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1.012</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>1.049509e-06</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.347878e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>System</td>\n",
       "      <td>Philips Norelco One Blade</td>\n",
       "      <td>1.456</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.975</td>\n",
       "      <td>2.8333</td>\n",
       "      <td>2.2000</td>\n",
       "      <td>9.090236e-08</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-5.795453e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Disposables Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.482</td>\n",
       "      <td>23.7641</td>\n",
       "      <td>31505982.0</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.7826</td>\n",
       "      <td>0.7285</td>\n",
       "      <td>2.603607e-01</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>3.956618e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>System Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.608</td>\n",
       "      <td>37.7149</td>\n",
       "      <td>89502984.0</td>\n",
       "      <td>1.008</td>\n",
       "      <td>2.6071</td>\n",
       "      <td>2.4299</td>\n",
       "      <td>7.396393e-01</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>-3.956618e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Grand Total</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>32.7146</td>\n",
       "      <td>121008966.0</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.6233</td>\n",
       "      <td>1.5111</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Sector                      Brand  Relative Price  \\\n",
       "0         Disposables                   Gillette           0.421   \n",
       "1              System                   Gillette           1.614   \n",
       "2         Disposables                     Schick           0.600   \n",
       "3              System                    Harry's           1.391   \n",
       "4         Disposables                     Pbg Pl           0.254   \n",
       "5              System                     Pbg Pl           1.017   \n",
       "6         Disposables                        Bic           1.088   \n",
       "7              System          Dollar Shave Club           0.469   \n",
       "8              System                      Cremo           1.192   \n",
       "9              System                  Comfort 3           0.115   \n",
       "10             System  Philips Norelco One Blade           1.456   \n",
       "11  Disposables Total                        NaN           0.482   \n",
       "12       System Total                        NaN           1.608   \n",
       "13        Grand Total                        NaN           1.000   \n",
       "\n",
       "    Av Price/Unit  Value Sales  IYA Price/KG  Base Price/KG  Av Price/KG  \\\n",
       "0         30.1140   18840686.0         1.047         0.6712       0.6369   \n",
       "1         38.2887   87444504.0         1.004         2.6168       2.4393   \n",
       "2         18.1432   11787957.0         0.875         1.0201       0.9068   \n",
       "3         23.1168    2025791.0         0.951         2.2693       2.1014   \n",
       "4         19.9690      65638.0         1.008         0.3845       0.3833   \n",
       "5         20.0119      31979.0         1.008         1.5378       1.5369   \n",
       "6         17.2226     811701.0         0.794         1.8573       1.6445   \n",
       "7          6.6585        273.0         0.330         1.6486       0.7091   \n",
       "8          9.6452        299.0         0.656         2.8647       1.8012   \n",
       "9          5.7727        127.0         1.012         0.1903       0.1735   \n",
       "10        11.0000         11.0         0.975         2.8333       2.2000   \n",
       "11        23.7641   31505982.0         0.993         0.7826       0.7285   \n",
       "12        37.7149   89502984.0         1.008         2.6071       2.4299   \n",
       "13        32.7146  121008966.0         0.994         1.6233       1.5111   \n",
       "\n",
       "     Value Share   WoB %  Value Share DYA  \n",
       "0   1.556966e-01  0.1557     3.253487e-02  \n",
       "1   7.226283e-01  0.7226    -2.361869e-03  \n",
       "2   9.741391e-02  0.0974    -2.392982e-02  \n",
       "3   1.674083e-02  0.0167     1.608945e-02  \n",
       "4   5.424226e-04  0.0005    -1.070477e-02  \n",
       "5   2.642697e-04  0.0003    -1.049923e-02  \n",
       "6   6.707776e-03  0.0067     6.078811e-03  \n",
       "7   2.256031e-06  0.0000    -2.261744e-03  \n",
       "8   2.470891e-06  0.0000    -4.922875e-03  \n",
       "9   1.049509e-06  0.0000     2.347878e-07  \n",
       "10  9.090236e-08  0.0000    -5.795453e-07  \n",
       "11  2.603607e-01  0.2604     3.956618e-03  \n",
       "12  7.396393e-01  0.7396    -3.956618e-03  \n",
       "13  1.000000e+00  1.0000     0.000000e+00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shelf_Sector_all_brands['Manual Shave Men | Bj\\'s And Sam\\'s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f50239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
